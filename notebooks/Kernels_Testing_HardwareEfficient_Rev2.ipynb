{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeE6sxhb3DCB",
        "outputId": "7f1ab49d-3550-4e5c-9884-a6fdd7c4ab4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 17 16:26:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0             27W /   70W |     102MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwBIzbiR3aT_",
        "outputId": "7d9d96f9-80da-4484-bc90-cf95a49da5ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.575.51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "nvmlInit()\n",
        "handle = nvmlDeviceGetHandleByIndex(0)\n",
        "print(\"Power draw (W):\", nvmlDeviceGetPowerUsage(handle) / 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxeFETC75KGK",
        "outputId": "4587c6f7-d430-41b6-8dee-3525346fdf1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power draw (W): 10.051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import defaultdict, deque\n",
        "import numpy as np\n",
        "import pynvml\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "PYNVML_AVAILABLE = True\n",
        "\n",
        "@dataclass\n",
        "class GPUMetrics:\n",
        "  def __post_init__(self):\n",
        "    if self.timestamp is None:\n",
        "      self.timestamp = time.time()\n",
        "  timestamp: float\n",
        "  power_draw: float  # Watts\n",
        "  temperature: float  # Celsius\n",
        "  memory_used: int   # Bytes\n",
        "  memory_total: int  # Bytes\n",
        "  gpu_utilization: float  # Percentage\n",
        "  memory_utilization: float  # Percentage}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExpertProfile:\n",
        "    \"\"\"Profile data for a specific expert.\"\"\"\n",
        "    expert_id: str\n",
        "    flops: int\n",
        "    memory_footprint: int\n",
        "    avg_latency: float\n",
        "    energy_cost: float  # Estimated Joules\n",
        "    activation_count: int\n",
        "    last_updated: float\n",
        "\n",
        "\n",
        "class GPUProfiler:\n",
        "    \"\"\"\n",
        "    Continuously monitors GPU metrics and provides energy profiling for MoE experts.\n",
        "    Runs in a separate thread to avoid blocking main computation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device_id: int = 0, poll_interval: float = 0.1):\n",
        "        self.device_id = device_id\n",
        "        self.poll_interval = poll_interval\n",
        "        self.is_running = False\n",
        "        self.metrics_history = deque(maxlen=1000)  # Keep last 1000 samples\n",
        "        self.expert_profiles = {}\n",
        "        self.operation_stack = []  # Stack for nested operations\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "        if PYNVML_AVAILABLE:\n",
        "            try:\n",
        "                pynvml.nvmlInit()\n",
        "                self.handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n",
        "                self.nvml_available = True\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Failed to initialize NVML: {e}\")\n",
        "                self.nvml_available = False\n",
        "        else:\n",
        "            self.nvml_available = False\n",
        "\n",
        "        self.polling_thread = None\n",
        "\n",
        "    def start_profiling(self):\n",
        "        \"\"\"Start the GPU monitoring thread.\"\"\"\n",
        "        if self.is_running:\n",
        "            return\n",
        "\n",
        "        self.is_running = True\n",
        "        self.polling_thread = threading.Thread(target=self._polling_loop, daemon=True)\n",
        "        self.polling_thread.start()\n",
        "        logging.info(\"GPU profiling started\")\n",
        "\n",
        "    def stop_profiling(self):\n",
        "        \"\"\"Stop the GPU monitoring thread.\"\"\"\n",
        "        self.is_running = False\n",
        "        if self.polling_thread:\n",
        "            self.polling_thread.join(timeout=1.0)\n",
        "        logging.info(\"GPU profiling stopped\")\n",
        "\n",
        "    def _polling_loop(self):\n",
        "        \"\"\"Main polling loop running in separate thread.\"\"\"\n",
        "        while self.is_running:\n",
        "            try:\n",
        "                metrics = self._collect_gpu_metrics()\n",
        "                if metrics:\n",
        "                    with self.lock:\n",
        "                        self.metrics_history.append(metrics)\n",
        "                time.sleep(self.poll_interval)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in GPU polling: {e}\")\n",
        "                time.sleep(self.poll_interval)\n",
        "\n",
        "    def _collect_gpu_metrics(self) -> Optional[GPUMetrics]:\n",
        "        \"\"\"Collect current GPU metrics.\"\"\"\n",
        "        if not self.nvml_available:\n",
        "            # Return dummy metrics for testing\n",
        "            return GPUMetrics(\n",
        "                timestamp=time.time(),\n",
        "                power_draw=150.0 + np.random.normal(0, 10),\n",
        "                temperature=65.0 + np.random.normal(0, 5),\n",
        "                memory_used=int(4e9 + np.random.normal(0, 1e8)),\n",
        "                memory_total=int(8e9),\n",
        "                gpu_utilization=80.0 + np.random.normal(0, 10),\n",
        "                memory_utilization=50.0 + np.random.normal(0, 5)\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            power_draw = pynvml.nvmlDeviceGetPowerUsage(self.handle) / 1000.0  # mW to W\n",
        "            temperature = pynvml.nvmlDeviceGetTemperature(self.handle, pynvml.NVML_TEMPERATURE_GPU)\n",
        "\n",
        "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)\n",
        "            memory_used = mem_info.used\n",
        "            memory_total = mem_info.total\n",
        "\n",
        "            util_rates = pynvml.nvmlDeviceGetUtilizationRates(self.handle)\n",
        "            gpu_util = util_rates.gpu\n",
        "            memory_util = util_rates.memory\n",
        "\n",
        "            return GPUMetrics(\n",
        "                timestamp=time.time(),\n",
        "                power_draw=power_draw,\n",
        "                temperature=temperature,\n",
        "                memory_used=memory_used,\n",
        "                memory_total=memory_total,\n",
        "                gpu_utilization=gpu_util,\n",
        "                memory_utilization=memory_util\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to collect GPU metrics: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_current_metrics(self) -> Optional[GPUMetrics]:\n",
        "        \"\"\"Get the most recent GPU metrics.\"\"\"\n",
        "        with self.lock:\n",
        "            return self.metrics_history[-1] if self.metrics_history else None\n",
        "\n",
        "    def get_metrics_window(self, duration: float = 1.0) -> List[GPUMetrics]:\n",
        "        \"\"\"Get metrics from the last `duration` seconds.\"\"\"\n",
        "        current_time = time.time()\n",
        "        cutoff_time = current_time - duration\n",
        "\n",
        "        with self.lock:\n",
        "            return [m for m in self.metrics_history if m.timestamp >= cutoff_time]\n",
        "\n",
        "    def start_operation(self, operation_name: str, expert_id: Optional[str] = None):\n",
        "        \"\"\"Start tracking an operation (e.g., expert forward pass).\"\"\"\n",
        "        operation_data = {\n",
        "            'name': operation_name,\n",
        "            'expert_id': expert_id,\n",
        "            'start_time': time.time(),\n",
        "            'start_metrics': self.get_current_metrics()\n",
        "        }\n",
        "        self.operation_stack.append(operation_data)\n",
        "\n",
        "    def end_operation(self) -> Optional[Dict]:\n",
        "        \"\"\"End tracking the current operation and return profiling data.\"\"\"\n",
        "        if not self.operation_stack:\n",
        "            return None\n",
        "\n",
        "        operation_data = self.operation_stack.pop()\n",
        "        end_time = time.time()\n",
        "        end_metrics = self.get_current_metrics()\n",
        "\n",
        "        duration = end_time - operation_data['start_time']\n",
        "\n",
        "        # Calculate energy consumption during operation\n",
        "        energy_consumed = 0.0\n",
        "        if operation_data['start_metrics'] and end_metrics:\n",
        "            avg_power = (operation_data['start_metrics'].power_draw + end_metrics.power_draw) / 2.0\n",
        "            energy_consumed = avg_power * duration  # Joules\n",
        "\n",
        "        result = {\n",
        "            'operation': operation_data['name'],\n",
        "            'expert_id': operation_data['expert_id'],\n",
        "            'duration': duration,\n",
        "            'energy_consumed': energy_consumed,\n",
        "            'start_metrics': operation_data['start_metrics'],\n",
        "            'end_metrics': end_metrics\n",
        "        }\n",
        "\n",
        "        # Update expert profile if applicable\n",
        "        if operation_data['expert_id']:\n",
        "            self._update_expert_profile(operation_data['expert_id'], result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _update_expert_profile(self, expert_id: str, operation_result: Dict):\n",
        "        \"\"\"Update the profile for a specific expert.\"\"\"\n",
        "        if expert_id not in self.expert_profiles:\n",
        "            self.expert_profiles[expert_id] = ExpertProfile(\n",
        "                expert_id=expert_id,\n",
        "                flops=0,\n",
        "                memory_footprint=0,\n",
        "                avg_latency=0.0,\n",
        "                energy_cost=0.0,\n",
        "                activation_count=0,\n",
        "                last_updated=time.time()\n",
        "            )\n",
        "\n",
        "        profile = self.expert_profiles[expert_id]\n",
        "        profile.activation_count += 1\n",
        "\n",
        "        # Update running averages\n",
        "        alpha = 0.1  # Exponential moving average factor\n",
        "        profile.avg_latency = (1 - alpha) * profile.avg_latency + alpha * operation_result['duration']\n",
        "        profile.energy_cost = (1 - alpha) * profile.energy_cost + alpha * operation_result['energy_consumed']\n",
        "        profile.last_updated = time.time()\n",
        "\n",
        "    def estimate_expert_flops(self, expert_module: nn.Module, input_shape: Tuple[int, ...]) -> int:\n",
        "        \"\"\"\n",
        "        Estimate FLOPs for an expert module.\n",
        "        This is a simplified estimation - you might want to use more sophisticated methods.\n",
        "        \"\"\"\n",
        "        total_flops = 0\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = torch.randn(input_shape)\n",
        "\n",
        "        def flop_count_hook(module, input, output):\n",
        "            nonlocal total_flops\n",
        "            if isinstance(module, nn.Linear):\n",
        "                # For linear layers: input_size * output_size * batch_size\n",
        "                in_features = module.in_features\n",
        "                out_features = module.out_features\n",
        "                batch_size = input[0].shape[0] if input else 1\n",
        "                total_flops += in_features * out_features * batch_size\n",
        "            elif isinstance(module, nn.Conv2d):\n",
        "                # Simplified conv2d FLOP estimation\n",
        "                kernel_flops = module.kernel_size[0] * module.kernel_size[1] * module.in_channels\n",
        "                output_elements = output.numel() if hasattr(output, 'numel') else 1\n",
        "                total_flops += kernel_flops * output_elements\n",
        "\n",
        "        # Register hooks\n",
        "        hooks = []\n",
        "        for module in expert_module.modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                hooks.append(module.register_forward_hook(flop_count_hook))\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                expert_module(dummy_input)\n",
        "        finally:\n",
        "            # Remove hooks\n",
        "            for hook in hooks:\n",
        "                hook.remove()\n",
        "\n",
        "        return total_flops\n",
        "\n",
        "    def get_expert_profile(self, expert_id: str) -> Optional[ExpertProfile]:\n",
        "        \"\"\"Get the profile for a specific expert.\"\"\"\n",
        "        return self.expert_profiles.get(expert_id)\n",
        "\n",
        "    def get_all_expert_profiles(self) -> Dict[str, ExpertProfile]:\n",
        "        \"\"\"Get profiles for all experts.\"\"\"\n",
        "        return self.expert_profiles.copy()\n",
        "\n",
        "    def save_profiles(self, filepath: str):\n",
        "        \"\"\"Save expert profiles to JSON file.\"\"\"\n",
        "        profiles_dict = {\n",
        "            expert_id: asdict(profile)\n",
        "            for expert_id, profile in self.expert_profiles.items()\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(profiles_dict, f, indent=2)\n",
        "\n",
        "    def load_profiles(self, filepath: str):\n",
        "        \"\"\"Load expert profiles from JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                profiles_dict = json.load(f)\n",
        "\n",
        "            self.expert_profiles = {\n",
        "                expert_id: ExpertProfile(**profile_data)\n",
        "                for expert_id, profile_data in profiles_dict.items()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load profiles: {e}\")\n",
        "\n",
        "    def get_power_statistics(self, duration: float = 10.0) -> Dict[str, float]:\n",
        "        \"\"\"Get power consumption statistics over a time window.\"\"\"\n",
        "        metrics = self.get_metrics_window(duration)\n",
        "        if not metrics:\n",
        "            return {}\n",
        "\n",
        "        power_values = [m.power_draw for m in metrics]\n",
        "\n",
        "        return {\n",
        "            'mean_power': np.mean(power_values),\n",
        "            'max_power': np.max(power_values),\n",
        "            'min_power': np.min(power_values),\n",
        "            'std_power': np.std(power_values),\n",
        "            'total_energy': np.sum(power_values) * self.poll_interval  # Approximate\n",
        "        }\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"Context manager entry.\"\"\"\n",
        "        self.start_profiling()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Context manager exit.\"\"\"\n",
        "        self.stop_profiling()\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Example of how to use the GPUProfiler\n",
        "    profiler = GPUProfiler(device_id=0, poll_interval=0.1)\n",
        "\n",
        "    # Start profiling\n",
        "    profiler.start_profiling()\n",
        "\n",
        "    # Simulate some operations\n",
        "    time.sleep(1.0)\n",
        "\n",
        "    # Example expert operation\n",
        "    profiler.start_operation(\"expert_forward\", \"expert_0\")\n",
        "    time.sleep(0.5)  # Simulate computation\n",
        "    result = profiler.end_operation()\n",
        "\n",
        "    print(\"Operation result:\", result)\n",
        "\n",
        "    # Get current metrics\n",
        "    current = profiler.get_current_metrics()\n",
        "    if current:\n",
        "        print(f\"Current power: {current.power_draw:.2f}W, Temp: {current.temperature:.1f}°C\")\n",
        "\n",
        "    # Get power statistics\n",
        "    stats = profiler.get_power_statistics(duration=5.0)\n",
        "    print(\"Power statistics:\", stats)\n",
        "\n",
        "    # Stop profiling\n",
        "    profiler.stop_profiling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNfJuZtj5MHw",
        "outputId": "8bdbb0e5-e738-4260-83c6-dc9f66cf7ac4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation result: {'operation': 'expert_forward', 'expert_id': 'expert_0', 'duration': 0.5008032321929932, 'energy_consumed': 5.083403208374977, 'start_metrics': GPUMetrics(timestamp=1750177592.0765254, power_draw=10.051, temperature=45, memory_used=277872640, memory_total=16106127360, gpu_utilization=0, memory_utilization=0), 'end_metrics': GPUMetrics(timestamp=1750177592.5354972, power_draw=10.25, temperature=45, memory_used=277872640, memory_total=16106127360, gpu_utilization=0, memory_utilization=0)}\n",
            "Current power: 10.25W, Temp: 45.0°C\n",
            "Power statistics: {'mean_power': np.float64(10.143), 'max_power': np.float64(10.349), 'min_power': np.float64(10.051), 'std_power': np.float64(0.10237815057074293), 'total_energy': np.float64(14.200200000000002)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "\n",
        "class ThermalState(Enum):\n",
        "    \"\"\"Enumeration of thermal states for the system.\"\"\"\n",
        "    COOL = \"cool\"\n",
        "    WARM = \"warm\"\n",
        "    HOT = \"hot\"\n",
        "    CRITICAL = \"critical\"\n",
        "    THROTTLED = \"throttled\" # Added for explicit throttled state\n",
        "\n",
        "class PowerMode(Enum):\n",
        "    \"\"\"Enumeration of power modes.\"\"\"\n",
        "    LOW_POWER = \"low_power\"\n",
        "    BALANCED = \"balanced\"\n",
        "    PERFORMANCE = \"performance\"\n",
        "    EMERGENCY = \"emergency\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ThermalBudget:\n",
        "    \"\"\"Represents current thermal constraints and budgets.\"\"\"\n",
        "    max_temperature: float\n",
        "    max_power: float\n",
        "    max_energy_per_token: float # This might be dynamic based on expert costs\n",
        "    current_temperature: float\n",
        "    current_power: float\n",
        "    thermal_headroom: float\n",
        "    power_headroom: float\n",
        "    recommended_experts: List[str] # Now a list for specific expert IDs\n",
        "    throttle_factor: float = 1.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ThermalSignal:\n",
        "    \"\"\"Complete thermal signal with all relevant information.\"\"\"\n",
        "    timestamp: float\n",
        "    thermal_state: ThermalState\n",
        "    power_mode: PowerMode\n",
        "    temperature: float\n",
        "    power_draw: float\n",
        "    thermal_budget: ThermalBudget\n",
        "    expert_recommendations: Dict[str, float]  # expert_id -> priority score\n",
        "    throttle_recommendations: Dict[str, float]  # operation -> throttle factor (e.g., 'global_throttle': 0.8)\n",
        "    emergency_actions: List[str]\n",
        "\n",
        "\n",
        "class ThermalSignalGenerator:\n",
        "    \"\"\"\n",
        "    Generates thermal signals based on GPU profiling data and thermal budgets.\n",
        "    This module determines when to throttle computation, switch experts, or\n",
        "    take emergency actions based on thermal constraints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 profiler: GPUProfiler,\n",
        "                 cost_table_path: str = \"energy/cost_table.json\"):\n",
        "        self.profiler = profiler\n",
        "        self.cost_table_path = Path(cost_table_path)\n",
        "        self.cost_table = self._load_cost_table()\n",
        "\n",
        "        # Thermal parameters from cost table\n",
        "        self.thermal_params = self.cost_table.get(\"thermal_parameters\", {})\n",
        "        self.energy_budgets = self.cost_table.get(\"energy_budgets\", {})\n",
        "        self.expert_profiles = self.cost_table.get(\"expert_profiles\", {})\n",
        "\n",
        "        # State tracking\n",
        "        self.current_mode = PowerMode.BALANCED\n",
        "        self.thermal_history: List[ThermalSignal] = []\n",
        "        self.last_signal_time = 0.0\n",
        "        self.emergency_cooldown_active = False # Flag for emergency state\n",
        "        self.emergency_cooldown_duration = self.thermal_params.get(\"emergency_cooldown_duration\", 30.0)\n",
        "        self.emergency_cooldown_start_time = 0.0\n",
        "\n",
        "        # Thermal model parameters\n",
        "        self.base_temp = self.thermal_params.get(\"base_temperature\", 45.0)\n",
        "        self.warm_temp_threshold = self.thermal_params.get(\"warm_temperature_threshold\", self.base_temp + 20)\n",
        "        self.hot_temp_threshold = self.thermal_params.get(\"hot_temperature_threshold\", 83.0) # Renamed for consistency\n",
        "        self.critical_temp = self.thermal_params.get(\"critical_temperature\", 87.0)\n",
        "        self.thermal_time_constant = self.thermal_params.get(\"thermal_time_constant\", 15.0)\n",
        "\n",
        "        logging.basicConfig(level=logging.INFO) # Ensure logging is configured\n",
        "        logging.info(f\"ThermalSignal initialized with thresholds: {self.hot_temp_threshold}°C hot, {self.critical_temp}°C critical\")\n",
        "\n",
        "    def _load_cost_table(self) -> Dict:\n",
        "        \"\"\"Load the cost table configuration.\"\"\"\n",
        "        try:\n",
        "            with open(self.cost_table_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load cost table from {self.cost_table_path}: {e}\")\n",
        "            return self._get_default_cost_table()\n",
        "\n",
        "    def _get_default_cost_table(self) -> Dict:\n",
        "        \"\"\"Return default cost table if loading fails.\"\"\"\n",
        "        return {\n",
        "            \"thermal_parameters\": {\n",
        "                \"base_temperature\": 45.0,\n",
        "                \"warm_temperature_threshold\": 65.0, # Added specific warm threshold\n",
        "                \"hot_temperature_threshold\": 83.0,\n",
        "                \"critical_temperature\": 87.0,\n",
        "                \"thermal_time_constant\": 15.0,\n",
        "                \"emergency_cooldown_duration\": 30.0\n",
        "            },\n",
        "            \"energy_budgets\": {\n",
        "                \"low_power\": {\"max_power_watts\": 200, \"max_temperature\": 75.0, \"max_energy_per_token_mj\": 5.0},\n",
        "                \"balanced\": {\"max_power_watts\": 350, \"max_temperature\": 80.0, \"max_energy_per_token_mj\": 3.0},\n",
        "                \"performance\": {\"max_power_watts\": 450, \"max_temperature\": 85.0, \"max_energy_per_token_mj\": 1.5}\n",
        "            },\n",
        "            \"expert_profiles\": {\n",
        "                \"expert_A\": {\"average_power_watts\": 50, \"energy_per_token_mj\": 2.0, \"thermal_impact\": 0.1},\n",
        "                \"expert_B\": {\"average_power_watts\": 70, \"energy_per_token_mj\": 2.5, \"thermal_impact\": 0.15},\n",
        "                \"expert_C\": {\"average_power_watts\": 30, \"energy_per_token_mj\": 1.0, \"thermal_impact\": 0.05}\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_thermal_signal(self) -> Optional[ThermalSignal]:\n",
        "        \"\"\"\n",
        "        Generate current thermal signal based on GPU metrics and thermal model.\n",
        "        \"\"\"\n",
        "        current_metrics = self.profiler.get_current_metrics()\n",
        "        if not current_metrics:\n",
        "            logging.warning(\"No GPU metrics available to generate thermal signal.\")\n",
        "            return None\n",
        "\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Check for active emergency cooldown\n",
        "        if self.emergency_cooldown_active:\n",
        "            if current_time - self.emergency_cooldown_start_time < self.emergency_cooldown_duration:\n",
        "                # Still in emergency cooldown, prioritize cool-down actions\n",
        "                logging.info(f\"Emergency cooldown active. Remaining: {self.emergency_cooldown_duration - (current_time - self.emergency_cooldown_start_time):.1f}s\")\n",
        "                # Force emergency mode during cooldown\n",
        "                power_mode = PowerMode.EMERGENCY\n",
        "                thermal_state = ThermalState.THROTTLED # Or CRITICAL if temp is still high\n",
        "                if current_metrics.temperature < self.warm_temp_threshold: # Exit emergency if sufficiently cooled\n",
        "                    self.emergency_cooldown_active = False\n",
        "                    logging.info(\"Exiting emergency cooldown: temperature has dropped.\")\n",
        "                else:\n",
        "                    # Still hot during cooldown\n",
        "                    if current_metrics.temperature >= self.critical_temp:\n",
        "                         thermal_state = ThermalState.CRITICAL\n",
        "                    elif current_metrics.temperature >= self.hot_temp_threshold:\n",
        "                         thermal_state = ThermalState.HOT\n",
        "                    else:\n",
        "                         thermal_state = ThermalState.THROTTLED # Indicates active throttling for cooldown\n",
        "            else:\n",
        "                self.emergency_cooldown_active = False\n",
        "                logging.info(\"Emergency cooldown period ended.\")\n",
        "\n",
        "        # Determine thermal state if not in active cooldown\n",
        "        if not self.emergency_cooldown_active:\n",
        "            thermal_state = self._classify_thermal_state(current_metrics)\n",
        "\n",
        "        # Determine power mode (may be overridden by thermal state)\n",
        "        # If emergency cooldown is active, power_mode is already set to EMERGENCY\n",
        "        if not self.emergency_cooldown_active:\n",
        "            power_mode = self._determine_power_mode(current_metrics, thermal_state)\n",
        "        self.current_mode = power_mode # Update internal state\n",
        "\n",
        "        # Calculate thermal budget\n",
        "        thermal_budget = self._calculate_thermal_budget(current_metrics, power_mode)\n",
        "\n",
        "        # Generate expert recommendations\n",
        "        expert_recommendations = self._generate_expert_recommendations(\n",
        "            current_metrics, thermal_budget, thermal_state\n",
        "        )\n",
        "\n",
        "        # Generate throttle recommendations\n",
        "        throttle_recommendations = self._generate_throttle_recommendations(\n",
        "            current_metrics, thermal_state, thermal_budget\n",
        "        )\n",
        "\n",
        "        # Check for emergency actions (can trigger cooldown)\n",
        "        emergency_actions = self._check_emergency_actions(current_metrics, thermal_state)\n",
        "        if emergency_actions and \"initiate_emergency_cooldown\" in emergency_actions and not self.emergency_cooldown_active:\n",
        "            self.emergency_cooldown_active = True\n",
        "            self.emergency_cooldown_start_time = current_time\n",
        "            logging.warning(\"Initiating emergency cooldown due to critical thermal state!\")\n",
        "\n",
        "\n",
        "        signal = ThermalSignal(\n",
        "            timestamp=current_time,\n",
        "            thermal_state=thermal_state,\n",
        "            power_mode=power_mode,\n",
        "            temperature=current_metrics.temperature,\n",
        "            power_draw=current_metrics.power_draw,\n",
        "            thermal_budget=thermal_budget,\n",
        "            expert_recommendations=expert_recommendations,\n",
        "            throttle_recommendations=throttle_recommendations,\n",
        "            emergency_actions=emergency_actions\n",
        "        )\n",
        "\n",
        "        # Update history\n",
        "        self.thermal_history.append(signal)\n",
        "        if len(self.thermal_history) > 100:  # Keep a reasonable history size\n",
        "            self.thermal_history.pop(0)\n",
        "\n",
        "        self.last_signal_time = current_time\n",
        "\n",
        "        return signal\n",
        "\n",
        "    def _classify_thermal_state(self, metrics: GPUMetrics) -> ThermalState:\n",
        "        \"\"\"Classify current thermal state based on temperature.\"\"\"\n",
        "        temp = metrics.temperature\n",
        "\n",
        "        if temp >= self.critical_temp:\n",
        "            return ThermalState.CRITICAL\n",
        "        elif temp >= self.hot_temp_threshold:\n",
        "            return ThermalState.HOT\n",
        "        elif temp >= self.warm_temp_threshold:\n",
        "            return ThermalState.WARM\n",
        "        else:\n",
        "            return ThermalState.COOL\n",
        "\n",
        "    def _determine_power_mode(self, metrics: GPUMetrics, thermal_state: ThermalState) -> PowerMode:\n",
        "        \"\"\"\n",
        "        Determine the appropriate power mode based on thermal state and current usage.\n",
        "        This could involve hysteresis to prevent rapid mode switching.\n",
        "        \"\"\"\n",
        "        proposed_mode = self.current_mode # Start with current mode\n",
        "\n",
        "        # Logic for power mode transition\n",
        "        if thermal_state == ThermalState.CRITICAL:\n",
        "            proposed_mode = PowerMode.EMERGENCY\n",
        "        elif thermal_state == ThermalState.HOT:\n",
        "            if proposed_mode == PowerMode.PERFORMANCE: # Drop from performance if hot\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "            elif proposed_mode == PowerMode.BALANCED and metrics.power_draw > self.energy_budgets[\"balanced\"][\"max_power_watts\"] * 0.9:\n",
        "                # If balanced and still drawing too much power, consider low power\n",
        "                proposed_mode = PowerMode.LOW_POWER\n",
        "        elif thermal_state == ThermalState.WARM:\n",
        "            if proposed_mode == PowerMode.PERFORMANCE and metrics.power_draw > self.energy_budgets[\"performance\"][\"max_power_watts\"]:\n",
        "                # If warm but drawing too much for performance, drop to balanced\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "            elif proposed_mode == PowerMode.LOW_POWER and metrics.temperature < self.warm_temp_threshold - 5 and metrics.gpu_utilization < 0.5:\n",
        "                # If low power and cooled down, can move to balanced if not highly utilized\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "        elif thermal_state == ThermalState.COOL:\n",
        "            if proposed_mode == PowerMode.LOW_POWER and metrics.temperature < self.base_temp + 5 and metrics.gpu_utilization < 0.3:\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "            elif proposed_mode == PowerMode.BALANCED and metrics.temperature < self.base_temp + 10 and metrics.gpu_utilization > 0.8:\n",
        "                # If balanced and good temperature, consider performance if high utilization\n",
        "                proposed_mode = PowerMode.PERFORMANCE\n",
        "\n",
        "        # Apply hysteresis: only change mode if conditions persist or change is drastic\n",
        "        # For simplicity, we'll keep it direct for now, but a real system might use a timer\n",
        "        # or history to prevent oscillations.\n",
        "\n",
        "        if proposed_mode != self.current_mode:\n",
        "            logging.info(f\"Power mode transition: {self.current_mode.value} -> {proposed_mode.value} due to {thermal_state.value} state.\")\n",
        "            self.current_mode = proposed_mode # Update internal state\n",
        "\n",
        "        return proposed_mode\n",
        "\n",
        "    def _calculate_thermal_budget(self, metrics: GPUMetrics, power_mode: PowerMode) -> ThermalBudget:\n",
        "        \"\"\"\n",
        "        Calculate current thermal budget based on the selected power mode and current metrics.\n",
        "        \"\"\"\n",
        "        mode_budget = self.energy_budgets.get(power_mode.value, self.energy_budgets[\"balanced\"])\n",
        "\n",
        "        max_temp = mode_budget.get(\"max_temperature\", 80.0)\n",
        "        max_power = mode_budget.get(\"max_power_watts\", 350.0)\n",
        "        max_energy_per_token = mode_budget.get(\"max_energy_per_token_mj\", 3.0)\n",
        "\n",
        "        thermal_headroom = max_temp - metrics.temperature\n",
        "        power_headroom = max_power - metrics.power_draw\n",
        "\n",
        "        # Simple throttle factor calculation based on temperature headroom\n",
        "        throttle_factor = 1.0\n",
        "        if metrics.temperature >= self.hot_temp_threshold:\n",
        "            # Linear throttle as temperature approaches critical\n",
        "            temp_range = self.critical_temp - self.hot_temp_threshold\n",
        "            if temp_range > 0:\n",
        "                throttle_factor = 1.0 - ((metrics.temperature - self.hot_temp_threshold) / temp_range) * 0.5 # Up to 50% throttle\n",
        "            throttle_factor = max(0.1, throttle_factor) # Ensure it doesn't go below 10%\n",
        "            logging.warning(f\"Temperature is hot ({metrics.temperature}°C), applying throttle factor: {throttle_factor:.2f}\")\n",
        "\n",
        "        # Also consider power headroom for throttling\n",
        "        if metrics.power_draw > max_power * 1.1: # 10% overshoot on power budget\n",
        "            power_throttle = 1.0 - ((metrics.power_draw - max_power) / (max_power * 0.5)) # throttle up to 50% for 50% power overshoot\n",
        "            throttle_factor = min(throttle_factor, max(0.1, power_throttle))\n",
        "            logging.warning(f\"Power draw ({metrics.power_draw:.1f}W) exceeds budget, adjusting throttle factor: {throttle_factor:.2f}\")\n",
        "\n",
        "\n",
        "        # Dummy recommended experts for now - a real system would use a model\n",
        "        # or heuristics based on expert costs and current budget.\n",
        "        # This part should be driven by the _generate_expert_recommendations method.\n",
        "        recommended_experts: List[str] = [] # This field is populated by _generate_expert_recommendations\n",
        "\n",
        "        return ThermalBudget(\n",
        "            max_temperature=max_temp,\n",
        "            max_power=max_power,\n",
        "            max_energy_per_token=max_energy_per_token,\n",
        "            current_temperature=metrics.temperature,\n",
        "            current_power=metrics.power_draw,\n",
        "            thermal_headroom=thermal_headroom,\n",
        "            power_headroom=power_headroom,\n",
        "            recommended_experts=recommended_experts, # Will be filled by another method\n",
        "            throttle_factor=throttle_factor\n",
        "        )\n",
        "\n",
        "    def _generate_expert_recommendations(\n",
        "        self, metrics: GPUMetrics, budget: ThermalBudget, thermal_state: ThermalState\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Generate recommendations for expert prioritization based on thermal budget\n",
        "        and expert profiles. Experts with lower energy costs and thermal impact\n",
        "        are prioritized when resources are constrained.\n",
        "        \"\"\"\n",
        "        expert_priority: Dict[str, float] = {}\n",
        "\n",
        "        for expert_id, profile in self.expert_profiles.items():\n",
        "            # Calculate a basic priority score. Lower is better (more efficient).\n",
        "            # This is a simplified heuristic. A more advanced system might\n",
        "            # consider recent expert usage, task requirements, etc.\n",
        "            energy_cost = profile.get(\"energy_per_token_mj\", 100.0)\n",
        "            thermal_impact = profile.get(\"thermal_impact\", 1.0)\n",
        "            avg_power = profile.get(\"average_power_watts\", 100.0)\n",
        "\n",
        "            score = 0.0\n",
        "\n",
        "            # Prioritize experts with lower energy consumption\n",
        "            score += energy_cost / budget.max_energy_per_token # Lower is better, so divide\n",
        "\n",
        "            # Prioritize experts that are less thermally impactful\n",
        "            score += thermal_impact * (metrics.temperature / self.critical_temp) * 5.0 # Higher impact at higher temps lowers priority more\n",
        "\n",
        "            # Adjust based on power headroom: if power is tight, penalize high-power experts\n",
        "            if budget.power_headroom < 50: # If power is getting tight (e.g., less than 50W headroom)\n",
        "                score += (avg_power / budget.max_power) * 2.0 # Penalize high power consumption\n",
        "\n",
        "            # If current state is hot or critical, heavily penalize experts that generate a lot of heat/power\n",
        "            if thermal_state in [ThermalState.HOT, ThermalState.CRITICAL]:\n",
        "                score += (thermal_impact * 10) + (avg_power / budget.max_power * 5)\n",
        "\n",
        "            expert_priority[expert_id] = score\n",
        "\n",
        "        # Sort experts by priority score (lower score means higher priority)\n",
        "        sorted_experts = sorted(expert_priority.items(), key=lambda item: item[1])\n",
        "\n",
        "        # Convert back to dictionary, perhaps only returning top N or all with scores\n",
        "        return {expert_id: score for expert_id, score in sorted_experts}\n",
        "\n",
        "    def _generate_throttle_recommendations(\n",
        "        self, metrics: GPUMetrics, thermal_state: ThermalState, budget: ThermalBudget\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Generate throttle recommendations for various operations.\n",
        "        A global throttle factor is primarily determined by the thermal budget.\n",
        "        \"\"\"\n",
        "        throttle_recs: Dict[str, float] = {}\n",
        "\n",
        "        # The primary throttle factor comes from the thermal budget calculation\n",
        "        global_throttle_factor = budget.throttle_factor\n",
        "        throttle_recs[\"global_compute_throttle\"] = global_throttle_factor\n",
        "\n",
        "        # Add more specific throttles if needed, e.g., memory bandwidth throttle\n",
        "        # based on memory temperature or utilization, if available in metrics.\n",
        "        if metrics.memory_utilization > 0.9 and thermal_state in [ThermalState.HOT, ThermalState.CRITICAL]:\n",
        "            throttle_recs[\"memory_bandwidth_throttle\"] = max(0.5, 1.0 - (metrics.memory_utilization - 0.9) * 5) # 50% throttle if 100% util\n",
        "            logging.warning(f\"High memory utilization and thermal state: {throttle_recs['memory_bandwidth_throttle']:.2f} memory throttle.\")\n",
        "\n",
        "\n",
        "        if thermal_state == ThermalState.CRITICAL:\n",
        "            throttle_recs[\"global_compute_throttle\"] = min(0.1, global_throttle_factor) # Aggressive throttle\n",
        "            logging.critical(\"CRITICAL thermal state: Aggressive global throttle applied.\")\n",
        "        elif thermal_state == ThermalState.HOT:\n",
        "            throttle_recs[\"global_compute_throttle\"] = min(0.5, global_throttle_factor) # Moderate throttle\n",
        "            logging.warning(\"HOT thermal state: Moderate global throttle applied.\")\n",
        "        elif thermal_state == ThermalState.THROTTLED: # Explicit throttled state\n",
        "            throttle_recs[\"global_compute_throttle\"] = min(0.2, global_throttle_factor) # More aggressive throttle\n",
        "            logging.warning(\"THROTTLED thermal state: Global throttle applied for cooldown.\")\n",
        "\n",
        "\n",
        "        return throttle_recs\n",
        "\n",
        "    def _check_emergency_actions(self, metrics: GPUMetrics, thermal_state: ThermalState) -> List[str]:\n",
        "        \"\"\"\n",
        "        Check if any emergency actions are required, such as initiating a full cooldown.\n",
        "        \"\"\"\n",
        "        emergency_actions: List[str] = []\n",
        "\n",
        "        if thermal_state == ThermalState.CRITICAL:\n",
        "            logging.critical(f\"Temperature {metrics.temperature}°C is CRITICAL! Recommending emergency cooldown.\")\n",
        "            emergency_actions.append(\"initiate_emergency_cooldown\")\n",
        "            emergency_actions.append(\"alert_system_operator\") # Example of other emergency actions\n",
        "\n",
        "        # Add other conditions that might trigger emergency actions, e.g.,\n",
        "        # persistent high power draw despite throttling, or fan failure detection.\n",
        "\n",
        "        return emergency_actions"
      ],
      "metadata": {
        "id": "XNiT_0Q_5pav"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TopKRouter(nn.Module):\n",
        "    \"\"\"\n",
        "    Top-K router for MoE with load balancing loss and detailed profiling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_experts: int,\n",
        "        top_k: int = 2,\n",
        "        capacity_factor: float = 1.25,\n",
        "        gate_noise: float = 1e-2,\n",
        "        expert_dropout: float = 0.0,\n",
        "        balance_loss_weight: float = 0.01,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_experts = n_experts\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.gate_noise = gate_noise\n",
        "        self.expert_dropout = expert_dropout\n",
        "        self.balance_loss_weight = balance_loss_weight\n",
        "\n",
        "        # Gating network - simple linear layer\n",
        "        self.gate = nn.Linear(d_model, n_experts, bias=False)\n",
        "\n",
        "        # Initialize gate weights\n",
        "        nn.init.normal_(self.gate.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        # For tracking expert usage statistics\n",
        "        self.register_buffer('expert_usage_counts', torch.zeros(n_experts))\n",
        "        self.register_buffer('total_tokens_processed', torch.tensor(0.0))\n",
        "\n",
        "    def add_noise(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add noise to gate logits for better exploration.\"\"\"\n",
        "        if self.training and self.gate_noise > 0:\n",
        "            noise = torch.randn_like(logits) * self.gate_noise\n",
        "            return logits + noise\n",
        "        return logits\n",
        "\n",
        "    def compute_load_balancing_loss(\n",
        "        self,\n",
        "        gate_logits: torch.Tensor,\n",
        "        selected_experts: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute load balancing loss to encourage even expert usage.\n",
        "\n",
        "        Args:\n",
        "            gate_logits: Raw gate logits [batch_size * seq_len, n_experts]\n",
        "            selected_experts: Selected expert indices [batch_size * seq_len, top_k]\n",
        "\n",
        "        Returns:\n",
        "            Load balancing loss scalar\n",
        "        \"\"\"\n",
        "        # Compute gate probabilities\n",
        "        gate_probs = F.softmax(gate_logits, dim=-1)  # [B*T, n_experts]\n",
        "\n",
        "        # Fraction of tokens assigned to each expert\n",
        "        expert_mask = F.one_hot(selected_experts, num_classes=self.n_experts).float()  # [B*T, top_k, n_experts]\n",
        "        expert_assignment = expert_mask.sum(dim=1)  # [B*T, n_experts]\n",
        "        tokens_per_expert = expert_assignment.sum(dim=0)  # [n_experts]\n",
        "\n",
        "        # Normalize by total tokens\n",
        "        total_tokens = gate_logits.shape[0] * self.top_k\n",
        "        fraction_per_expert = tokens_per_expert / total_tokens\n",
        "\n",
        "        # Average gate probability for each expert\n",
        "        avg_gate_prob = gate_probs.mean(dim=0)  # [n_experts]\n",
        "\n",
        "        # Load balancing loss: minimize the dot product of these two distributions\n",
        "        # This encourages both distributions to be uniform\n",
        "        balance_loss = (fraction_per_expert * avg_gate_prob).sum() * self.n_experts\n",
        "\n",
        "        return balance_loss\n",
        "\n",
        "    def compute_capacity(self, batch_size: int, seq_len: int) -> int:\n",
        "        \"\"\"Compute expert capacity based on capacity factor.\"\"\"\n",
        "        tokens_per_expert = (batch_size * seq_len * self.top_k) / self.n_experts\n",
        "        capacity = int(tokens_per_expert * self.capacity_factor)\n",
        "        return max(capacity, 4)  # Minimum capacity\n",
        "\n",
        "    def profile_expert_timing(self, selected_experts: torch.Tensor) -> Dict:\n",
        "        \"\"\"\n",
        "        Profile per-expert timing and usage.\n",
        "\n",
        "        Args:\n",
        "            selected_experts: Selected expert indices [batch_size * seq_len, top_k]\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with profiling information\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Count expert usage\n",
        "        expert_counts = torch.zeros(self.n_experts, device=selected_experts.device)\n",
        "        for expert_id in range(self.n_experts):\n",
        "            expert_counts[expert_id] = (selected_experts == expert_id).sum().float()\n",
        "\n",
        "        # Update global statistics\n",
        "        self.expert_usage_counts += expert_counts\n",
        "        self.total_tokens_processed += selected_experts.numel()\n",
        "\n",
        "        # Compute usage statistics\n",
        "        total_assignments = expert_counts.sum()\n",
        "        expert_utilization = expert_counts / (total_assignments + 1e-8)\n",
        "\n",
        "        metrics.update({\n",
        "            'expert_usage_current': expert_counts.cpu().numpy(),\n",
        "            'expert_utilization_current': expert_utilization.cpu().numpy(),\n",
        "            'expert_usage_cumulative': self.expert_usage_counts.cpu().numpy(),\n",
        "            'total_assignments': total_assignments.item(),\n",
        "            'usage_variance': expert_utilization.var().item(),\n",
        "            'max_expert_usage': expert_utilization.max().item(),\n",
        "            'min_expert_usage': expert_utilization.min().item(),\n",
        "        })\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict:\n",
        "        \"\"\"\n",
        "        Forward pass of the router.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing routing decisions and metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Reshape for easier processing\n",
        "        x_flat = x.view(-1, d_model)  # [batch_size * seq_len, d_model]\n",
        "\n",
        "        # Timing for gate computation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            gate_start = torch.cuda.Event(enable_timing=True)\n",
        "            gate_end = torch.cuda.Event(enable_timing=True)\n",
        "            gate_start.record()\n",
        "\n",
        "        # Compute gate logits\n",
        "        gate_logits = self.gate(x_flat)  # [batch_size * seq_len, n_experts]\n",
        "\n",
        "        # Add noise for exploration\n",
        "        gate_logits = self.add_noise(gate_logits)\n",
        "\n",
        "        # Get top-k experts\n",
        "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            gate_end.record()\n",
        "            torch.cuda.synchronize()\n",
        "            gate_time = gate_start.elapsed_time(gate_end)\n",
        "        else:\n",
        "            gate_time = 0.0\n",
        "\n",
        "        # Compute routing probabilities (softmax over top-k)\n",
        "        top_k_probs = F.softmax(top_k_values, dim=-1)  # [batch_size * seq_len, top_k]\n",
        "\n",
        "        # Compute load balancing loss\n",
        "        balance_loss = self.compute_load_balancing_loss(gate_logits, top_k_indices)\n",
        "\n",
        "        # Profile expert usage\n",
        "        usage_metrics = self.profile_expert_timing(top_k_indices)\n",
        "\n",
        "        # Compute expert capacity\n",
        "        capacity = self.compute_capacity(batch_size, seq_len)\n",
        "\n",
        "        # Apply expert dropout during training\n",
        "        if self.training and self.expert_dropout > 0:\n",
        "            dropout_mask = torch.rand_like(top_k_probs) > self.expert_dropout\n",
        "            top_k_probs = top_k_probs * dropout_mask\n",
        "            # Renormalize\n",
        "            top_k_probs = top_k_probs / (top_k_probs.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        # Prepare output\n",
        "        output = {\n",
        "            'expert_indices': top_k_indices,  # [batch_size * seq_len, top_k]\n",
        "            'expert_weights': top_k_probs,    # [batch_size * seq_len, top_k]\n",
        "            'gate_logits': gate_logits,       # [batch_size * seq_len, n_experts]\n",
        "            'balance_loss': balance_loss * self.balance_loss_weight,\n",
        "            'capacity': capacity,\n",
        "            'metrics': {\n",
        "                'gate_computation_time_ms': gate_time,\n",
        "                'balance_loss_raw': balance_loss.item(),\n",
        "                'balance_loss_weighted': (balance_loss * self.balance_loss_weight).item(),\n",
        "                'capacity': capacity,\n",
        "                'avg_top_k_confidence': top_k_probs.mean().item(),\n",
        "                'gate_entropy': -F.softmax(gate_logits, dim=-1).mul(F.log_softmax(gate_logits, dim=-1)).sum(-1).mean().item(),\n",
        "                **usage_metrics,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_expert_stats(self) -> Dict:\n",
        "        \"\"\"Get comprehensive expert usage statistics.\"\"\"\n",
        "        if self.total_tokens_processed == 0:\n",
        "            return {'message': 'No tokens processed yet'}\n",
        "\n",
        "        cumulative_usage = self.expert_usage_counts / self.total_tokens_processed\n",
        "\n",
        "        return {\n",
        "            'total_tokens_processed': self.total_tokens_processed.item(),\n",
        "            'expert_usage_distribution': cumulative_usage.cpu().numpy(),\n",
        "            'usage_std': cumulative_usage.std().item(),\n",
        "            'usage_coefficient_of_variation': cumulative_usage.std().item() / (cumulative_usage.mean().item() + 1e-8),\n",
        "            'most_used_expert': cumulative_usage.argmax().item(),\n",
        "            'least_used_expert': cumulative_usage.argmin().item(),\n",
        "            'perfect_balance_target': 1.0 / self.n_experts,\n",
        "        }\n",
        "\n",
        "    def reset_stats(self):\n",
        "        \"\"\"Reset expert usage statistics.\"\"\"\n",
        "        self.expert_usage_counts.zero_()\n",
        "        self.total_tokens_processed.zero_()\n",
        "\n",
        "\n",
        "# Auxiliary router for comparison/experimentation\n",
        "class SwitchRouter(TopKRouter):\n",
        "    \"\"\"\n",
        "    Switch Transformer style router (top-1 with capacity dropping).\n",
        "    Inherits from TopKRouter but modifies behavior for top-1 routing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, n_experts: int, **kwargs):\n",
        "        # Force top_k = 1 for Switch routing\n",
        "        super().__init__(d_model, n_experts, top_k=1, **kwargs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict:\n",
        "        \"\"\"Switch-style routing with capacity dropping.\"\"\"\n",
        "        # Get base routing decisions\n",
        "        output = super().forward(x)\n",
        "\n",
        "        # For Switch, we need to implement capacity dropping\n",
        "        batch_size, seq_len = x.shape[:2]\n",
        "        capacity = output['capacity']\n",
        "        expert_indices = output['expert_indices'].squeeze(-1)  # [batch_size * seq_len]\n",
        "\n",
        "        # Count tokens per expert\n",
        "        expert_counts = torch.zeros(self.n_experts, device=x.device)\n",
        "        for expert_id in range(self.n_experts):\n",
        "            expert_counts[expert_id] = (expert_indices == expert_id).sum()\n",
        "\n",
        "        # Create capacity mask\n",
        "        capacity_mask = torch.ones_like(expert_indices, dtype=torch.bool)\n",
        "\n",
        "        for expert_id in range(self.n_experts):\n",
        "            expert_tokens = (expert_indices == expert_id).nonzero(as_tuple=True)[0]\n",
        "            if len(expert_tokens) > capacity:\n",
        "                # Randomly drop tokens exceeding capacity\n",
        "                dropped_indices = expert_tokens[capacity:]\n",
        "                capacity_mask[dropped_indices] = False\n",
        "\n",
        "        # Update metrics\n",
        "        tokens_dropped = (~capacity_mask).sum().item()\n",
        "        output['metrics']['tokens_dropped'] = tokens_dropped\n",
        "        output['metrics']['drop_rate'] = tokens_dropped / expert_indices.numel()\n",
        "        output['capacity_mask'] = capacity_mask\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the router\n",
        "    d_model = 256\n",
        "    n_experts = 8\n",
        "    top_k = 2\n",
        "    batch_size = 4\n",
        "    seq_len = 32\n",
        "\n",
        "    router = TopKRouter(\n",
        "        d_model=d_model,\n",
        "        n_experts=n_experts,\n",
        "        top_k=top_k,\n",
        "        capacity_factor=1.5,\n",
        "        balance_loss_weight=0.01,\n",
        "    )\n",
        "\n",
        "    # Test input\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = router(x)\n",
        "\n",
        "    print(\"Router output keys:\", output.keys())\n",
        "    print(f\"Expert indices shape: {output['expert_indices'].shape}\")\n",
        "    print(f\"Expert weights shape: {output['expert_weights'].shape}\")\n",
        "    print(f\"Balance loss: {output['balance_loss'].item():.6f}\")\n",
        "    print(f\"Expert capacity: {output['capacity']}\")\n",
        "\n",
        "    print(\"\\nMetrics:\")\n",
        "    for key, value in output['metrics'].items():\n",
        "        if isinstance(value, (list, np.ndarray)):\n",
        "            print(f\"  {key}: {np.array(value)}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"\\nExpert statistics:\")\n",
        "    stats = router.get_expert_stats()\n",
        "    for key, value in stats.items():\n",
        "        if isinstance(value, (list, np.ndarray)):\n",
        "            print(f\"  {key}: {np.array(value)}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Test Switch router\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing Switch Router\")\n",
        "\n",
        "    switch_router = SwitchRouter(d_model=d_model, n_experts=n_experts)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        switch_output = switch_router(x)\n",
        "\n",
        "    print(f\"Switch expert indices shape: {switch_output['expert_indices'].shape}\")\n",
        "    print(f\"Tokens dropped: {switch_output['metrics']['tokens_dropped']}\")\n",
        "    print(f\"Drop rate: {switch_output['metrics']['drop_rate']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gvrgY9W5vXT",
        "outputId": "64b33064-41b1-4056-df50-48da389f7044"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Router output keys: dict_keys(['expert_indices', 'expert_weights', 'gate_logits', 'balance_loss', 'capacity', 'metrics'])\n",
            "Expert indices shape: torch.Size([128, 2])\n",
            "Expert weights shape: torch.Size([128, 2])\n",
            "Balance loss: 0.010017\n",
            "Expert capacity: 48\n",
            "\n",
            "Metrics:\n",
            "  gate_computation_time_ms: 0.3401919901371002\n",
            "  balance_loss_raw: 1.0016876459121704\n",
            "  balance_loss_weighted: 0.01001687627285719\n",
            "  capacity: 48\n",
            "  avg_top_k_confidence: 0.5\n",
            "  gate_entropy: 2.0340373516082764\n",
            "  expert_usage_current: [30. 27. 33. 30. 34. 38. 30. 34.]\n",
            "  expert_utilization_current: [0.1171875  0.10546875 0.12890625 0.1171875  0.1328125  0.1484375\n",
            " 0.1171875  0.1328125 ]\n",
            "  expert_usage_cumulative: [30. 27. 33. 30. 34. 38. 30. 34.]\n",
            "  total_assignments: 256.0\n",
            "  usage_variance: 0.0001787458168109879\n",
            "  max_expert_usage: 0.1484375\n",
            "  min_expert_usage: 0.10546875\n",
            "\n",
            "Expert statistics:\n",
            "  total_tokens_processed: 256.0\n",
            "  expert_usage_distribution: [0.1171875  0.10546875 0.12890625 0.1171875  0.1328125  0.1484375\n",
            " 0.1171875  0.1328125 ]\n",
            "  usage_std: 0.013369585387408733\n",
            "  usage_coefficient_of_variation: 0.10695667454273591\n",
            "  most_used_expert: 5\n",
            "  least_used_expert: 1\n",
            "  perfect_balance_target: 0.125\n",
            "\n",
            "==================================================\n",
            "Testing Switch Router\n",
            "Switch expert indices shape: torch.Size([128, 1])\n",
            "Tokens dropped: 2\n",
            "Drop rate: 0.016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple, Optional, List, Any\n",
        "import time\n",
        "import numpy as np\n",
        "# from fairscale.nn.moe import MOELayer # Removing fairscale dependency\n",
        "\n",
        "class SimpleMoELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified, non-distributed MoE layer for profiling.\n",
        "    Handles top-k routing, expert dispatch, and output combining.\n",
        "    \"\"\"\n",
        "    def __init__(self, gate: nn.Module, experts: nn.ModuleList, top_k: int = 2, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.gate = gate # The gating network (e.g., a Linear layer)\n",
        "        self.experts = experts # A ModuleList of expert networks\n",
        "        self.n_experts = len(experts)\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor # Keep for potential future use/comparison, not strictly used in this simple version\n",
        "\n",
        "        if top_k > self.n_experts:\n",
        "            raise ValueError(f\"top_k ({top_k}) cannot be greater than n_experts ({self.n_experts})\")\n",
        "\n",
        "        # Dictionary to store accumulated expert timings\n",
        "        self.expert_timings: Dict[int, float] = {}\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass of the simple MoE layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size * seq_len, d_model] (assuming flattened input)\n",
        "\n",
        "        Returns:\n",
        "            output: Output tensor [batch_size * seq_len, d_model]\n",
        "            aux_loss: Load balancing loss scalar\n",
        "            metrics: Dictionary containing routing metrics\n",
        "        \"\"\"\n",
        "        # Input shape is expected to be [num_tokens, d_model]\n",
        "        num_tokens, d_model = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Compute gate logits\n",
        "        gate_logits = self.gate(x)  # [num_tokens, n_experts]\n",
        "\n",
        "        # Get top-k experts and their probabilities\n",
        "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1) # [num_tokens, top_k]\n",
        "        top_k_probs = F.softmax(top_k_values, dim=-1) # [num_tokens, top_k]\n",
        "\n",
        "        # Compute load balancing loss (similar to the router's logic)\n",
        "        gate_probs_all = F.softmax(gate_logits, dim=-1) # [num_tokens, n_experts]\n",
        "\n",
        "        # Fraction of tokens assigned to each expert (based on top-1 for simplicity in loss)\n",
        "        # For top-k, a more accurate loss might be needed, but this is a common approximation\n",
        "        top1_indices = top_k_indices[:, 0] # [num_tokens]\n",
        "        expert_mask_top1 = F.one_hot(top1_indices, num_classes=self.n_experts).float() # [num_tokens, n_experts]\n",
        "        tokens_per_expert_for_loss = expert_mask_top1.sum(dim=0) # [n_experts]\n",
        "        total_tokens_for_loss = num_tokens # Use total tokens for normalization\n",
        "\n",
        "        # Normalize by total tokens\n",
        "        fraction_per_expert = tokens_per_expert_for_loss / (total_tokens_for_loss + 1e-8) # Add epsilon for stability\n",
        "\n",
        "        # Average gate probability for each expert (average over all tokens)\n",
        "        avg_gate_prob = gate_probs_all.mean(dim=0) # [n_experts]\n",
        "\n",
        "        # Load balancing loss\n",
        "        # This formulation encourages both fraction_per_expert and avg_gate_prob to be uniform\n",
        "        aux_loss = (fraction_per_expert * avg_gate_prob).sum() * self.n_experts\n",
        "\n",
        "\n",
        "        # Dispatch tokens to experts\n",
        "        output = torch.zeros_like(x) # [num_tokens, d_model]\n",
        "        metrics: Dict[str, Any] = {} # Use Dict[str, Any] for flexibility\n",
        "\n",
        "        # Collect metrics for profiling\n",
        "        expert_usage_counts = torch.zeros(self.n_experts, device=device)\n",
        "        expert_batch_timings: Dict[int, float] = {} # Timings for this batch\n",
        "\n",
        "        # Loop through each expert\n",
        "        for expert_id in range(self.n_experts):\n",
        "            # Find tokens routed to this expert (at any of the top-k positions)\n",
        "            expert_tokens_mask = (top_k_indices == expert_id).any(dim=-1) # [num_tokens]\n",
        "            expert_token_indices = torch.where(expert_tokens_mask)[0] # Indices of tokens for this expert\n",
        "\n",
        "            if expert_token_indices.numel() > 0:\n",
        "                # Get inputs for this expert\n",
        "                expert_input = x[expert_token_indices] # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Get weights for this expert's tokens\n",
        "                expert_weights_for_tokens = torch.zeros(expert_token_indices.numel(), device=device)\n",
        "                for i, token_idx in enumerate(expert_token_indices):\n",
        "                    pos_in_topk = torch.where(top_k_indices[token_idx] == expert_id)[0]\n",
        "                    if pos_in_topk.numel() > 0:\n",
        "                         expert_weights_for_tokens[i] = top_k_probs[token_idx, pos_in_topk].sum()\n",
        "\n",
        "\n",
        "                # Record start time for expert computation\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.synchronize()\n",
        "                    start_event = torch.cuda.Event(enable_timing=True)\n",
        "                    end_event = torch.cuda.Event(enable_timing=True)\n",
        "                    start_event.record()\n",
        "                else:\n",
        "                    start_time = time.time()\n",
        "\n",
        "\n",
        "                # Run expert forward pass\n",
        "                expert_output = self.experts[expert_id](expert_input) # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Record end time and calculate duration\n",
        "                if torch.cuda.is_available():\n",
        "                    end_event.record()\n",
        "                    torch.cuda.synchronize()\n",
        "                    duration_ms = start_event.elapsed_time(end_event)\n",
        "                else:\n",
        "                    duration_ms = (time.time() - start_time) * 1000.0 # Convert to ms\n",
        "\n",
        "                expert_batch_timings[expert_id] = duration_ms\n",
        "                self.expert_timings[expert_id] = self.expert_timings.get(expert_id, 0.0) + duration_ms # Accumulate timing\n",
        "\n",
        "\n",
        "                # Weight expert output by the routing probability and scatter back\n",
        "                weighted_expert_output = expert_output * expert_weights_for_tokens.unsqueeze(-1)\n",
        "\n",
        "                # Scatter weighted output back to the original token positions\n",
        "                output[expert_token_indices] += weighted_expert_output\n",
        "\n",
        "                # Update usage counts\n",
        "                expert_usage_counts[expert_id] = expert_token_indices.numel()\n",
        "\n",
        "\n",
        "        # Add metrics to the dictionary\n",
        "        metrics['expert_usage_current'] = expert_usage_counts.cpu().numpy()\n",
        "        metrics['total_assignments'] = expert_usage_counts.sum().item() # Total tokens dispatched\n",
        "        metrics['expert_batch_timings_ms'] = expert_batch_timings\n",
        "        metrics['expert_cumulative_timings_ms'] = self.expert_timings\n",
        "\n",
        "\n",
        "        return output, aux_loss, metrics\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, n_experts: int = 8,\n",
        "                 top_k: int = 2, dropout: float = 0.1, use_moe: bool = True, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.use_moe = use_moe\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        if use_moe:\n",
        "            # Gating network for the MoE layer\n",
        "            gate_layer = nn.Linear(d_model, n_experts, bias=False)\n",
        "             # Initialize gate weights\n",
        "            nn.init.normal_(gate_layer.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "            # Create a ModuleList with n_experts distinct instances of the expert_module\n",
        "            experts_list = nn.ModuleList([\n",
        "                nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(dropout),\n",
        "                                   nn.Linear(d_ff, d_model)) for _ in range(n_experts)\n",
        "            ])\n",
        "\n",
        "            self.moe_layer = SimpleMoELayer(\n",
        "                gate=gate_layer,\n",
        "                experts=experts_list,\n",
        "                top_k=top_k,\n",
        "                capacity_factor=capacity_factor # Not used in SimpleMoELayer but kept for compatibility\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                              nn.ReLU(),\n",
        "                                              nn.Dropout(dropout),\n",
        "                                              nn.Linear(d_ff, d_model))\n",
        "\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.expert_timings = {} # Keep for potential future profiling\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass with optional profiling.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "            mask: Attention mask\n",
        "            profile: Whether to collect timing information\n",
        "\n",
        "        Returns:\n",
        "            output: Transformed tensor\n",
        "            metrics: Dictionary containing routing metrics and timings\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Self-attention\n",
        "        residual = x\n",
        "        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(residual + self.dropout(attn_out))\n",
        "\n",
        "        # MoE or FFN\n",
        "        residual = x\n",
        "\n",
        "        if self.use_moe:\n",
        "            # Reshape input for MoE layer: [batch_size * seq_len, d_model]\n",
        "            batch_size, seq_len, d_model = x.shape\n",
        "            x_flat = x.view(-1, d_model)\n",
        "\n",
        "            # Profile MoE forward pass\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "\n",
        "            # MoE forward pass using SimpleMoELayer\n",
        "            moe_out_flat, aux_loss, moe_metrics = self.moe_layer(x_flat)\n",
        "\n",
        "            if profile and torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                total_time = start_event.elapsed_time(end_event)\n",
        "                metrics['moe_forward_time_ms'] = total_time\n",
        "                metrics.update(moe_metrics) # Add simple moe metrics\n",
        "\n",
        "            # Reshape output back to [batch_size, seq_len, d_model]\n",
        "            moe_out = moe_out_flat.view(batch_size, seq_len, d_model)\n",
        "\n",
        "            x = residual + self.dropout(moe_out)\n",
        "            metrics['aux_loss'] = aux_loss\n",
        "\n",
        "        else:\n",
        "            # Standard FFN\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "                ffn_out = self.feed_forward(x)\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                metrics['ffn_time_ms'] = start_event.elapsed_time(end_event)\n",
        "            else:\n",
        "                ffn_out = self.feed_forward(x)\n",
        "\n",
        "            x = residual + self.dropout(ffn_out)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x, metrics\n",
        "\n",
        "\n",
        "class MoETransformer(nn.Module):\n",
        "    \"\"\"Simple MoE Transformer model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 6,\n",
        "        d_ff: int = 2048,\n",
        "        n_experts: int = 8,\n",
        "        top_k: int = 2,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        use_moe_layers: Optional[list] = None,  # Which layers use MoE\n",
        "        capacity_factor: float = 1.25,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Determine which layers use MoE\n",
        "        if use_moe_layers is None:\n",
        "            # By default, use MoE in every other layer starting from layer 1\n",
        "            use_moe_layers = [i % 2 == 1 for i in range(n_layers)]\n",
        "        elif len(use_moe_layers) != n_layers:\n",
        "             raise ValueError(f\"Length of use_moe_layers ({len(use_moe_layers)}) must match n_layers ({n_layers})\")\n",
        "\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                d_ff=d_ff,\n",
        "                n_experts=n_experts,\n",
        "                top_k=top_k,\n",
        "                dropout=dropout,\n",
        "                use_moe=use_moe_layers[i],\n",
        "                capacity_factor=capacity_factor,\n",
        "            )\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token indices [batch_size, seq_len]\n",
        "            attention_mask: Attention mask [batch_size, seq_len]\n",
        "            profile: Whether to collect profiling information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing logits, aux_loss, and optional metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(0, seq_len, device=device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "\n",
        "        # Attention mask for causal modeling\n",
        "        if attention_mask is None:\n",
        "            # Create causal mask\n",
        "            causal_mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
        "            ).bool()\n",
        "        else:\n",
        "            causal_mask = attention_mask\n",
        "\n",
        "        total_aux_loss = 0.0\n",
        "        all_metrics = {} if profile else None\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Pass profiler instance to the layer if needed, or handle profiling inside layer\n",
        "            x, layer_metrics = layer(x, mask=causal_mask, profile=profile)\n",
        "\n",
        "            # Accumulate auxiliary loss from MoE layers\n",
        "            if 'aux_loss' in layer_metrics:\n",
        "                total_aux_loss += layer_metrics['aux_loss']\n",
        "\n",
        "            # Collect metrics\n",
        "            if profile:\n",
        "                for key, value in layer_metrics.items():\n",
        "                    if key != 'aux_loss':\n",
        "                        # Append metrics to lists if they exist, otherwise create\n",
        "                        metric_key = f'layer_{i}__{key}'\n",
        "                        if metric_key in all_metrics:\n",
        "                            if isinstance(all_metrics[metric_key], list):\n",
        "                                all_metrics[metric_key].append(value)\n",
        "                            else: # Convert to list if first time appending\n",
        "                                all_metrics[metric_key] = [all_metrics[metric_key], value]\n",
        "                        else:\n",
        "                            all_metrics[metric_key] = value\n",
        "\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        output = {\n",
        "            'logits': logits,\n",
        "            'aux_loss': total_aux_loss,\n",
        "        }\n",
        "\n",
        "        if profile:\n",
        "            output['metrics'] = all_metrics\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model\n",
        "    model = MoETransformer(\n",
        "        vocab_size=1000,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        n_layers=4,\n",
        "        d_ff=1024,\n",
        "        n_experts=4,\n",
        "        top_k=2,\n",
        "        use_moe_layers=[False, True, False, True],  # MoE in layers 1 and 3\n",
        "    )\n",
        "\n",
        "    # Test input\n",
        "    batch_size, seq_len = 2, 32\n",
        "    input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
        "\n",
        "    # Forward pass with profiling\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, profile=True)\n",
        "\n",
        "    print(f\"Output logits shape: {output['logits'].shape}\")\n",
        "    print(f\"Auxiliary loss: {output['aux_loss'].item():.6f}\")\n",
        "\n",
        "    if 'metrics' in output:\n",
        "        print(\"\\nProfiling metrics:\")\n",
        "        for key, value in output['metrics'].items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"  {key}: {value.shape}\")\n",
        "            elif isinstance(value, list):\n",
        "                 print(f\"  {key}: {value}\") # Print list content\n",
        "            elif isinstance(value, dict):\n",
        "                 print(f\"  {key}: {value}\") # Print dict content\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFxw54dM6hoX",
        "outputId": "35fd40d2-0c66-498e-e966-84848a3db151"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits shape: torch.Size([2, 32, 1000])\n",
            "Auxiliary loss: 2.034574\n",
            "\n",
            "Profiling metrics:\n",
            "  layer_0__ffn_time_ms: 2.3575680255889893\n",
            "  layer_1__moe_forward_time_ms: 9.569855690002441\n",
            "  layer_1__expert_usage_current: [28. 31. 38. 31.]\n",
            "  layer_1__total_assignments: 128.0\n",
            "  layer_1__expert_batch_timings_ms: {0: 0.7941120266914368, 1: 0.9747200012207031, 2: 0.9547520279884338, 3: 0.8738560080528259}\n",
            "  layer_1__expert_cumulative_timings_ms: {0: 0.7941120266914368, 1: 0.9747200012207031, 2: 0.9547520279884338, 3: 0.8738560080528259}\n",
            "  layer_2__ffn_time_ms: 1.3054399490356445\n",
            "  layer_3__moe_forward_time_ms: 9.73363208770752\n",
            "  layer_3__expert_usage_current: [28. 30. 24. 46.]\n",
            "  layer_3__total_assignments: 128.0\n",
            "  layer_3__expert_batch_timings_ms: {0: 0.9148160219192505, 1: 0.9413120150566101, 2: 0.8222399950027466, 3: 1.049407958984375}\n",
            "  layer_3__expert_cumulative_timings_ms: {0: 0.9148160219192505, 1: 0.9413120150566101, 2: 0.8222399950027466, 3: 1.049407958984375}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d0c412d",
        "outputId": "61cd1c53-d824-4f6d-a195-553c13f329f7"
      },
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "\n",
        "# Set environment variables for distributed training\n",
        "# In a real distributed setup, these would be set differently on each node/process\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29500' # Use a free port\n",
        "os.environ['RANK'] = '0' # Rank of the current process\n",
        "os.environ['WORLD_SIZE'] = '1' # Total number of processes\n",
        "\n",
        "# Initialize the distributed environment\n",
        "if not dist.is_initialized():\n",
        "    dist.init_process_group(\"nccl\" if torch.cuda.is_available() else \"gloo\")\n",
        "    print(f\"Distributed environment initialized with backend: {dist.get_backend()}\")\n",
        "    print(f\"Rank {dist.get_rank()} out of {dist.get_world_size()} processes\")\n",
        "else:\n",
        "    print(\"Distributed environment already initialized.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distributed environment already initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf69bb6e"
      },
      "source": [
        "# Task\n",
        "Integrate GPU profiling and thermal signal generation into the provided model evaluation code. Modify the `SimpleMoELayer` and `TransformerBlock` to collect detailed expert usage metrics. Update the evaluation loop to use `GPUProfiler` and `ThermalSignalGenerator`, collect metrics, and demonstrate basic analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b782521"
      },
      "source": [
        "## Refine `simplemoelayer` for detailed profiling\n",
        "\n",
        "### Subtask:\n",
        "Modify the `SimpleMoELayer` class to record the execution time and token counts for each expert it dispatches to.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a2c41fd"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `SimpleMoELayer` class to include expert timing and token count collection within its forward pass, storing this information in the returned metrics dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232fff34",
        "outputId": "ca940767-8257-44bd-aa55-eda84289f8eb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class SimpleMoELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified, non-distributed MoE layer for profiling.\n",
        "    Handles top-k routing, expert dispatch, and output combining.\n",
        "    \"\"\"\n",
        "    def __init__(self, gate: nn.Module, experts: nn.ModuleList, top_k: int = 2, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.gate = gate # The gating network (e.g., a Linear layer)\n",
        "        self.experts = experts # A ModuleList of expert networks\n",
        "        self.n_experts = len(experts)\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor # Keep for potential future use/comparison, not strictly used in this simple version\n",
        "\n",
        "        if top_k > self.n_experts:\n",
        "            raise ValueError(f\"top_k ({top_k}) cannot be greater than n_experts ({self.n_experts})\")\n",
        "\n",
        "        # Dictionary to store accumulated expert timings\n",
        "        self.expert_timings: Dict[int, float] = {}\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass of the simple MoE layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size * seq_len, d_model] (assuming flattened input)\n",
        "\n",
        "        Returns:\n",
        "            output: Output tensor [batch_size * seq_len, d_model]\n",
        "            aux_loss: Load balancing loss scalar\n",
        "            metrics: Dictionary containing routing metrics\n",
        "        \"\"\"\n",
        "        # Input shape is expected to be [num_tokens, d_model]\n",
        "        num_tokens, d_model = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Compute gate logits\n",
        "        gate_logits = self.gate(x)  # [num_tokens, n_experts]\n",
        "\n",
        "        # Get top-k experts and their probabilities\n",
        "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1) # [num_tokens, top_k]\n",
        "        top_k_probs = F.softmax(top_k_values, dim=-1) # [num_tokens, top_k]\n",
        "\n",
        "        # Compute load balancing loss (similar to the router's logic)\n",
        "        gate_probs_all = F.softmax(gate_logits, dim=-1) # [num_tokens, n_experts]\n",
        "\n",
        "        # Fraction of tokens assigned to each expert (based on top-1 for simplicity in loss)\n",
        "        # For top-k, a more accurate loss might be needed, but this is a common approximation\n",
        "        top1_indices = top_k_indices[:, 0] # [num_tokens]\n",
        "        expert_mask_top1 = F.one_hot(top1_indices, num_classes=self.n_experts).float() # [num_tokens, n_experts]\n",
        "        tokens_per_expert_for_loss = expert_mask_top1.sum(dim=0) # [n_experts]\n",
        "        total_tokens_for_loss = num_tokens # Use total tokens for normalization\n",
        "\n",
        "        # Normalize by total tokens\n",
        "        fraction_per_expert = tokens_per_expert_for_loss / (total_tokens_for_loss + 1e-8) # Add epsilon for stability\n",
        "\n",
        "        # Average gate probability for each expert (average over all tokens)\n",
        "        avg_gate_prob = gate_probs_all.mean(dim=0) # [n_experts]\n",
        "\n",
        "        # Load balancing loss\n",
        "        # This formulation encourages both fraction_per_expert and avg_gate_prob to be uniform\n",
        "        aux_loss = (fraction_per_expert * avg_gate_prob).sum() * self.n_experts\n",
        "\n",
        "\n",
        "        # Dispatch tokens to experts\n",
        "        output = torch.zeros_like(x) # [num_tokens, d_model]\n",
        "        metrics: Dict[str, Any] = {} # Use Dict[str, Any] for flexibility\n",
        "\n",
        "        # Collect metrics for profiling\n",
        "        expert_usage_counts = torch.zeros(self.n_experts, device=device)\n",
        "        expert_batch_timings: Dict[int, float] = {} # Timings for this batch\n",
        "\n",
        "        # Loop through each expert\n",
        "        for expert_id in range(self.n_experts):\n",
        "            # Find tokens routed to this expert (at any of the top-k positions)\n",
        "            expert_tokens_mask = (top_k_indices == expert_id).any(dim=-1) # [num_tokens]\n",
        "            expert_token_indices = torch.where(expert_tokens_mask)[0] # Indices of tokens for this expert\n",
        "\n",
        "            if expert_token_indices.numel() > 0:\n",
        "                # Get inputs for this expert\n",
        "                expert_input = x[expert_token_indices] # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Get weights for this expert's tokens\n",
        "                expert_weights_for_tokens = torch.zeros(expert_token_indices.numel(), device=device)\n",
        "                for i, token_idx in enumerate(expert_token_indices):\n",
        "                    pos_in_topk = torch.where(top_k_indices[token_idx] == expert_id)[0]\n",
        "                    if pos_in_topk.numel() > 0:\n",
        "                         expert_weights_for_tokens[i] = top_k_probs[token_idx, pos_in_topk].sum()\n",
        "\n",
        "\n",
        "                # Record start time for expert computation\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.synchronize()\n",
        "                    start_event = torch.cuda.Event(enable_timing=True)\n",
        "                    end_event = torch.cuda.Event(enable_timing=True)\n",
        "                    start_event.record()\n",
        "                else:\n",
        "                    start_time = time.time()\n",
        "\n",
        "\n",
        "                # Run expert forward pass\n",
        "                expert_output = self.experts[expert_id](expert_input) # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Record end time and calculate duration\n",
        "                if torch.cuda.is_available():\n",
        "                    end_event.record()\n",
        "                    torch.cuda.synchronize()\n",
        "                    duration_ms = start_event.elapsed_time(end_event)\n",
        "                else:\n",
        "                    duration_ms = (time.time() - start_time) * 1000.0 # Convert to ms\n",
        "\n",
        "                expert_batch_timings[expert_id] = duration_ms\n",
        "                self.expert_timings[expert_id] = self.expert_timings.get(expert_id, 0.0) + duration_ms # Accumulate timing\n",
        "\n",
        "\n",
        "                # Weight expert output by the routing probability and scatter back\n",
        "                weighted_expert_output = expert_output * expert_weights_for_tokens.unsqueeze(-1)\n",
        "\n",
        "                # Scatter weighted output back to the original token positions\n",
        "                output[expert_token_indices] += weighted_expert_output\n",
        "\n",
        "                # Update usage counts\n",
        "                expert_usage_counts[expert_id] = expert_token_indices.numel()\n",
        "\n",
        "\n",
        "        # Add metrics to the dictionary\n",
        "        metrics['expert_usage_current'] = expert_usage_counts.cpu().numpy()\n",
        "        metrics['total_assignments'] = expert_usage_counts.sum().item() # Total tokens dispatched\n",
        "        metrics['expert_batch_timings_ms'] = expert_batch_timings\n",
        "        metrics['expert_cumulative_timings_ms'] = self.expert_timings\n",
        "\n",
        "\n",
        "        return output, aux_loss, metrics\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, n_experts: int = 8,\n",
        "                 top_k: int = 2, dropout: float = 0.1, use_moe: bool = True, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.use_moe = use_moe\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        if use_moe:\n",
        "            # Gating network for the MoE layer\n",
        "            gate_layer = nn.Linear(d_model, n_experts, bias=False)\n",
        "             # Initialize gate weights\n",
        "            nn.init.normal_(gate_layer.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "            # Create a ModuleList with n_experts distinct instances of the expert_module\n",
        "            experts_list = nn.ModuleList([\n",
        "                nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(dropout),\n",
        "                                   nn.Linear(d_ff, d_model)) for _ in range(n_experts)\n",
        "            ])\n",
        "\n",
        "            self.moe_layer = SimpleMoELayer(\n",
        "                gate=gate_layer,\n",
        "                experts=experts_list,\n",
        "                top_k=top_k,\n",
        "                capacity_factor=capacity_factor # Not used in SimpleMoELayer but kept for compatibility\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                              nn.ReLU(),\n",
        "                                              nn.Dropout(dropout),\n",
        "                                              nn.Linear(d_ff, d_model))\n",
        "\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.expert_timings = {} # Keep for potential future profiling\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass with optional profiling.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "            mask: Attention mask\n",
        "            profile: Whether to collect timing information\n",
        "\n",
        "        Returns:\n",
        "            output: Transformed tensor\n",
        "            metrics: Dictionary containing routing metrics and timings\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Self-attention\n",
        "        residual = x\n",
        "        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(residual + self.dropout(attn_out))\n",
        "\n",
        "        # MoE or FFN\n",
        "        residual = x\n",
        "\n",
        "        if self.use_moe:\n",
        "            # Reshape input for MoE layer: [batch_size * seq_len, d_model]\n",
        "            batch_size, seq_len, d_model = x.shape\n",
        "            x_flat = x.view(-1, d_model)\n",
        "\n",
        "            # Profile MoE forward pass\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "\n",
        "            # MoE forward pass using SimpleMoELayer\n",
        "            moe_out_flat, aux_loss, moe_metrics = self.moe_layer(x_flat)\n",
        "\n",
        "            if profile and torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                total_time = start_event.elapsed_time(end_event)\n",
        "                metrics['moe_forward_time_ms'] = total_time\n",
        "                metrics.update(moe_metrics) # Add simple moe metrics\n",
        "\n",
        "            # Reshape output back to [batch_size, seq_len, d_model]\n",
        "            moe_out = moe_out_flat.view(batch_size, seq_len, d_model)\n",
        "\n",
        "            x = residual + self.dropout(moe_out)\n",
        "            metrics['aux_loss'] = aux_loss\n",
        "\n",
        "        else:\n",
        "            # Standard FFN\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "                ffn_out = self.feed_forward(x)\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                metrics['ffn_time_ms'] = start_event.elapsed_time(end_event)\n",
        "            else:\n",
        "                ffn_out = self.feed_forward(x)\n",
        "\n",
        "            x = residual + self.dropout(ffn_out)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x, metrics\n",
        "\n",
        "\n",
        "class MoETransformer(nn.Module):\n",
        "    \"\"\"Simple MoE Transformer model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 6,\n",
        "        d_ff: int = 2048,\n",
        "        n_experts: int = 8,\n",
        "        top_k: int = 2,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        use_moe_layers: Optional[list] = None,  # Which layers use MoE\n",
        "        capacity_factor: float = 1.25,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Determine which layers use MoE\n",
        "        if use_moe_layers is None:\n",
        "            # By default, use MoE in every other layer starting from layer 1\n",
        "            use_moe_layers = [i % 2 == 1 for i in range(n_layers)]\n",
        "        elif len(use_moe_layers) != n_layers:\n",
        "             raise ValueError(f\"Length of use_moe_layers ({len(use_moe_layers)}) must match n_layers ({n_layers})\")\n",
        "\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                d_ff=d_ff,\n",
        "                n_experts=n_experts,\n",
        "                top_k=top_k,\n",
        "                dropout=dropout,\n",
        "                use_moe=use_moe_layers[i],\n",
        "                capacity_factor=capacity_factor,\n",
        "            )\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token indices [batch_size, seq_len]\n",
        "            attention_mask: Attention mask [batch_size, seq_len]\n",
        "            profile: Whether to collect profiling information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing logits, aux_loss, and optional metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(0, seq_len, device=device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "\n",
        "        # Attention mask for causal modeling\n",
        "        if attention_mask is None:\n",
        "            # Create causal mask\n",
        "            causal_mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
        "            ).bool()\n",
        "        else:\n",
        "            causal_mask = attention_mask\n",
        "\n",
        "        total_aux_loss = 0.0\n",
        "        all_metrics = {} if profile else None\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Pass profiler instance to the layer if needed, or handle profiling inside layer\n",
        "            x, layer_metrics = layer(x, mask=causal_mask, profile=profile)\n",
        "\n",
        "            # Accumulate auxiliary loss from MoE layers\n",
        "            if 'aux_loss' in layer_metrics:\n",
        "                total_aux_loss += layer_metrics['aux_loss']\n",
        "\n",
        "            # Collect metrics\n",
        "            if profile:\n",
        "                for key, value in layer_metrics.items():\n",
        "                    if key != 'aux_loss':\n",
        "                        # Append metrics to lists if they exist, otherwise create\n",
        "                        metric_key = f'layer_{i}__{key}'\n",
        "                        if metric_key in all_metrics:\n",
        "                            if isinstance(all_metrics[metric_key], list) or isinstance(all_metrics[metric_key], dict):\n",
        "                                all_metrics[metric_key].append(value)\n",
        "                            else: # Convert to list if first time appending\n",
        "                                all_metrics[metric_key] = [all_metrics[metric_key], value]\n",
        "                        else:\n",
        "                            all_metrics[metric_key] = value\n",
        "\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        output = {\n",
        "            'logits': logits,\n",
        "            'aux_loss': total_aux_loss,\n",
        "        }\n",
        "\n",
        "        if profile:\n",
        "            output['metrics'] = all_metrics\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage and testing (optional, moved from main execution block)\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model\n",
        "    model = MoETransformer(\n",
        "        vocab_size=1000,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        n_layers=4,\n",
        "        d_ff=1024,\n",
        "        n_experts=4,\n",
        "        top_k=2,\n",
        "        use_moe_layers=[False, True, False, True],  # MoE in layers 1 and 3\n",
        "    )\n",
        "\n",
        "    # Test input\n",
        "    batch_size, seq_len = 2, 32\n",
        "    input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
        "\n",
        "    # Forward pass with profiling\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, profile=True)\n",
        "\n",
        "    print(f\"Output logits shape: {output['logits'].shape}\")\n",
        "    print(f\"Auxiliary loss: {output['aux_loss'].item():.6f}\")\n",
        "\n",
        "    if 'metrics' in output:\n",
        "        print(\"\\nProfiling metrics:\")\n",
        "        for key, value in output['metrics'].items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"  {key}: {value.shape}\")\n",
        "            elif isinstance(value, list):\n",
        "                 print(f\"  {key}: {value}\") # Print list content\n",
        "            elif isinstance(value, dict):\n",
        "                 print(f\"  {key}: {value}\") # Print dict content\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits shape: torch.Size([2, 32, 1000])\n",
            "Auxiliary loss: 2.008421\n",
            "\n",
            "Profiling metrics:\n",
            "  layer_0__ffn_time_ms: 1.5088000297546387\n",
            "  layer_1__moe_forward_time_ms: 9.496352195739746\n",
            "  layer_1__expert_usage_current: [30. 33. 32. 33.]\n",
            "  layer_1__total_assignments: 128.0\n",
            "  layer_1__expert_batch_timings_ms: {0: 0.8581439852714539, 1: 0.942080020904541, 2: 0.9077439904212952, 3: 0.8822720050811768}\n",
            "  layer_1__expert_cumulative_timings_ms: {0: 0.8581439852714539, 1: 0.942080020904541, 2: 0.9077439904212952, 3: 0.8822720050811768}\n",
            "  layer_2__ffn_time_ms: 1.1815999746322632\n",
            "  layer_3__moe_forward_time_ms: 9.417920112609863\n",
            "  layer_3__expert_usage_current: [25. 34. 35. 34.]\n",
            "  layer_3__total_assignments: 128.0\n",
            "  layer_3__expert_batch_timings_ms: {0: 0.8087360262870789, 1: 0.9081599712371826, 2: 0.9254720211029053, 3: 0.9127039909362793}\n",
            "  layer_3__expert_cumulative_timings_ms: {0: 0.8087360262870789, 1: 0.9081599712371826, 2: 0.9254720211029053, 3: 0.9127039909362793}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02d4f90"
      },
      "source": [
        "**Reasoning**:\n",
        "The `SimpleMoELayer` class has been successfully modified to include expert timing and token counts in the metrics. The next step is to update the evaluation loop to utilize the `GPUProfiler` and `ThermalSignalGenerator` and collect the detailed metrics from the model's forward pass. This involves instantiating the `ThermalSignalGenerator` and modifying the `evaluate_model` function to get thermal signals and aggregate the detailed MoE metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71f5b19"
      },
      "source": [
        "**Reasoning**:\n",
        "The ThermalSignalGenerator failed to load the cost table because the file does not exist. Create a dummy `energy/cost_table.json` file with default values to allow the `ThermalSignalGenerator` to initialize successfully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "192c7ae4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dummy cost table exists, the `ThermalSignalGenerator` should be able to initialize correctly. The next step is to rerun the evaluation loop code to integrate the GPU profiling and thermal signal generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415e0de5"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a `NameError` because `avg_inference_time_per_batch_ms` was used in the logging message outside the scope where it was defined within the `evaluate_model` function. This variable is part of the `results` dictionary returned by the function. Access the variable from the `baseline_results` dictionary instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb14bed"
      },
      "source": [
        "## Update `transformerblock` to pass detailed metrics\n",
        "\n",
        "### Subtask:\n",
        "Update the `TransformerBlock` to collect the detailed expert metrics from the `SimpleMoELayer` and include them in the `layer_metrics` dictionary returned by its `forward` method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dd30765"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the TransformerBlock's forward method to include the detailed expert metrics from the SimpleMoELayer in its returned metrics dictionary. The previous subtask modified the SimpleMoELayer to return these metrics, and this step ensures they are propagated correctly through the TransformerBlock.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db114366"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple, Optional, List, Any\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class SimpleMoELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified, non-distributed MoE layer for profiling.\n",
        "    Handles top-k routing, expert dispatch, and output combining.\n",
        "    \"\"\"\n",
        "    def __init__(self, gate: nn.Module, experts: nn.ModuleList, top_k: int = 2, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.gate = gate # The gating network (e.g., a Linear layer)\n",
        "        self.experts = experts # A ModuleList of expert networks\n",
        "        self.n_experts = len(experts)\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor # Keep for potential future use/comparison, not strictly used in this simple version\n",
        "\n",
        "        if top_k > self.n_experts:\n",
        "            raise ValueError(f\"top_k ({top_k}) cannot be greater than n_experts ({self.n_experts})\")\n",
        "\n",
        "        # Dictionary to store accumulated expert timings\n",
        "        self.expert_timings: Dict[int, float] = {}\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass of the simple MoE layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size * seq_len, d_model] (assuming flattened input)\n",
        "\n",
        "        Returns:\n",
        "            output: Output tensor [batch_size * seq_len, d_model]\n",
        "            aux_loss: Load balancing loss scalar\n",
        "            metrics: Dictionary containing routing metrics\n",
        "        \"\"\"\n",
        "        # Input shape is expected to be [num_tokens, d_model]\n",
        "        num_tokens, d_model = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Compute gate logits\n",
        "        gate_logits = self.gate(x)  # [num_tokens, n_experts]\n",
        "\n",
        "        # Get top-k experts and their probabilities\n",
        "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1) # [num_tokens, top_k]\n",
        "        top_k_probs = F.softmax(top_k_values, dim=-1) # [num_tokens, top_k]\n",
        "\n",
        "        # Compute load balancing loss (similar to the router's logic)\n",
        "        gate_probs_all = F.softmax(gate_logits, dim=-1) # [num_tokens, n_experts]\n",
        "\n",
        "        # Fraction of tokens assigned to each expert (based on top-1 for simplicity in loss)\n",
        "        # For top-k, a more accurate loss might be needed, but this is a common approximation\n",
        "        top1_indices = top_k_indices[:, 0] # [num_tokens]\n",
        "        expert_mask_top1 = F.one_hot(top1_indices, num_classes=self.n_experts).float() # [num_tokens, n_experts]\n",
        "        tokens_per_expert_for_loss = expert_mask_top1.sum(dim=0) # [n_experts]\n",
        "        total_tokens_for_loss = num_tokens # Use total tokens for normalization\n",
        "\n",
        "        # Normalize by total tokens\n",
        "        fraction_per_expert = tokens_per_expert_for_loss / (total_tokens_for_loss + 1e-8) # Add epsilon for stability\n",
        "\n",
        "        # Average gate probability for each expert (average over all tokens)\n",
        "        avg_gate_prob = gate_probs_all.mean(dim=0) # [n_experts]\n",
        "\n",
        "        # Load balancing loss\n",
        "        # This formulation encourages both fraction_per_expert and avg_gate_prob to be uniform\n",
        "        aux_loss = (fraction_per_expert * avg_gate_prob).sum() * self.n_experts\n",
        "\n",
        "\n",
        "        # Dispatch tokens to experts\n",
        "        output = torch.zeros_like(x) # [num_tokens, d_model]\n",
        "        metrics: Dict[str, Any] = {} # Use Dict[str, Any] for flexibility\n",
        "\n",
        "        # Collect metrics for profiling\n",
        "        expert_usage_counts = torch.zeros(self.n_experts, device=device)\n",
        "        expert_batch_timings: Dict[int, float] = {} # Timings for this batch\n",
        "\n",
        "        # Loop through each expert\n",
        "        for expert_id in range(self.n_experts):\n",
        "            # Find tokens routed to this expert (at any of the top-k positions)\n",
        "            expert_tokens_mask = (top_k_indices == expert_id).any(dim=-1) # [num_tokens]\n",
        "            expert_token_indices = torch.where(expert_tokens_mask)[0] # Indices of tokens for this expert\n",
        "\n",
        "            if expert_token_indices.numel() > 0:\n",
        "                # Get inputs for this expert\n",
        "                expert_input = x[expert_token_indices] # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Get weights for this expert's tokens\n",
        "                expert_weights_for_tokens = torch.zeros(expert_token_indices.numel(), device=device)\n",
        "                for i, token_idx in enumerate(expert_token_indices):\n",
        "                    pos_in_topk = torch.where(top_k_indices[token_idx] == expert_id)[0]\n",
        "                    if pos_in_topk.numel() > 0:\n",
        "                         expert_weights_for_tokens[i] = top_k_probs[token_idx, pos_in_topk].sum()\n",
        "\n",
        "\n",
        "                # Record start time for expert computation\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.synchronize()\n",
        "                    start_event = torch.cuda.Event(enable_timing=True)\n",
        "                    end_event = torch.cuda.Event(enable_timing=True)\n",
        "                    start_event.record()\n",
        "                else:\n",
        "                    start_time = time.time()\n",
        "\n",
        "\n",
        "                # Run expert forward pass\n",
        "                expert_output = self.experts[expert_id](expert_input) # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Record end time and calculate duration\n",
        "                if torch.cuda.is_available():\n",
        "                    end_event.record()\n",
        "                    torch.cuda.synchronize()\n",
        "                    duration_ms = start_event.elapsed_time(end_event)\n",
        "                else:\n",
        "                    duration_ms = (time.time() - start_time) * 1000.0 # Convert to ms\n",
        "\n",
        "                expert_batch_timings[expert_id] = duration_ms\n",
        "                self.expert_timings[expert_id] = self.expert_timings.get(expert_id, 0.0) + duration_ms # Accumulate timing\n",
        "\n",
        "\n",
        "                # Weight expert output by the routing probability and scatter back\n",
        "                weighted_expert_output = expert_output * expert_weights_for_tokens.unsqueeze(-1)\n",
        "\n",
        "                # Scatter weighted output back to the original token positions\n",
        "                output[expert_token_indices] += weighted_expert_output\n",
        "\n",
        "                # Update usage counts\n",
        "                expert_usage_counts[expert_id] = expert_token_indices.numel()\n",
        "\n",
        "\n",
        "        # Add metrics to the dictionary\n",
        "        metrics['expert_usage_current'] = expert_usage_counts.cpu().numpy()\n",
        "        metrics['total_assignments'] = expert_usage_counts.sum().item() # Total tokens dispatched\n",
        "        metrics['expert_batch_timings_ms'] = expert_batch_timings\n",
        "        metrics['expert_cumulative_timings_ms'] = self.expert_timings\n",
        "\n",
        "\n",
        "        return output, aux_loss, metrics\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, n_experts: int = 8,\n",
        "                 top_k: int = 2, dropout: float = 0.1, use_moe: bool = True, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.use_moe = use_moe\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        if use_moe:\n",
        "            # Gating network for the MoE layer\n",
        "            gate_layer = nn.Linear(d_model, n_experts, bias=False)\n",
        "             # Initialize gate weights\n",
        "            nn.init.normal_(gate_layer.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "            # Create a ModuleList with n_experts distinct instances of the expert_module\n",
        "            experts_list = nn.ModuleList([\n",
        "                nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(dropout),\n",
        "                                   nn.Linear(d_ff, d_model)) for _ in range(n_experts)\n",
        "            ])\n",
        "\n",
        "            self.moe_layer = SimpleMoELayer(\n",
        "                gate=gate_layer,\n",
        "                experts=experts_list,\n",
        "                top_k=top_k,\n",
        "                capacity_factor=capacity_factor # Not used in SimpleMoELayer but kept for compatibility\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                              nn.ReLU(),\n",
        "                                              nn.Dropout(dropout),\n",
        "                                              nn.Linear(d_ff, d_model))\n",
        "\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass with optional profiling.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "            mask: Attention mask\n",
        "            profile: Whether to collect timing information\n",
        "\n",
        "        Returns:\n",
        "            output: Transformed tensor\n",
        "            metrics: Dictionary containing routing metrics and timings\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Self-attention\n",
        "        residual = x\n",
        "        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(residual + self.dropout(attn_out))\n",
        "\n",
        "        # MoE or FFN\n",
        "        residual = x\n",
        "\n",
        "        if self.use_moe:\n",
        "            # Reshape input for MoE layer: [batch_size * seq_len, d_model]\n",
        "            batch_size, seq_len, d_model = x.shape\n",
        "            x_flat = x.view(-1, d_model)\n",
        "\n",
        "            # Profile MoE forward pass\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "\n",
        "            # MoE forward pass using SimpleMoELayer\n",
        "            # moe_metrics now includes expert-level timings and usage\n",
        "            moe_out_flat, aux_loss, moe_metrics = self.moe_layer(x_flat)\n",
        "\n",
        "            if profile and torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                total_time = start_event.elapsed_time(end_event)\n",
        "                metrics['moe_forward_time_ms'] = total_time\n",
        "\n",
        "            # Add all metrics from the moe_layer to the transformer block's metrics\n",
        "            metrics.update(moe_metrics)\n",
        "\n",
        "            # Reshape output back to [batch_size, seq_len, d_model]\n",
        "            moe_out = moe_out_flat.view(batch_size, seq_len, d_model)\n",
        "\n",
        "            x = residual + self.dropout(moe_out)\n",
        "            metrics['aux_loss'] = aux_loss\n",
        "\n",
        "        else:\n",
        "            # Standard FFN\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "                ffn_out = self.feed_forward(x)\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                metrics['ffn_time_ms'] = start_event.elapsed_time(end_event)\n",
        "            else:\n",
        "                ffn_out = self.feed_forward(x)\n",
        "\n",
        "            x = residual + self.dropout(ffn_out)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x, metrics\n",
        "\n",
        "\n",
        "class MoETransformer(nn.Module):\n",
        "    \"\"\"Simple MoE Transformer model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 6,\n",
        "        d_ff: int = 2048,\n",
        "        n_experts: int = 8,\n",
        "        top_k: int = 2,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        use_moe_layers: Optional[list] = None,  # Which layers use MoE\n",
        "        capacity_factor: float = 1.25,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Determine which layers use MoE\n",
        "        if use_moe_layers is None:\n",
        "            # By default, use MoE in every other layer starting from layer 1\n",
        "            use_moe_layers = [i % 2 == 1 for i in range(n_layers)]\n",
        "        elif len(use_moe_layers) != n_layers:\n",
        "             raise ValueError(f\"Length of use_moe_layers ({len(use_moe_layers)}) must match n_layers ({n_layers})\")\n",
        "\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                d_ff=d_ff,\n",
        "                n_experts=n_experts,\n",
        "                top_k=top_k,\n",
        "                dropout=dropout,\n",
        "                use_moe=use_moe_layers[i],\n",
        "                capacity_factor=capacity_factor,\n",
        "            )\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token indices [batch_size, seq_len]\n",
        "            attention_mask: Attention mask [batch_size, seq_len]\n",
        "            profile: Whether to collect profiling information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing logits, aux_loss, and optional metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(0, seq_len, device=device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "\n",
        "        # Attention mask for causal modeling\n",
        "        if attention_mask is None:\n",
        "            # Create causal mask\n",
        "            causal_mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
        "            ).bool()\n",
        "        else:\n",
        "            causal_mask = attention_mask\n",
        "\n",
        "        total_aux_loss = 0.0\n",
        "        all_metrics = {} if profile else None\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Pass profiler instance to the layer if needed, or handle profiling inside layer\n",
        "            x, layer_metrics = layer(x, mask=causal_mask, profile=profile)\n",
        "\n",
        "            # Accumulate auxiliary loss from MoE layers\n",
        "            if 'aux_loss' in layer_metrics:\n",
        "                total_aux_loss += layer_metrics['aux_loss']\n",
        "\n",
        "            # Collect metrics\n",
        "            if profile:\n",
        "                for key, value in layer_metrics.items():\n",
        "                    if key != 'aux_loss':\n",
        "                        # Append metrics to lists if they exist, otherwise create\n",
        "                        metric_key = f'layer_{i}__{key}'\n",
        "                        if metric_key in all_metrics:\n",
        "                            if isinstance(all_metrics[metric_key], list) or isinstance(all_metrics[metric_key], dict):\n",
        "                                all_metrics[metric_key].append(value)\n",
        "                            else: # Convert to list if first time appending\n",
        "                                all_metrics[metric_key] = [all_metrics[metric_key], value]\n",
        "                        else:\n",
        "                            all_metrics[metric_key] = value\n",
        "\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        output = {\n",
        "            'logits': logits,\n",
        "            'aux_loss': total_aux_loss,\n",
        "        }\n",
        "\n",
        "        if profile:\n",
        "            output['metrics'] = all_metrics\n",
        "\n",
        "        return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Define default cost table data\n",
        "default_cost_table = {\n",
        "    \"thermal_parameters\": {\n",
        "        \"base_temperature\": 45.0,\n",
        "        \"warm_temperature_threshold\": 65.0,\n",
        "        \"hot_temperature_threshold\": 83.0,\n",
        "        \"critical_temperature\": 87.0,\n",
        "        \"thermal_time_constant\": 15.0,\n",
        "        \"emergency_cooldown_duration\": 30.0\n",
        "    },\n",
        "    \"energy_budgets\": {\n",
        "        \"low_power\": {\"max_power_watts\": 200, \"max_temperature\": 75.0, \"max_energy_per_token_mj\": 5.0},\n",
        "        \"balanced\": {\"max_power_watts\": 350, \"max_temperature\": 80.0, \"max_energy_per_token_mj\": 3.0},\n",
        "        \"performance\": {\"max_power_watts\": 450, \"max_temperature\": 85.0, \"max_energy_per_token_mj\": 1.5},\n",
        "        \"emergency\": {\"max_power_watts\": 100, \"max_temperature\": 70.0, \"max_energy_per_token_mj\": 10.0} # Added emergency budget\n",
        "    },\n",
        "    \"expert_profiles\": {\n",
        "        \"expert_0\": {\"average_power_watts\": 50, \"energy_per_token_mj\": 2.0, \"thermal_impact\": 0.1},\n",
        "        \"expert_1\": {\"average_power_watts\": 70, \"energy_per_token_mj\": 2.5, \"thermal_impact\": 0.15},\n",
        "        \"expert_2\": {\"average_power_watts\": 30, \"energy_per_token_mj\": 1.0, \"thermal_impact\": 0.05},\n",
        "        \"expert_3\": {\"average_power_watts\": 60, \"energy_per_token_mj\": 2.2, \"thermal_impact\": 0.12},\n",
        "        \"expert_4\": {\"average_power_watts\": 55, \"energy_per_token_mj\": 1.8, \"thermal_impact\": 0.09},\n",
        "        \"expert_5\": {\"average_power_watts\": 75, \"energy_per_token_mj\": 2.8, \"thermal_impact\": 0.18},\n",
        "        \"expert_6\": {\"average_power_watts\": 40, \"energy_per_token_mj\": 1.2, \"thermal_impact\": 0.06},\n",
        "        \"expert_7\": {\"average_power_watts\": 65, \"energy_per_token_mj\": 2.3, \"thermal_impact\": 0.14}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the directory and file path\n",
        "file_path = Path(\"energy/cost_table.json\")\n",
        "\n",
        "# Check if the file exists\n",
        "if not file_path.exists():\n",
        "    # Create the directory if it doesn't exist\n",
        "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Write the default data to the JSON file\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(default_cost_table, f, indent=2)\n",
        "\n",
        "    print(f\"Created dummy cost table at: {file_path}\")\n",
        "else:\n",
        "    print(f\"Cost table already exists at: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrMftdTouiKt",
        "outputId": "27bdd3a8-f187-41e1-e2e4-da5b3af3bb11"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dummy cost table at: energy/cost_table.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0468cb43"
      },
      "source": [
        "## Integrate `gpuprofiler` in the evaluation loop\n",
        "\n",
        "### Subtask:\n",
        "Integrate `gpuprofiler` in the evaluation loop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32436113"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the evaluate_model function to accept the GPUProfiler instance and collect GPU metrics periodically, storing them for later averaging and logging.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fec8e3a"
      },
      "source": [
        "## Integrate `thermalsignalgenerator` in the evaluation loop\n",
        "\n",
        "### Subtask:\n",
        "Integrate `thermalsignalgenerator` in the evaluation loop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f92ccd6"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the evaluate\\_model function to accept the thermal signal generator, call it within the loop, store the signals, and include them in the return dictionary, updating the logging accordingly. In the main block, initialize and pass the thermal signal generator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbdd5b6e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Dict, Tuple, Optional, List, Any\n",
        "import time\n",
        "import logging\n",
        "import math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "class WikiText2Dataset(Dataset):\n",
        "    # just a simulation for now, skeleton code\n",
        "    def __init__(self, vocab_size: int = 1000, seq_len: int = 512, num_samples: int = 1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = num_samples\n",
        "        logging.info(f\"Initialized WikiText2Dataset with {num_samples} samples, seq_len={seq_len}, vocab_size={vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Simulate text data: random token IDs\n",
        "        input_ids = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
        "        # For language modeling, the target is usually the next token\n",
        "        labels = torch.cat([input_ids[1:], torch.tensor([0])]) # Simple shift\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "\n",
        "def evaluate_model(\n",
        "    model: MoETransformer,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    profiler: GPUProfiler, # Accept GPUProfiler instance\n",
        "    thermal_signal_generator: ThermalSignalGenerator, # Accept ThermalSignalGenerator instance\n",
        "    log_interval: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs an evaluation loop for the MoE model, logging inference time\n",
        "    and GPU metrics, and collecting thermal signals.\n",
        "\n",
        "    Args:\n",
        "        model: The MoETransformer model.\n",
        "        dataloader: DataLoader for the evaluation dataset.\n",
        "        device: Device to run evaluation on (e.g., 'cuda' or 'cpu').\n",
        "        profiler: GPUProfiler instance for logging metrics.\n",
        "        thermal_signal_generator: ThermalSignalGenerator instance.\n",
        "        log_interval: How often to log progress and metrics.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing average perplexity, total inference time,\n",
        "        average power draw, and aggregated MoE metrics.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    inference_times_ms = []\n",
        "    power_draws_watts = []\n",
        "    temperatures_c = []\n",
        "    gpu_utilizations_percent = []\n",
        "\n",
        "    # Aggregated MoE metrics across all layers and batches\n",
        "    aggregated_moe_metrics: Dict[str, List[Any]] = {} # Use Any to handle dicts/lists\n",
        "\n",
        "    thermal_signals: List[ThermalSignal] = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    logging.info(f\"Starting evaluation on device: {device}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get thermal signal before computation\n",
        "            thermal_signal = thermal_signal_generator.get_thermal_signal()\n",
        "            if thermal_signal:\n",
        "                thermal_signals.append(thermal_signal)\n",
        "                # Log thermal state and recommendations\n",
        "                logging.info(\n",
        "                    f\"Batch {batch_idx+1} Thermal Signal: State={thermal_signal.thermal_state.value}, \"\n",
        "                    f\"PowerMode={thermal_signal.power_mode.value}, Temp={thermal_signal.temperature:.1f}°C, \"\n",
        "                    f\"Power={thermal_signal.power_draw:.1f}W\"\n",
        "                )\n",
        "                # Note: In a real system, you would use these signals to adapt model behavior\n",
        "                # (e.g., select different experts, apply throttle factors). For this\n",
        "                # baseline evaluation, we just log them.\n",
        "\n",
        "\n",
        "            # Measure inference time for the batch\n",
        "            if torch.cuda.is_available():\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "                start_event.record()\n",
        "\n",
        "            # Forward pass with profiling enabled for MoE metrics\n",
        "            model_output = model(input_ids, profile=True) # Enable profiling in model for detailed metrics\n",
        "            logits = model_output['logits']\n",
        "            aux_loss = model_output.get('aux_loss', torch.tensor(0.0)).item()\n",
        "            metrics = model_output.get('metrics', {})\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "                batch_inference_time_ms = start_event.elapsed_time(end_event)\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "            else:\n",
        "                # Approximate time for CPU\n",
        "                batch_inference_time_ms = (time.time() - start_time) * 1000 # Rough estimate\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "\n",
        "\n",
        "            # Calculate loss (for perplexity)\n",
        "            # Reshape logits and labels for CrossEntropyLoss\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            labels_flat = labels.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=0) # Assuming 0 is padding/ignore\n",
        "\n",
        "            total_loss += loss.item() * labels.numel() # Accumulate loss weighted by number of elements\n",
        "            total_tokens += labels.numel()\n",
        "            total_batches += 1\n",
        "\n",
        "            # Collect and store GPU metrics for this batch\n",
        "            gpu_metrics = profiler.get_current_metrics()\n",
        "            if gpu_metrics:\n",
        "                power_draws_watts.append(gpu_metrics.power_draw)\n",
        "                temperatures_c.append(gpu_metrics.temperature)\n",
        "                gpu_utilizations_percent.append(gpu_metrics.gpu_utilization)\n",
        "\n",
        "            # Aggregate MoE specific metrics\n",
        "            if metrics:\n",
        "                for key, value in metrics.items():\n",
        "                    if isinstance(value, np.ndarray):\n",
        "                        # Convert arrays to lists for consistent aggregation\n",
        "                        value = value.tolist()\n",
        "\n",
        "                    if key not in aggregated_moe_metrics:\n",
        "                         aggregated_moe_metrics[key] = []\n",
        "\n",
        "                    if isinstance(value, (int, float, list)):\n",
        "                         aggregated_moe_metrics[key].append(value)\n",
        "                    elif isinstance(value, dict):\n",
        "                         # For dictionaries (like expert timings), aggregate per key\n",
        "                         for sub_key, sub_value in value.items():\n",
        "                             agg_key = f'{key}__{sub_key}' # e.g., 'expert_batch_timings_ms__0'\n",
        "                             if agg_key not in aggregated_moe_metrics:\n",
        "                                 aggregated_moe_metrics[agg_key] = []\n",
        "                             aggregated_moe_metrics[agg_key].append(sub_value)\n",
        "                    else:\n",
        "                         logging.warning(f\"Skipping aggregation for metric {key} with unsupported type {type(value)}\")\n",
        "\n",
        "\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                avg_batch_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "                current_perplexity = math.exp(avg_batch_loss) if avg_batch_loss < 100 else float('inf') # Avoid overflow\n",
        "\n",
        "                log_msg = (\n",
        "                    f\"Batch {batch_idx+1}/{len(dataloader)} | \"\n",
        "                    f\"Loss: {avg_batch_loss:.4f} | \"\n",
        "                    f\"Perplexity: {current_perplexity:.2f} | \"\n",
        "                    f\"Batch Time: {batch_inference_time_ms:.2f} ms\"\n",
        "                )\n",
        "                # Log collected GPU metrics at log interval\n",
        "                if gpu_metrics:\n",
        "                    log_msg += (\n",
        "                        f\" | Power: {gpu_metrics.power_draw:.1f}W | \"\n",
        "                        f\"Temp: {gpu_metrics.temperature:.1f}°C | \"\n",
        "                        f\"GPU Util: {gpu_metrics.gpu_utilization:.1f}%\"\n",
        "                    )\n",
        "                logging.info(log_msg)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_inference_duration_sec = end_time - start_time\n",
        "\n",
        "    # Calculate overall averages\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "    final_perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
        "\n",
        "    # Calculate averages for collected GPU metrics\n",
        "    avg_inference_time_ms = np.mean(inference_times_ms) if inference_times_ms else 0\n",
        "    avg_power_draw_watts = np.mean(power_draws_watts) if power_draws_watts else 0\n",
        "    avg_temperature_c = np.mean(temperatures_c) if temperatures_c else 0\n",
        "    avg_gpu_utilization_percent = np.mean(gpu_utilizations_percent) if gpu_utilizations_percent else 0\n",
        "\n",
        "    # Aggregate MoE metrics (e.g., average expert usage across batches)\n",
        "    final_moe_metrics: Dict[str, Any] = {}\n",
        "    for key, values_list in aggregated_moe_metrics.items():\n",
        "        if not values_list:\n",
        "            continue\n",
        "\n",
        "        if isinstance(values_list[0], (int, float)):\n",
        "            final_moe_metrics[f'avg_{key}'] = np.mean(values_list)\n",
        "        elif isinstance(values_list[0], list) or isinstance(values_list[0], np.ndarray):\n",
        "             # For list/array metrics (like expert_usage), average the lists/arrays\n",
        "             # Ensure all lists/arrays have the same shape before averaging\n",
        "             try:\n",
        "                 final_moe_metrics[f'avg_{key}'] = np.mean([np.array(v) for v in values_list], axis=0).tolist()\n",
        "             except Exception as e:\n",
        "                 logging.warning(f\"Could not average list/array metric {key}: {e}\")\n",
        "                 final_moe_metrics[f'raw_{key}'] = values_list # Store raw list if averaging fails\n",
        "        elif isinstance(values_list[0], dict):\n",
        "             # This case should ideally be handled by the sub_key aggregation above,\n",
        "             # but as a fallback, log a warning.\n",
        "             logging.warning(f\"Metric {key} contains dictionaries, averaging not supported directly.\")\n",
        "             final_moe_metrics[f'raw_{key}'] = values_list # Store raw list of dicts\n",
        "\n",
        "    results = {\n",
        "        \"final_perplexity\": final_perplexity,\n",
        "        \"total_inference_duration_sec\": total_inference_duration_sec,\n",
        "        \"avg_inference_time_per_batch_ms\": avg_inference_time_ms,\n",
        "        \"avg_power_draw_watts\": avg_power_draw_watts,\n",
        "        \"avg_temperature_c\": avg_temperature_c,\n",
        "        \"avg_gpu_utilization_percent\": avg_gpu_utilization_percent,\n",
        "        \"aggregated_moe_metrics\": final_moe_metrics,\n",
        "        \"thermal_signals\": thermal_signals # Include collected thermal signals\n",
        "    }\n",
        "\n",
        "    logging.info(\"\\n--- Evaluation Summary ---\")\n",
        "    logging.info(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
        "    logging.info(f\"Total Inference Duration: {total_inference_duration_sec:.2f} seconds\")\n",
        "    # Corrected access to avg_inference_time_per_batch_ms\n",
        "    logging.info(f\"Average Batch Inference Time: {results['avg_inference_time_per_batch_ms']:.2f} ms\")\n",
        "    if results['avg_power_draw_watts'] > 0:\n",
        "        logging.info(f\"Average Power Draw: {results['avg_power_draw_watts']:.1f} W\")\n",
        "        logging.info(f\"Average Temperature: {results['avg_temperature_c']:.1f} °C\")\n",
        "        logging.info(f\"Average GPU Utilization: {results['avg_gpu_utilization_percent']:.1f} %\")\n",
        "    logging.info(\"Aggregated MoE Metrics:\")\n",
        "    for k, v in final_moe_metrics.items():\n",
        "        # Format array output nicely\n",
        "        if isinstance(v, list) and all(isinstance(i, (int, float)) for i in v):\n",
        "             logging.info(f\"  {k}: {np.array(v)}\")\n",
        "        else:\n",
        "             logging.info(f\"  {k}: {v}\")\n",
        "\n",
        "    logging.info(\"\\nCollected Thermal Signals:\")\n",
        "    for i, signal in enumerate(results['thermal_signals']):\n",
        "         logging.info(f\"  Signal {i+1}: Temp={signal.temperature:.1f}°C, Power={signal.power_draw:.1f}W, State={signal.thermal_state.value}\")\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Main execution block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # 1. Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 2. Initialize GPUProfiler\n",
        "    profiler = GPUProfiler()\n",
        "    profiler.start_profiling() # Start the profiling thread\n",
        "\n",
        "    # 3. Initialize ThermalSignalGenerator\n",
        "    # Assuming a default cost table exists or is handled by the class\n",
        "    thermal_signal_generator = ThermalSignalGenerator(profiler=profiler)\n",
        "    logging.info(\"ThermalSignalGenerator initialized.\")\n",
        "\n",
        "\n",
        "    # 4. Model Parameters (Adjust as needed for your specific MoE setup)\n",
        "    VOCAB_SIZE = 10000 # Example vocab size\n",
        "    D_MODEL = 512\n",
        "    N_HEADS = 8\n",
        "    N_LAYERS = 6\n",
        "    D_FF = 2048\n",
        "    N_EXPERTS = 8\n",
        "    TOP_K = 2\n",
        "    MAX_SEQ_LEN = 512\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    # Set which layers use MoE (e.g., every other layer)\n",
        "    USE_MOE_LAYERS = [i % 2 == 1 for i in range(N_LAYERS)] # [False, True, False, True, False, True]\n",
        "\n",
        "    # 5. Instantiate MoE Model\n",
        "    logging.info(\"Initializing MoETransformer model...\")\n",
        "    model = MoETransformer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        n_experts=N_EXPERTS,\n",
        "        top_k=TOP_K,\n",
        "        max_seq_len=MAX_SEQ_LEN,\n",
        "        use_moe_layers=USE_MOE_LAYERS\n",
        "    ).to(device)\n",
        "    logging.info(f\"Model instantiated with {sum(USE_MOE_LAYERS)} MoE layers.\")\n",
        "\n",
        "    # Optional: Load a pre-trained checkpoint if you have one\n",
        "    # checkpoint_path = \"path/to/your/checkpoint.pth\"\n",
        "    # if Path(checkpoint_path).exists():\n",
        "    #     logging.info(f\"Loading model checkpoint from {checkpoint_path}...\")\n",
        "    #     model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    #     logging.info(\"Model checkpoint loaded.\")\n",
        "    # else:\n",
        "    #     logging.warning(\"No model checkpoint found. Using randomly initialized weights.\")\n",
        "\n",
        "\n",
        "    # 6. Prepare Dataset and DataLoader (using simulated WikiText-2 for baseline)\n",
        "    # For actual WikiText-2, you'd use torchtext or similar to load and preprocess.\n",
        "    # Example: from torchtext.datasets import WikiText2\n",
        "    # For now, we use our dummy dataset.\n",
        "    logging.info(\"Preparing dataset...\")\n",
        "    eval_dataset = WikiText2Dataset(vocab_size=VOCAB_SIZE, seq_len=MAX_SEQ_LEN, num_samples=100) # Use a small number of samples for baseline\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    logging.info(f\"Evaluation DataLoader ready with {len(eval_dataloader)} batches.\")\n",
        "\n",
        "    # 7. Run Evaluation\n",
        "    logging.info(\"\\n--- Starting Baseline Inference Evaluation ---\")\n",
        "    baseline_results = evaluate_model(\n",
        "        model=model,\n",
        "        dataloader=eval_dataloader,\n",
        "        device=device,\n",
        "        profiler=profiler, # Pass the profiler instance\n",
        "        thermal_signal_generator=thermal_signal_generator, # Pass thermal signal generator\n",
        "        log_interval=10\n",
        "    )\n",
        "\n",
        "    # 8. Final Sanity Checks and Cleanup\n",
        "    logging.info(\"\\n--- Sanity Checks ---\")\n",
        "    if baseline_results['final_perplexity'] < float('inf'):\n",
        "        logging.info(f\"Perplexity sanity check: {baseline_results['final_perplexity']:.2f} (lower is better, typically starts high for untrained models)\")\n",
        "    else:\n",
        "        logging.warning(\"Perplexity is infinite. This might indicate issues like very high loss or training with random weights.\")\n",
        "\n",
        "    logging.info(f\"Average Power Draw: {baseline_results['avg_power_draw_watts']:.2f} W\")\n",
        "    logging.info(f\"Average Inference Time per Batch: {baseline_results['avg_inference_time_per_batch_ms']:.2f} ms\")\n",
        "\n",
        "    # Access detailed MoE metrics\n",
        "    if 'aggregated_moe_metrics' in baseline_results:\n",
        "        logging.info(\"\\nDetailed MoE Metrics (Averaged):\")\n",
        "        for key, value in baseline_results['aggregated_moe_metrics'].items():\n",
        "            logging.info(f\"  {key}: {value}\")\n",
        "\n",
        "    logging.info(\"\\nCollected Thermal Signals:\")\n",
        "    for i, signal in enumerate(baseline_results['thermal_signals']):\n",
        "         logging.info(f\"  Signal {i+1}: Temp={signal.temperature:.1f}°C, Power={signal.power_draw:.1f}W, State={signal.thermal_state.value}\")\n",
        "\n",
        "\n",
        "    profiler.stop_profiling() # Stop the profiling thread\n",
        "    logging.info(\"Evaluation complete.\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81e2e93"
      },
      "source": [
        "## Demonstrate basic metric analysis\n",
        "\n",
        "### Subtask:\n",
        "After the evaluation loop in `evaluate_model`, process and present the collected profiling and thermal metrics. This could include calculating averages, visualizing trends (if storing historical data), and showing expert usage distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "430b2445"
      },
      "source": [
        "**Reasoning**:\n",
        "Process and present the collected profiling and thermal metrics as requested in the instructions. This involves iterating through the collected metrics, calculating averages and summaries, and printing them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50b0ccb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4023b59b-89cd-418d-ce55-47bdaad95dd9"
      },
      "source": [
        "# Access aggregated MoE metrics and thermal signals from the results\n",
        "aggregated_moe_metrics = baseline_results.get('aggregated_moe_metrics', {})\n",
        "thermal_signals = baseline_results.get('thermal_signals', [])\n",
        "\n",
        "print(\"\\n--- Detailed MoE Metrics Analysis ---\")\n",
        "if aggregated_moe_metrics:\n",
        "    for key, values_list in aggregated_moe_metrics.items():\n",
        "        if not values_list:\n",
        "            print(f\"  {key}: No data collected\")\n",
        "            continue\n",
        "\n",
        "        # Check if values_list is actually a list before accessing elements\n",
        "        if not isinstance(values_list, list):\n",
        "             # If it's not a list, it's likely a single scalar value already\n",
        "             print(f\"  {key.replace('avg_', '')}: {values_list}\")\n",
        "             continue\n",
        "\n",
        "\n",
        "        # Handle different types of aggregated metrics (assuming values_list is now a list)\n",
        "        if 'usage_current' in key and values_list and isinstance(values_list[0], list):\n",
        "            # Expert usage is a list of counts per expert per batch\n",
        "            try:\n",
        "                avg_usage_per_expert = np.mean([np.array(v) for v in values_list if v is not None], axis=0) # Added check for None\n",
        "                print(f\"  Average {key.replace('avg_', '')} across batches: {avg_usage_per_expert}\")\n",
        "            except Exception as e:\n",
        "                 logging.warning(f\"Could not average list metric {key}: {e}\")\n",
        "                 print(f\"  Raw {key}: {values_list}\")\n",
        "\n",
        "        elif ('timings_ms' in key or 'time_ms' in key) and values_list and isinstance(values_list[0], float): # Added 'time_ms' check\n",
        "             # Expert batch timings or FFN timings are floats per batch\n",
        "             avg_timing = np.mean(values_list)\n",
        "             print(f\"  Average {key.replace('avg_', '')} across batches: {avg_timing:.2f} ms\")\n",
        "\n",
        "        elif 'cumulative_timings_ms' in key and values_list and isinstance(values_list[0], dict): # Changed to check for dict\n",
        "             # Cumulative timings are stored as dicts per batch\n",
        "             # Aggregate by summing up times for each expert across batches\n",
        "             total_cumulative_timings = defaultdict(float)\n",
        "             for batch_dict in values_list:\n",
        "                 if batch_dict: # Handle cases where the dict might be empty or None\n",
        "                     for expert_id, timing in batch_dict.items():\n",
        "                         total_cumulative_timings[expert_id] += timing\n",
        "\n",
        "             print(f\"  Total {key.replace('avg_', '')}: {dict(total_cumulative_timings)}\")\n",
        "\n",
        "        elif values_list and isinstance(values_list[0], (int, float)):\n",
        "            # Other simple scalar metrics collected across batches\n",
        "            print(f\"  {key.replace('avg_', '')}: {np.mean(values_list):.4f}\")\n",
        "        else:\n",
        "            print(f\"  Raw {key}: {values_list}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"No aggregated MoE metrics collected.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Thermal Signal Analysis ---\")\n",
        "if thermal_signals:\n",
        "    temperatures = [s.temperature for s in thermal_signals]\n",
        "    power_draws = [s.power_draw for s in thermal_signals]\n",
        "    thermal_states = [s.thermal_state for s in thermal_signals]\n",
        "    power_modes = [s.power_mode for s in thermal_signals]\n",
        "\n",
        "    print(f\"  Total Thermal Signals Collected: {len(thermal_signals)}\")\n",
        "    print(f\"  Temperature: Avg={np.mean(temperatures)}°C, Max={np.max(temperatures)}°C, Min={np.min(temperatures)}°C\") # Added min/max formatting\n",
        "    print(f\"  Power Draw: Avg={np.mean(power_draws)}W, Max={np.max(power_draws)}W, Min={np.min(power_draws)}W\") # Added min/max formatting\n",
        "\n",
        "    # Summarize thermal states and power modes\n",
        "    thermal_state_counts = {}\n",
        "    if thermal_signals: # Ensure thermal_signals is not empty before iterating\n",
        "        for state in ThermalState:\n",
        "            count = thermal_states.count(state)\n",
        "            if count > 0:\n",
        "                thermal_state_counts[state.value] = count\n",
        "        print(f\"  Thermal State Distribution: {thermal_state_counts}\")\n",
        "\n",
        "        power_mode_counts = {}\n",
        "        for mode in PowerMode:\n",
        "            count = power_modes.count(mode)\n",
        "            if count > 0:\n",
        "                power_mode_counts[mode.value] = count\n",
        "        print(f\"  Power Mode Distribution: {power_mode_counts}\")\n",
        "\n",
        "else:\n",
        "    print(\"No thermal signals collected.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Total Thermal Signals Collected: 25\n",
            "  Temperature: Avg=63.72°C, Max=66°C, Min=62°C\n",
            "  Power Draw: Avg=31.777959999999997W, Max=34.495W, Min=30.564W\n",
            "  Thermal State Distribution: {'cool': 17, 'warm': 8}\n",
            "  Power Mode Distribution: {'balanced': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78178239"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `IndexError: invalid index to scalar variable.` in the line `elif isinstance(values_list[0], (int, float)):` suggests that `values_list` is not a list but a scalar value (int or float) in some cases. This can happen because the aggregation logic in `evaluate_model` might store scalar metrics directly under their original key if there's only one value collected (e.g., in the case of FFN layers which don't return nested metrics). The code should check if `values_list` is a list before attempting to access `values_list[0]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbYZ9hMuAemj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f5bb67-e619-4828-9400-ce15ea12ae5e"
      },
      "source": [
        "# Access aggregated MoE metrics and thermal signals from the results\n",
        "aggregated_moe_metrics = baseline_results.get('aggregated_moe_metrics', {})\n",
        "thermal_signals = baseline_results.get('thermal_signals', [])\n",
        "\n",
        "logging.info(\"\\n--- Detailed MoE Metrics Analysis ---\")\n",
        "if aggregated_moe_metrics:\n",
        "    for key, values_list in aggregated_moe_metrics.items():\n",
        "        if not values_list:\n",
        "            print(f\"  {key}: No data collected\")\n",
        "            continue\n",
        "\n",
        "        # Check if values_list is actually a list before accessing elements\n",
        "        if not isinstance(values_list, list):\n",
        "             # If it's not a list, it's likely a single scalar value already\n",
        "             print(f\"  {key.replace('avg_', '')}: {values_list}\")\n",
        "             continue\n",
        "\n",
        "        # Handle different types of aggregated metrics (assuming values_list is now a list)\n",
        "        if 'usage_current' in key and isinstance(values_list[0], list):\n",
        "            # Expert usage is a list of counts per expert per batch\n",
        "            try:\n",
        "                avg_usage_per_expert = np.mean([np.array(v) for v in values_list], axis=0)\n",
        "                print(f\"  Average {key.replace('avg_', '')} across batches: {avg_usage_per_expert}\")\n",
        "            except Exception as e:\n",
        "                 logging.warning(f\"Could not average list metric {key}: {e}\")\n",
        "                 print(f\"  Raw {key}: {values_list}\")\n",
        "\n",
        "        elif 'timings_ms' in key and isinstance(values_list[0], float):\n",
        "             # Expert batch timings are floats per expert per batch (aggregated by sub_key)\n",
        "             avg_timing = np.mean(values_list)\n",
        "             print(f\"  Average {key.replace('avg_', '')} across batches: {avg_timing:.2f} ms\")\n",
        "\n",
        "        elif 'cumulative_timings_ms' in key and isinstance(values_list[0], float):\n",
        "             # Cumulative timings (aggregated by sub_key) - the last value is the total\n",
        "             # Note: The aggregation logic stored the *last* cumulative value per batch,\n",
        "             # so we can just take the mean of these last values across batches,\n",
        "             # or, more accurately, the sum if we wanted total cumulative time across all batches.\n",
        "             # Let's just print the averaged 'last' cumulative value for simplicity here.\n",
        "             avg_last_cumulative_timing = np.mean(values_list)\n",
        "             print(f\"  Average Final {key.replace('avg_', '')}: {avg_last_cumulative_timing:.2f} ms\")\n",
        "\n",
        "        elif isinstance(values_list[0], (int, float)):\n",
        "            # Other simple scalar metrics\n",
        "            print(f\"  {key.replace('avg_', '')}: {np.mean(values_list):.4f}\")\n",
        "        else:\n",
        "            print(f\"  Raw {key}: {values_list}\")\n",
        "\n",
        "else:\n",
        "    print(\"No aggregated MoE metrics collected.\")\n",
        "\n",
        "print(\"\\n--- Thermal Signal Analysis ---\")\n",
        "if thermal_signals:\n",
        "    temperatures = [s.temperature for s in thermal_signals]\n",
        "    power_draws = [s.power_draw for s in thermal_signals]\n",
        "    thermal_states = [s.thermal_state for s in thermal_signals]\n",
        "    power_modes = [s.power_mode for s in thermal_signals]\n",
        "\n",
        "    print(f\"  Total Thermal Signals Collected: {len(thermal_signals)}\")\n",
        "    print(f\"  Temperature: Avg={np.mean(temperatures):.1f}°C, Max={np.max(temperatures):.1f}°C, Min={np.min(temperatures):.1f}°C\")\n",
        "    print(f\"  Power Draw: Avg={np.mean(power_draws):.1f}W, Max={np.max(power_draws):.1f}W, Min={np.min(power_draws):.1f}W\")\n",
        "\n",
        "    # Summarize thermal states and power modes\n",
        "    thermal_state_counts = {}\n",
        "    for state in ThermalState:\n",
        "        count = thermal_states.count(state)\n",
        "        if count > 0:\n",
        "            thermal_state_counts[state.value] = count\n",
        "    print(f\"  Thermal State Distribution: {thermal_state_counts}\")\n",
        "\n",
        "    power_mode_counts = {}\n",
        "    for mode in PowerMode:\n",
        "        count = power_modes.count(mode)\n",
        "        if count > 0:\n",
        "            power_mode_counts[mode.value] = count\n",
        "    print(f\"  Power Mode Distribution: {power_mode_counts}\")\n",
        "\n",
        "else:\n",
        "    print(\"No thermal signals collected.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  layer_0__ffn_time_ms: 3.5900300788879393\n",
            "  layer_1__moe_forward_time_ms: 658.0288623046876\n",
            "  layer_1__expert_usage_current: 512.0000\n",
            "  layer_1__total_assignments: 4096.0\n",
            "  layer_1__expert_batch_timings_ms__0: 1.241844472885132\n",
            "  layer_1__expert_batch_timings_ms__1: 1.1928422594070434\n",
            "  layer_1__expert_batch_timings_ms__2: 0.8485900783538818\n",
            "  layer_1__expert_batch_timings_ms__3: 1.3191347074508668\n",
            "  layer_1__expert_batch_timings_ms__4: 1.4396569681167604\n",
            "  layer_1__expert_batch_timings_ms__5: 0.9450944042205811\n",
            "  layer_1__expert_batch_timings_ms__6: 0.9551884770393372\n",
            "  layer_1__expert_batch_timings_ms__7: 0.9880921649932861\n",
            "  layer_1__expert_cumulative_timings_ms__0: 16.085323395729066\n",
            "  layer_1__expert_cumulative_timings_ms__1: 15.411649560928344\n",
            "  layer_1__expert_cumulative_timings_ms__2: 11.117712593078613\n",
            "  layer_1__expert_cumulative_timings_ms__3: 17.216543798446654\n",
            "  layer_1__expert_cumulative_timings_ms__4: 18.825777950286867\n",
            "  layer_1__expert_cumulative_timings_ms__5: 13.203634016513824\n",
            "  layer_1__expert_cumulative_timings_ms__6: 12.6549862074852\n",
            "  layer_1__expert_cumulative_timings_ms__7: 12.976775772571564\n",
            "  layer_2__ffn_time_ms: 3.5799590301513673\n",
            "  layer_3__moe_forward_time_ms: 656.4172436523437\n",
            "  layer_3__expert_usage_current: 512.0000\n",
            "  layer_3__total_assignments: 4096.0\n",
            "  layer_3__expert_batch_timings_ms__0: 1.1954035139083863\n",
            "  layer_3__expert_batch_timings_ms__1: 1.1834739255905151\n",
            "  layer_3__expert_batch_timings_ms__2: 1.4897625684738158\n",
            "  layer_3__expert_batch_timings_ms__3: 0.8529228830337524\n",
            "  layer_3__expert_batch_timings_ms__4: 0.9846246361732482\n",
            "  layer_3__expert_batch_timings_ms__5: 0.9795737600326538\n",
            "  layer_3__expert_batch_timings_ms__6: 1.1763161611557007\n",
            "  layer_3__expert_batch_timings_ms__7: 1.013096957206726\n",
            "  layer_3__expert_cumulative_timings_ms__0: 15.596764087677002\n",
            "  layer_3__expert_cumulative_timings_ms__1: 15.446364393234253\n",
            "  layer_3__expert_cumulative_timings_ms__2: 19.52323329925537\n",
            "  layer_3__expert_cumulative_timings_ms__3: 11.212842266559601\n",
            "  layer_3__expert_cumulative_timings_ms__4: 12.870873575210572\n",
            "  layer_3__expert_cumulative_timings_ms__5: 12.922350075244903\n",
            "  layer_3__expert_cumulative_timings_ms__6: 15.357863688468933\n",
            "  layer_3__expert_cumulative_timings_ms__7: 13.223545639514922\n",
            "  layer_4__ffn_time_ms: 3.624976644515991\n",
            "  layer_5__moe_forward_time_ms: 668.0604125976563\n",
            "  layer_5__expert_usage_current: 512.0000\n",
            "  layer_5__total_assignments: 4096.0\n",
            "  layer_5__expert_batch_timings_ms__0: 0.8373247981071472\n",
            "  layer_5__expert_batch_timings_ms__1: 0.6301990461349487\n",
            "  layer_5__expert_batch_timings_ms__2: 1.410251531600952\n",
            "  layer_5__expert_batch_timings_ms__3: 1.169761266708374\n",
            "  layer_5__expert_batch_timings_ms__4: 0.7454464030265808\n",
            "  layer_5__expert_batch_timings_ms__5: 1.7256243133544922\n",
            "  layer_5__expert_batch_timings_ms__6: 1.6014169692993163\n",
            "  layer_5__expert_batch_timings_ms__7: 0.9492467188835144\n",
            "  layer_5__expert_cumulative_timings_ms__0: 11.019502065181733\n",
            "  layer_5__expert_cumulative_timings_ms__1: 8.357967448234557\n",
            "  layer_5__expert_cumulative_timings_ms__2: 18.337411956787108\n",
            "  layer_5__expert_cumulative_timings_ms__3: 15.321005835533143\n",
            "  layer_5__expert_cumulative_timings_ms__4: 9.832697653770447\n",
            "  layer_5__expert_cumulative_timings_ms__5: 22.608106198310853\n",
            "  layer_5__expert_cumulative_timings_ms__6: 20.96892049789429\n",
            "  layer_5__expert_cumulative_timings_ms__7: 12.525309398174286\n",
            "\n",
            "--- Thermal Signal Analysis ---\n",
            "  Total Thermal Signals Collected: 25\n",
            "  Temperature: Avg=63.7°C, Max=66.0°C, Min=62.0°C\n",
            "  Power Draw: Avg=31.8W, Max=34.5W, Min=30.6W\n",
            "  Thermal State Distribution: {'cool': 17, 'warm': 8}\n",
            "  Power Mode Distribution: {'balanced': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a02c7dc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `SimpleMoELayer` was successfully modified to capture and report execution time per expert and token counts per expert for each batch.\n",
        "*   The `TransformerBlock` correctly propagates the detailed expert metrics from the `SimpleMoELayer` to the main model's output metrics dictionary.\n",
        "*   The evaluation loop in the `evaluate_model` function was successfully integrated with the `GPUProfiler` to collect GPU metrics (power draw, temperature, utilization) at each batch step.\n",
        "*   The `ThermalSignalGenerator` was integrated into the evaluation loop, allowing for the collection of thermal state information (temperature, power draw, thermal state, power mode) at the start of each batch.\n",
        "*   The collected MoE metrics (expert usage, timings) and thermal signals were aggregated and analyzed after the evaluation loop, including calculating averages, min/max values, and summarizing distributions.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The collected detailed expert timings and usage counts provide a foundation for analyzing expert performance and load balancing under different thermal conditions.\n",
        "*   The integration of thermal signals enables future work on implementing thermal-aware expert routing or dynamic power management strategies during inference based on real-time thermal state.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "444ff0de"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}