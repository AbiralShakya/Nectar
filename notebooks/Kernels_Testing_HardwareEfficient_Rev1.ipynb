{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeE6sxhb3DCB",
        "outputId": "dcee7089-86b9-47a6-d2bd-11143a9033de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 16 03:52:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwBIzbiR3aT_",
        "outputId": "0e04e3e3-b0d1-49b9-c178-134627361abd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.575.51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "nvmlInit()\n",
        "handle = nvmlDeviceGetHandleByIndex(0)\n",
        "print(\"Power draw (W):\", nvmlDeviceGetPowerUsage(handle) / 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxeFETC75KGK",
        "outputId": "220a723a-71cc-4eb9-9dab-ffb54ea50a41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power draw (W): 9.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import defaultdict, deque\n",
        "import numpy as np\n",
        "import pynvml\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "PYNVML_AVAILABLE = True\n",
        "\n",
        "@dataclass\n",
        "class GPUMetrics:\n",
        "  def __post_init__(self):\n",
        "    if self.timestamp is None:\n",
        "      self.timestamp = time.time()\n",
        "  timestamp: float\n",
        "  power_draw: float  # Watts\n",
        "  temperature: float  # Celsius\n",
        "  memory_used: int   # Bytes\n",
        "  memory_total: int  # Bytes\n",
        "  gpu_utilization: float  # Percentage\n",
        "  memory_utilization: float  # Percentage}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExpertProfile:\n",
        "    \"\"\"Profile data for a specific expert.\"\"\"\n",
        "    expert_id: str\n",
        "    flops: int\n",
        "    memory_footprint: int\n",
        "    avg_latency: float\n",
        "    energy_cost: float  # Estimated Joules\n",
        "    activation_count: int\n",
        "    last_updated: float\n",
        "\n",
        "\n",
        "class GPUProfiler:\n",
        "    \"\"\"\n",
        "    Continuously monitors GPU metrics and provides energy profiling for MoE experts.\n",
        "    Runs in a separate thread to avoid blocking main computation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device_id: int = 0, poll_interval: float = 0.1):\n",
        "        self.device_id = device_id\n",
        "        self.poll_interval = poll_interval\n",
        "        self.is_running = False\n",
        "        self.metrics_history = deque(maxlen=1000)  # Keep last 1000 samples\n",
        "        self.expert_profiles = {}\n",
        "        self.operation_stack = []  # Stack for nested operations\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "        if PYNVML_AVAILABLE:\n",
        "            try:\n",
        "                pynvml.nvmlInit()\n",
        "                self.handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n",
        "                self.nvml_available = True\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Failed to initialize NVML: {e}\")\n",
        "                self.nvml_available = False\n",
        "        else:\n",
        "            self.nvml_available = False\n",
        "\n",
        "        self.polling_thread = None\n",
        "\n",
        "    def start_profiling(self):\n",
        "        \"\"\"Start the GPU monitoring thread.\"\"\"\n",
        "        if self.is_running:\n",
        "            return\n",
        "\n",
        "        self.is_running = True\n",
        "        self.polling_thread = threading.Thread(target=self._polling_loop, daemon=True)\n",
        "        self.polling_thread.start()\n",
        "        logging.info(\"GPU profiling started\")\n",
        "\n",
        "    def stop_profiling(self):\n",
        "        \"\"\"Stop the GPU monitoring thread.\"\"\"\n",
        "        self.is_running = False\n",
        "        if self.polling_thread:\n",
        "            self.polling_thread.join(timeout=1.0)\n",
        "        logging.info(\"GPU profiling stopped\")\n",
        "\n",
        "    def _polling_loop(self):\n",
        "        \"\"\"Main polling loop running in separate thread.\"\"\"\n",
        "        while self.is_running:\n",
        "            try:\n",
        "                metrics = self._collect_gpu_metrics()\n",
        "                if metrics:\n",
        "                    with self.lock:\n",
        "                        self.metrics_history.append(metrics)\n",
        "                time.sleep(self.poll_interval)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in GPU polling: {e}\")\n",
        "                time.sleep(self.poll_interval)\n",
        "\n",
        "    def _collect_gpu_metrics(self) -> Optional[GPUMetrics]:\n",
        "        \"\"\"Collect current GPU metrics.\"\"\"\n",
        "        if not self.nvml_available:\n",
        "            # Return dummy metrics for testing\n",
        "            return GPUMetrics(\n",
        "                timestamp=time.time(),\n",
        "                power_draw=150.0 + np.random.normal(0, 10),\n",
        "                temperature=65.0 + np.random.normal(0, 5),\n",
        "                memory_used=int(4e9 + np.random.normal(0, 1e8)),\n",
        "                memory_total=int(8e9),\n",
        "                gpu_utilization=80.0 + np.random.normal(0, 10),\n",
        "                memory_utilization=50.0 + np.random.normal(0, 5)\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            power_draw = pynvml.nvmlDeviceGetPowerUsage(self.handle) / 1000.0  # mW to W\n",
        "            temperature = pynvml.nvmlDeviceGetTemperature(self.handle, pynvml.NVML_TEMPERATURE_GPU)\n",
        "\n",
        "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)\n",
        "            memory_used = mem_info.used\n",
        "            memory_total = mem_info.total\n",
        "\n",
        "            util_rates = pynvml.nvmlDeviceGetUtilizationRates(self.handle)\n",
        "            gpu_util = util_rates.gpu\n",
        "            memory_util = util_rates.memory\n",
        "\n",
        "            return GPUMetrics(\n",
        "                timestamp=time.time(),\n",
        "                power_draw=power_draw,\n",
        "                temperature=temperature,\n",
        "                memory_used=memory_used,\n",
        "                memory_total=memory_total,\n",
        "                gpu_utilization=gpu_util,\n",
        "                memory_utilization=memory_util\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to collect GPU metrics: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_current_metrics(self) -> Optional[GPUMetrics]:\n",
        "        \"\"\"Get the most recent GPU metrics.\"\"\"\n",
        "        with self.lock:\n",
        "            return self.metrics_history[-1] if self.metrics_history else None\n",
        "\n",
        "    def get_metrics_window(self, duration: float = 1.0) -> List[GPUMetrics]:\n",
        "        \"\"\"Get metrics from the last `duration` seconds.\"\"\"\n",
        "        current_time = time.time()\n",
        "        cutoff_time = current_time - duration\n",
        "\n",
        "        with self.lock:\n",
        "            return [m for m in self.metrics_history if m.timestamp >= cutoff_time]\n",
        "\n",
        "    def start_operation(self, operation_name: str, expert_id: Optional[str] = None):\n",
        "        \"\"\"Start tracking an operation (e.g., expert forward pass).\"\"\"\n",
        "        operation_data = {\n",
        "            'name': operation_name,\n",
        "            'expert_id': expert_id,\n",
        "            'start_time': time.time(),\n",
        "            'start_metrics': self.get_current_metrics()\n",
        "        }\n",
        "        self.operation_stack.append(operation_data)\n",
        "\n",
        "    def end_operation(self) -> Optional[Dict]:\n",
        "        \"\"\"End tracking the current operation and return profiling data.\"\"\"\n",
        "        if not self.operation_stack:\n",
        "            return None\n",
        "\n",
        "        operation_data = self.operation_stack.pop()\n",
        "        end_time = time.time()\n",
        "        end_metrics = self.get_current_metrics()\n",
        "\n",
        "        duration = end_time - operation_data['start_time']\n",
        "\n",
        "        # Calculate energy consumption during operation\n",
        "        energy_consumed = 0.0\n",
        "        if operation_data['start_metrics'] and end_metrics:\n",
        "            avg_power = (operation_data['start_metrics'].power_draw + end_metrics.power_draw) / 2.0\n",
        "            energy_consumed = avg_power * duration  # Joules\n",
        "\n",
        "        result = {\n",
        "            'operation': operation_data['name'],\n",
        "            'expert_id': operation_data['expert_id'],\n",
        "            'duration': duration,\n",
        "            'energy_consumed': energy_consumed,\n",
        "            'start_metrics': operation_data['start_metrics'],\n",
        "            'end_metrics': end_metrics\n",
        "        }\n",
        "\n",
        "        # Update expert profile if applicable\n",
        "        if operation_data['expert_id']:\n",
        "            self._update_expert_profile(operation_data['expert_id'], result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _update_expert_profile(self, expert_id: str, operation_result: Dict):\n",
        "        \"\"\"Update the profile for a specific expert.\"\"\"\n",
        "        if expert_id not in self.expert_profiles:\n",
        "            self.expert_profiles[expert_id] = ExpertProfile(\n",
        "                expert_id=expert_id,\n",
        "                flops=0,\n",
        "                memory_footprint=0,\n",
        "                avg_latency=0.0,\n",
        "                energy_cost=0.0,\n",
        "                activation_count=0,\n",
        "                last_updated=time.time()\n",
        "            )\n",
        "\n",
        "        profile = self.expert_profiles[expert_id]\n",
        "        profile.activation_count += 1\n",
        "\n",
        "        # Update running averages\n",
        "        alpha = 0.1  # Exponential moving average factor\n",
        "        profile.avg_latency = (1 - alpha) * profile.avg_latency + alpha * operation_result['duration']\n",
        "        profile.energy_cost = (1 - alpha) * profile.energy_cost + alpha * operation_result['energy_consumed']\n",
        "        profile.last_updated = time.time()\n",
        "\n",
        "    def estimate_expert_flops(self, expert_module: nn.Module, input_shape: Tuple[int, ...]) -> int:\n",
        "        \"\"\"\n",
        "        Estimate FLOPs for an expert module.\n",
        "        This is a simplified estimation - you might want to use more sophisticated methods.\n",
        "        \"\"\"\n",
        "        total_flops = 0\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = torch.randn(input_shape)\n",
        "\n",
        "        def flop_count_hook(module, input, output):\n",
        "            nonlocal total_flops\n",
        "            if isinstance(module, nn.Linear):\n",
        "                # For linear layers: input_size * output_size * batch_size\n",
        "                in_features = module.in_features\n",
        "                out_features = module.out_features\n",
        "                batch_size = input[0].shape[0] if input else 1\n",
        "                total_flops += in_features * out_features * batch_size\n",
        "            elif isinstance(module, nn.Conv2d):\n",
        "                # Simplified conv2d FLOP estimation\n",
        "                kernel_flops = module.kernel_size[0] * module.kernel_size[1] * module.in_channels\n",
        "                output_elements = output.numel() if hasattr(output, 'numel') else 1\n",
        "                total_flops += kernel_flops * output_elements\n",
        "\n",
        "        # Register hooks\n",
        "        hooks = []\n",
        "        for module in expert_module.modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                hooks.append(module.register_forward_hook(flop_count_hook))\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                expert_module(dummy_input)\n",
        "        finally:\n",
        "            # Remove hooks\n",
        "            for hook in hooks:\n",
        "                hook.remove()\n",
        "\n",
        "        return total_flops\n",
        "\n",
        "    def get_expert_profile(self, expert_id: str) -> Optional[ExpertProfile]:\n",
        "        \"\"\"Get the profile for a specific expert.\"\"\"\n",
        "        return self.expert_profiles.get(expert_id)\n",
        "\n",
        "    def get_all_expert_profiles(self) -> Dict[str, ExpertProfile]:\n",
        "        \"\"\"Get profiles for all experts.\"\"\"\n",
        "        return self.expert_profiles.copy()\n",
        "\n",
        "    def save_profiles(self, filepath: str):\n",
        "        \"\"\"Save expert profiles to JSON file.\"\"\"\n",
        "        profiles_dict = {\n",
        "            expert_id: asdict(profile)\n",
        "            for expert_id, profile in self.expert_profiles.items()\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(profiles_dict, f, indent=2)\n",
        "\n",
        "    def load_profiles(self, filepath: str):\n",
        "        \"\"\"Load expert profiles from JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                profiles_dict = json.load(f)\n",
        "\n",
        "            self.expert_profiles = {\n",
        "                expert_id: ExpertProfile(**profile_data)\n",
        "                for expert_id, profile_data in profiles_dict.items()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load profiles: {e}\")\n",
        "\n",
        "    def get_power_statistics(self, duration: float = 10.0) -> Dict[str, float]:\n",
        "        \"\"\"Get power consumption statistics over a time window.\"\"\"\n",
        "        metrics = self.get_metrics_window(duration)\n",
        "        if not metrics:\n",
        "            return {}\n",
        "\n",
        "        power_values = [m.power_draw for m in metrics]\n",
        "\n",
        "        return {\n",
        "            'mean_power': np.mean(power_values),\n",
        "            'max_power': np.max(power_values),\n",
        "            'min_power': np.min(power_values),\n",
        "            'std_power': np.std(power_values),\n",
        "            'total_energy': np.sum(power_values) * self.poll_interval  # Approximate\n",
        "        }\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"Context manager entry.\"\"\"\n",
        "        self.start_profiling()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Context manager exit.\"\"\"\n",
        "        self.stop_profiling()\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Example of how to use the GPUProfiler\n",
        "    profiler = GPUProfiler(device_id=0, poll_interval=0.1)\n",
        "\n",
        "    # Start profiling\n",
        "    profiler.start_profiling()\n",
        "\n",
        "    # Simulate some operations\n",
        "    time.sleep(1.0)\n",
        "\n",
        "    # Example expert operation\n",
        "    profiler.start_operation(\"expert_forward\", \"expert_0\")\n",
        "    time.sleep(0.5)  # Simulate computation\n",
        "    result = profiler.end_operation()\n",
        "\n",
        "    print(\"Operation result:\", result)\n",
        "\n",
        "    # Get current metrics\n",
        "    current = profiler.get_current_metrics()\n",
        "    if current:\n",
        "        print(f\"Current power: {current.power_draw:.2f}W, Temp: {current.temperature:.1f}°C\")\n",
        "\n",
        "    # Get power statistics\n",
        "    stats = profiler.get_power_statistics(duration=5.0)\n",
        "    print(\"Power statistics:\", stats)\n",
        "\n",
        "    # Stop profiling\n",
        "    profiler.stop_profiling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNfJuZtj5MHw",
        "outputId": "d28da4e8-6048-4a7b-cae4-987334a69b64"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation result: {'operation': 'expert_forward', 'expert_id': 'expert_0', 'duration': 0.5000874996185303, 'energy_consumed': 4.868351808786392, 'start_metrics': GPUMetrics(timestamp=1750046672.8425438, power_draw=9.735, temperature=39, memory_used=277872640, memory_total=16106127360, gpu_utilization=0, memory_utilization=0), 'end_metrics': GPUMetrics(timestamp=1750046673.360935, power_draw=9.735, temperature=39, memory_used=277872640, memory_total=16106127360, gpu_utilization=0, memory_utilization=0)}\n",
            "Current power: 9.73W, Temp: 39.0°C\n",
            "Power statistics: {'mean_power': np.float64(9.953333333333335), 'max_power': np.float64(10.625), 'min_power': np.float64(9.735), 'std_power': np.float64(0.293399083540188), 'total_energy': np.float64(14.930000000000001)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "\n",
        "class ThermalState(Enum):\n",
        "    \"\"\"Enumeration of thermal states for the system.\"\"\"\n",
        "    COOL = \"cool\"\n",
        "    WARM = \"warm\"\n",
        "    HOT = \"hot\"\n",
        "    CRITICAL = \"critical\"\n",
        "    THROTTLED = \"throttled\" # Added for explicit throttled state\n",
        "\n",
        "class PowerMode(Enum):\n",
        "    \"\"\"Enumeration of power modes.\"\"\"\n",
        "    LOW_POWER = \"low_power\"\n",
        "    BALANCED = \"balanced\"\n",
        "    PERFORMANCE = \"performance\"\n",
        "    EMERGENCY = \"emergency\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ThermalBudget:\n",
        "    \"\"\"Represents current thermal constraints and budgets.\"\"\"\n",
        "    max_temperature: float\n",
        "    max_power: float\n",
        "    max_energy_per_token: float # This might be dynamic based on expert costs\n",
        "    current_temperature: float\n",
        "    current_power: float\n",
        "    thermal_headroom: float\n",
        "    power_headroom: float\n",
        "    recommended_experts: List[str] # Now a list for specific expert IDs\n",
        "    throttle_factor: float = 1.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ThermalSignal:\n",
        "    \"\"\"Complete thermal signal with all relevant information.\"\"\"\n",
        "    timestamp: float\n",
        "    thermal_state: ThermalState\n",
        "    power_mode: PowerMode\n",
        "    temperature: float\n",
        "    power_draw: float\n",
        "    thermal_budget: ThermalBudget\n",
        "    expert_recommendations: Dict[str, float]  # expert_id -> priority score\n",
        "    throttle_recommendations: Dict[str, float]  # operation -> throttle factor (e.g., 'global_throttle': 0.8)\n",
        "    emergency_actions: List[str]\n",
        "\n",
        "\n",
        "class ThermalSignalGenerator:\n",
        "    \"\"\"\n",
        "    Generates thermal signals based on GPU profiling data and thermal budgets.\n",
        "    This module determines when to throttle computation, switch experts, or\n",
        "    take emergency actions based on thermal constraints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 profiler: GPUProfiler,\n",
        "                 cost_table_path: str = \"energy/cost_table.json\"):\n",
        "        self.profiler = profiler\n",
        "        self.cost_table_path = Path(cost_table_path)\n",
        "        self.cost_table = self._load_cost_table()\n",
        "\n",
        "        # Thermal parameters from cost table\n",
        "        self.thermal_params = self.cost_table.get(\"thermal_parameters\", {})\n",
        "        self.energy_budgets = self.cost_table.get(\"energy_budgets\", {})\n",
        "        self.expert_profiles = self.cost_table.get(\"expert_profiles\", {})\n",
        "\n",
        "        # State tracking\n",
        "        self.current_mode = PowerMode.BALANCED\n",
        "        self.thermal_history: List[ThermalSignal] = []\n",
        "        self.last_signal_time = 0.0\n",
        "        self.emergency_cooldown_active = False # Flag for emergency state\n",
        "        self.emergency_cooldown_duration = self.thermal_params.get(\"emergency_cooldown_duration\", 30.0)\n",
        "        self.emergency_cooldown_start_time = 0.0\n",
        "\n",
        "        # Thermal model parameters\n",
        "        self.base_temp = self.thermal_params.get(\"base_temperature\", 45.0)\n",
        "        self.warm_temp_threshold = self.thermal_params.get(\"warm_temperature_threshold\", self.base_temp + 20)\n",
        "        self.hot_temp_threshold = self.thermal_params.get(\"hot_temperature_threshold\", 83.0) # Renamed for consistency\n",
        "        self.critical_temp = self.thermal_params.get(\"critical_temperature\", 87.0)\n",
        "        self.thermal_time_constant = self.thermal_params.get(\"thermal_time_constant\", 15.0)\n",
        "\n",
        "        logging.basicConfig(level=logging.INFO) # Ensure logging is configured\n",
        "        logging.info(f\"ThermalSignal initialized with thresholds: {self.hot_temp_threshold}°C hot, {self.critical_temp}°C critical\")\n",
        "\n",
        "    def _load_cost_table(self) -> Dict:\n",
        "        \"\"\"Load the cost table configuration.\"\"\"\n",
        "        try:\n",
        "            with open(self.cost_table_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load cost table from {self.cost_table_path}: {e}\")\n",
        "            return self._get_default_cost_table()\n",
        "\n",
        "    def _get_default_cost_table(self) -> Dict:\n",
        "        \"\"\"Return default cost table if loading fails.\"\"\"\n",
        "        return {\n",
        "            \"thermal_parameters\": {\n",
        "                \"base_temperature\": 45.0,\n",
        "                \"warm_temperature_threshold\": 65.0, # Added specific warm threshold\n",
        "                \"hot_temperature_threshold\": 83.0,\n",
        "                \"critical_temperature\": 87.0,\n",
        "                \"thermal_time_constant\": 15.0,\n",
        "                \"emergency_cooldown_duration\": 30.0\n",
        "            },\n",
        "            \"energy_budgets\": {\n",
        "                \"low_power\": {\"max_power_watts\": 200, \"max_temperature\": 75.0, \"max_energy_per_token_mj\": 5.0},\n",
        "                \"balanced\": {\"max_power_watts\": 350, \"max_temperature\": 80.0, \"max_energy_per_token_mj\": 3.0},\n",
        "                \"performance\": {\"max_power_watts\": 450, \"max_temperature\": 85.0, \"max_energy_per_token_mj\": 1.5}\n",
        "            },\n",
        "            \"expert_profiles\": {\n",
        "                \"expert_A\": {\"average_power_watts\": 50, \"energy_per_token_mj\": 2.0, \"thermal_impact\": 0.1},\n",
        "                \"expert_B\": {\"average_power_watts\": 70, \"energy_per_token_mj\": 2.5, \"thermal_impact\": 0.15},\n",
        "                \"expert_C\": {\"average_power_watts\": 30, \"energy_per_token_mj\": 1.0, \"thermal_impact\": 0.05}\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_thermal_signal(self) -> Optional[ThermalSignal]:\n",
        "        \"\"\"\n",
        "        Generate current thermal signal based on GPU metrics and thermal model.\n",
        "        \"\"\"\n",
        "        current_metrics = self.profiler.get_current_metrics()\n",
        "        if not current_metrics:\n",
        "            logging.warning(\"No GPU metrics available to generate thermal signal.\")\n",
        "            return None\n",
        "\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Check for active emergency cooldown\n",
        "        if self.emergency_cooldown_active:\n",
        "            if current_time - self.emergency_cooldown_start_time < self.emergency_cooldown_duration:\n",
        "                # Still in emergency cooldown, prioritize cool-down actions\n",
        "                logging.info(f\"Emergency cooldown active. Remaining: {self.emergency_cooldown_duration - (current_time - self.emergency_cooldown_start_time):.1f}s\")\n",
        "                # Force emergency mode during cooldown\n",
        "                power_mode = PowerMode.EMERGENCY\n",
        "                thermal_state = ThermalState.THROTTLED # Or CRITICAL if temp is still high\n",
        "                if current_metrics.temperature < self.warm_temp_threshold: # Exit emergency if sufficiently cooled\n",
        "                    self.emergency_cooldown_active = False\n",
        "                    logging.info(\"Exiting emergency cooldown: temperature has dropped.\")\n",
        "                else:\n",
        "                    # Still hot during cooldown\n",
        "                    if current_metrics.temperature >= self.critical_temp:\n",
        "                         thermal_state = ThermalState.CRITICAL\n",
        "                    elif current_metrics.temperature >= self.hot_temp_threshold:\n",
        "                         thermal_state = ThermalState.HOT\n",
        "                    else:\n",
        "                         thermal_state = ThermalState.THROTTLED # Indicates active throttling for cooldown\n",
        "            else:\n",
        "                self.emergency_cooldown_active = False\n",
        "                logging.info(\"Emergency cooldown period ended.\")\n",
        "\n",
        "        # Determine thermal state if not in active cooldown\n",
        "        if not self.emergency_cooldown_active:\n",
        "            thermal_state = self._classify_thermal_state(current_metrics)\n",
        "\n",
        "        # Determine power mode (may be overridden by thermal state)\n",
        "        # If emergency cooldown is active, power_mode is already set to EMERGENCY\n",
        "        if not self.emergency_cooldown_active:\n",
        "            power_mode = self._determine_power_mode(current_metrics, thermal_state)\n",
        "        self.current_mode = power_mode # Update internal state\n",
        "\n",
        "        # Calculate thermal budget\n",
        "        thermal_budget = self._calculate_thermal_budget(current_metrics, power_mode)\n",
        "\n",
        "        # Generate expert recommendations\n",
        "        expert_recommendations = self._generate_expert_recommendations(\n",
        "            current_metrics, thermal_budget, thermal_state\n",
        "        )\n",
        "\n",
        "        # Generate throttle recommendations\n",
        "        throttle_recommendations = self._generate_throttle_recommendations(\n",
        "            current_metrics, thermal_state, thermal_budget\n",
        "        )\n",
        "\n",
        "        # Check for emergency actions (can trigger cooldown)\n",
        "        emergency_actions = self._check_emergency_actions(current_metrics, thermal_state)\n",
        "        if emergency_actions and \"initiate_emergency_cooldown\" in emergency_actions and not self.emergency_cooldown_active:\n",
        "            self.emergency_cooldown_active = True\n",
        "            self.emergency_cooldown_start_time = current_time\n",
        "            logging.warning(\"Initiating emergency cooldown due to critical thermal state!\")\n",
        "\n",
        "\n",
        "        signal = ThermalSignal(\n",
        "            timestamp=current_time,\n",
        "            thermal_state=thermal_state,\n",
        "            power_mode=power_mode,\n",
        "            temperature=current_metrics.temperature,\n",
        "            power_draw=current_metrics.power_draw,\n",
        "            thermal_budget=thermal_budget,\n",
        "            expert_recommendations=expert_recommendations,\n",
        "            throttle_recommendations=throttle_recommendations,\n",
        "            emergency_actions=emergency_actions\n",
        "        )\n",
        "\n",
        "        # Update history\n",
        "        self.thermal_history.append(signal)\n",
        "        if len(self.thermal_history) > 100:  # Keep a reasonable history size\n",
        "            self.thermal_history.pop(0)\n",
        "\n",
        "        self.last_signal_time = current_time\n",
        "\n",
        "        return signal\n",
        "\n",
        "    def _classify_thermal_state(self, metrics: GPUMetrics) -> ThermalState:\n",
        "        \"\"\"Classify current thermal state based on temperature.\"\"\"\n",
        "        temp = metrics.temperature\n",
        "\n",
        "        if temp >= self.critical_temp:\n",
        "            return ThermalState.CRITICAL\n",
        "        elif temp >= self.hot_temp_threshold:\n",
        "            return ThermalState.HOT\n",
        "        elif temp >= self.warm_temp_threshold:\n",
        "            return ThermalState.WARM\n",
        "        else:\n",
        "            return ThermalState.COOL\n",
        "\n",
        "    def _determine_power_mode(self, metrics: GPUMetrics, thermal_state: ThermalState) -> PowerMode:\n",
        "        \"\"\"\n",
        "        Determine the appropriate power mode based on thermal state and current usage.\n",
        "        This could involve hysteresis to prevent rapid mode switching.\n",
        "        \"\"\"\n",
        "        proposed_mode = self.current_mode # Start with current mode\n",
        "\n",
        "        # Logic for power mode transition\n",
        "        if thermal_state == ThermalState.CRITICAL:\n",
        "            proposed_mode = PowerMode.EMERGENCY\n",
        "        elif thermal_state == ThermalState.HOT:\n",
        "            if proposed_mode == PowerMode.PERFORMANCE: # Drop from performance if hot\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "            elif proposed_mode == PowerMode.BALANCED and metrics.power_draw > self.energy_budgets[\"balanced\"][\"max_power_watts\"] * 0.9:\n",
        "                # If balanced and still drawing too much power, consider low power\n",
        "                proposed_mode = PowerMode.LOW_POWER\n",
        "        elif thermal_state == ThermalState.WARM:\n",
        "            if proposed_mode == PowerMode.PERFORMANCE and metrics.power_draw > self.energy_budgets[\"performance\"][\"max_power_watts\"]:\n",
        "                # If warm but drawing too much for performance, drop to balanced\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "            elif proposed_mode == PowerMode.LOW_POWER and metrics.temperature < self.warm_temp_threshold - 5 and metrics.gpu_utilization < 0.5:\n",
        "                # If low power and cooled down, can move to balanced if not highly utilized\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "        elif thermal_state == ThermalState.COOL:\n",
        "            if proposed_mode == PowerMode.LOW_POWER and metrics.temperature < self.base_temp + 5 and metrics.gpu_utilization < 0.3:\n",
        "                proposed_mode = PowerMode.BALANCED\n",
        "            elif proposed_mode == PowerMode.BALANCED and metrics.temperature < self.base_temp + 10 and metrics.gpu_utilization > 0.8:\n",
        "                # If balanced and good temperature, consider performance if high utilization\n",
        "                proposed_mode = PowerMode.PERFORMANCE\n",
        "\n",
        "        # Apply hysteresis: only change mode if conditions persist or change is drastic\n",
        "        # For simplicity, we'll keep it direct for now, but a real system might use a timer\n",
        "        # or history to prevent oscillations.\n",
        "\n",
        "        if proposed_mode != self.current_mode:\n",
        "            logging.info(f\"Power mode transition: {self.current_mode.value} -> {proposed_mode.value} due to {thermal_state.value} state.\")\n",
        "            self.current_mode = proposed_mode # Update internal state\n",
        "\n",
        "        return proposed_mode\n",
        "\n",
        "    def _calculate_thermal_budget(self, metrics: GPUMetrics, power_mode: PowerMode) -> ThermalBudget:\n",
        "        \"\"\"\n",
        "        Calculate current thermal budget based on the selected power mode and current metrics.\n",
        "        \"\"\"\n",
        "        mode_budget = self.energy_budgets.get(power_mode.value, self.energy_budgets[\"balanced\"])\n",
        "\n",
        "        max_temp = mode_budget.get(\"max_temperature\", 80.0)\n",
        "        max_power = mode_budget.get(\"max_power_watts\", 350.0)\n",
        "        max_energy_per_token = mode_budget.get(\"max_energy_per_token_mj\", 3.0)\n",
        "\n",
        "        thermal_headroom = max_temp - metrics.temperature\n",
        "        power_headroom = max_power - metrics.power_draw\n",
        "\n",
        "        # Simple throttle factor calculation based on temperature headroom\n",
        "        throttle_factor = 1.0\n",
        "        if metrics.temperature >= self.hot_temp_threshold:\n",
        "            # Linear throttle as temperature approaches critical\n",
        "            temp_range = self.critical_temp - self.hot_temp_threshold\n",
        "            if temp_range > 0:\n",
        "                throttle_factor = 1.0 - ((metrics.temperature - self.hot_temp_threshold) / temp_range) * 0.5 # Up to 50% throttle\n",
        "            throttle_factor = max(0.1, throttle_factor) # Ensure it doesn't go below 10%\n",
        "            logging.warning(f\"Temperature is hot ({metrics.temperature}°C), applying throttle factor: {throttle_factor:.2f}\")\n",
        "\n",
        "        # Also consider power headroom for throttling\n",
        "        if metrics.power_draw > max_power * 1.1: # 10% overshoot on power budget\n",
        "            power_throttle = 1.0 - ((metrics.power_draw - max_power) / (max_power * 0.5)) # throttle up to 50% for 50% power overshoot\n",
        "            throttle_factor = min(throttle_factor, max(0.1, power_throttle))\n",
        "            logging.warning(f\"Power draw ({metrics.power_draw:.1f}W) exceeds budget, adjusting throttle factor: {throttle_factor:.2f}\")\n",
        "\n",
        "\n",
        "        # Dummy recommended experts for now - a real system would use a model\n",
        "        # or heuristics based on expert costs and current budget.\n",
        "        # This part should be driven by the _generate_expert_recommendations method.\n",
        "        recommended_experts: List[str] = [] # This field is populated by _generate_expert_recommendations\n",
        "\n",
        "        return ThermalBudget(\n",
        "            max_temperature=max_temp,\n",
        "            max_power=max_power,\n",
        "            max_energy_per_token=max_energy_per_token,\n",
        "            current_temperature=metrics.temperature,\n",
        "            current_power=metrics.power_draw,\n",
        "            thermal_headroom=thermal_headroom,\n",
        "            power_headroom=power_headroom,\n",
        "            recommended_experts=recommended_experts, # Will be filled by another method\n",
        "            throttle_factor=throttle_factor\n",
        "        )\n",
        "\n",
        "    def _generate_expert_recommendations(\n",
        "        self, metrics: GPUMetrics, budget: ThermalBudget, thermal_state: ThermalState\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Generate recommendations for expert prioritization based on thermal budget\n",
        "        and expert profiles. Experts with lower energy costs and thermal impact\n",
        "        are prioritized when resources are constrained.\n",
        "        \"\"\"\n",
        "        expert_priority: Dict[str, float] = {}\n",
        "\n",
        "        for expert_id, profile in self.expert_profiles.items():\n",
        "            # Calculate a basic priority score. Lower is better (more efficient).\n",
        "            # This is a simplified heuristic. A more advanced system might\n",
        "            # consider recent expert usage, task requirements, etc.\n",
        "            energy_cost = profile.get(\"energy_per_token_mj\", 100.0)\n",
        "            thermal_impact = profile.get(\"thermal_impact\", 1.0)\n",
        "            avg_power = profile.get(\"average_power_watts\", 100.0)\n",
        "\n",
        "            score = 0.0\n",
        "\n",
        "            # Prioritize experts with lower energy consumption\n",
        "            score += energy_cost / budget.max_energy_per_token # Lower is better, so divide\n",
        "\n",
        "            # Prioritize experts that are less thermally impactful\n",
        "            score += thermal_impact * (metrics.temperature / self.critical_temp) * 5.0 # Higher impact at higher temps lowers priority more\n",
        "\n",
        "            # Adjust based on power headroom: if power is tight, penalize high-power experts\n",
        "            if budget.power_headroom < 50: # If power is getting tight (e.g., less than 50W headroom)\n",
        "                score += (avg_power / budget.max_power) * 2.0 # Penalize high power consumption\n",
        "\n",
        "            # If current state is hot or critical, heavily penalize experts that generate a lot of heat/power\n",
        "            if thermal_state in [ThermalState.HOT, ThermalState.CRITICAL]:\n",
        "                score += (thermal_impact * 10) + (avg_power / budget.max_power * 5)\n",
        "\n",
        "            expert_priority[expert_id] = score\n",
        "\n",
        "        # Sort experts by priority score (lower score means higher priority)\n",
        "        sorted_experts = sorted(expert_priority.items(), key=lambda item: item[1])\n",
        "\n",
        "        # Convert back to dictionary, perhaps only returning top N or all with scores\n",
        "        return {expert_id: score for expert_id, score in sorted_experts}\n",
        "\n",
        "    def _generate_throttle_recommendations(\n",
        "        self, metrics: GPUMetrics, thermal_state: ThermalState, budget: ThermalBudget\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Generate throttle recommendations for various operations.\n",
        "        A global throttle factor is primarily determined by the thermal budget.\n",
        "        \"\"\"\n",
        "        throttle_recs: Dict[str, float] = {}\n",
        "\n",
        "        # The primary throttle factor comes from the thermal budget calculation\n",
        "        global_throttle_factor = budget.throttle_factor\n",
        "        throttle_recs[\"global_compute_throttle\"] = global_throttle_factor\n",
        "\n",
        "        # Add more specific throttles if needed, e.g., memory bandwidth throttle\n",
        "        # based on memory temperature or utilization, if available in metrics.\n",
        "        if metrics.memory_utilization > 0.9 and thermal_state in [ThermalState.HOT, ThermalState.CRITICAL]:\n",
        "            throttle_recs[\"memory_bandwidth_throttle\"] = max(0.5, 1.0 - (metrics.memory_utilization - 0.9) * 5) # 50% throttle if 100% util\n",
        "            logging.warning(f\"High memory utilization and thermal state: {throttle_recs['memory_bandwidth_throttle']:.2f} memory throttle.\")\n",
        "\n",
        "\n",
        "        if thermal_state == ThermalState.CRITICAL:\n",
        "            throttle_recs[\"global_compute_throttle\"] = min(0.1, global_throttle_factor) # Aggressive throttle\n",
        "            logging.critical(\"CRITICAL thermal state: Aggressive global throttle applied.\")\n",
        "        elif thermal_state == ThermalState.HOT:\n",
        "            throttle_recs[\"global_compute_throttle\"] = min(0.5, global_throttle_factor) # Moderate throttle\n",
        "            logging.warning(\"HOT thermal state: Moderate global throttle applied.\")\n",
        "        elif thermal_state == ThermalState.THROTTLED: # Explicit throttled state\n",
        "            throttle_recs[\"global_compute_throttle\"] = min(0.2, global_throttle_factor) # More aggressive throttle\n",
        "            logging.warning(\"THROTTLED thermal state: Global throttle applied for cooldown.\")\n",
        "\n",
        "\n",
        "        return throttle_recs\n",
        "\n",
        "    def _check_emergency_actions(self, metrics: GPUMetrics, thermal_state: ThermalState) -> List[str]:\n",
        "        \"\"\"\n",
        "        Check if any emergency actions are required, such as initiating a full cooldown.\n",
        "        \"\"\"\n",
        "        emergency_actions: List[str] = []\n",
        "\n",
        "        if thermal_state == ThermalState.CRITICAL:\n",
        "            logging.critical(f\"Temperature {metrics.temperature}°C is CRITICAL! Recommending emergency cooldown.\")\n",
        "            emergency_actions.append(\"initiate_emergency_cooldown\")\n",
        "            emergency_actions.append(\"alert_system_operator\") # Example of other emergency actions\n",
        "\n",
        "        # Add other conditions that might trigger emergency actions, e.g.,\n",
        "        # persistent high power draw despite throttling, or fan failure detection.\n",
        "\n",
        "        return emergency_actions"
      ],
      "metadata": {
        "id": "XNiT_0Q_5pav"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairscale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Udql2O66jeL",
        "outputId": "6c0bdcf0-96f0-4616-8cdc-fb00150aaba5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairscale\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from fairscale) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from fairscale) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->fairscale)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->fairscale) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->fairscale) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->fairscale) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332208 sha256=4202980fced3bf2fd6dc852604ee47d9c27eaa9f38dfcfe4250869e4f3a4cf04\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/ef/96/5044bde220b2ea299bdc6ec05051e0ef187fad45b341d1c273\n",
            "Successfully built fairscale\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fairscale\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fairscale-0.4.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TopKRouter(nn.Module):\n",
        "    \"\"\"\n",
        "    Top-K router for MoE with load balancing loss and detailed profiling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_experts: int,\n",
        "        top_k: int = 2,\n",
        "        capacity_factor: float = 1.25,\n",
        "        gate_noise: float = 1e-2,\n",
        "        expert_dropout: float = 0.0,\n",
        "        balance_loss_weight: float = 0.01,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_experts = n_experts\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.gate_noise = gate_noise\n",
        "        self.expert_dropout = expert_dropout\n",
        "        self.balance_loss_weight = balance_loss_weight\n",
        "\n",
        "        # Gating network - simple linear layer\n",
        "        self.gate = nn.Linear(d_model, n_experts, bias=False)\n",
        "\n",
        "        # Initialize gate weights\n",
        "        nn.init.normal_(self.gate.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        # For tracking expert usage statistics\n",
        "        self.register_buffer('expert_usage_counts', torch.zeros(n_experts))\n",
        "        self.register_buffer('total_tokens_processed', torch.tensor(0.0))\n",
        "\n",
        "    def add_noise(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add noise to gate logits for better exploration.\"\"\"\n",
        "        if self.training and self.gate_noise > 0:\n",
        "            noise = torch.randn_like(logits) * self.gate_noise\n",
        "            return logits + noise\n",
        "        return logits\n",
        "\n",
        "    def compute_load_balancing_loss(\n",
        "        self,\n",
        "        gate_logits: torch.Tensor,\n",
        "        selected_experts: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute load balancing loss to encourage even expert usage.\n",
        "\n",
        "        Args:\n",
        "            gate_logits: Raw gate logits [batch_size * seq_len, n_experts]\n",
        "            selected_experts: Selected expert indices [batch_size * seq_len, top_k]\n",
        "\n",
        "        Returns:\n",
        "            Load balancing loss scalar\n",
        "        \"\"\"\n",
        "        # Compute gate probabilities\n",
        "        gate_probs = F.softmax(gate_logits, dim=-1)  # [B*T, n_experts]\n",
        "\n",
        "        # Fraction of tokens assigned to each expert\n",
        "        expert_mask = F.one_hot(selected_experts, num_classes=self.n_experts).float()  # [B*T, top_k, n_experts]\n",
        "        expert_assignment = expert_mask.sum(dim=1)  # [B*T, n_experts]\n",
        "        tokens_per_expert = expert_assignment.sum(dim=0)  # [n_experts]\n",
        "\n",
        "        # Normalize by total tokens\n",
        "        total_tokens = gate_logits.shape[0] * self.top_k\n",
        "        fraction_per_expert = tokens_per_expert / total_tokens\n",
        "\n",
        "        # Average gate probability for each expert\n",
        "        avg_gate_prob = gate_probs.mean(dim=0)  # [n_experts]\n",
        "\n",
        "        # Load balancing loss: minimize the dot product of these two distributions\n",
        "        # This encourages both distributions to be uniform\n",
        "        balance_loss = (fraction_per_expert * avg_gate_prob).sum() * self.n_experts\n",
        "\n",
        "        return balance_loss\n",
        "\n",
        "    def compute_capacity(self, batch_size: int, seq_len: int) -> int:\n",
        "        \"\"\"Compute expert capacity based on capacity factor.\"\"\"\n",
        "        tokens_per_expert = (batch_size * seq_len * self.top_k) / self.n_experts\n",
        "        capacity = int(tokens_per_expert * self.capacity_factor)\n",
        "        return max(capacity, 4)  # Minimum capacity\n",
        "\n",
        "    def profile_expert_timing(self, selected_experts: torch.Tensor) -> Dict:\n",
        "        \"\"\"\n",
        "        Profile per-expert timing and usage.\n",
        "\n",
        "        Args:\n",
        "            selected_experts: Selected expert indices [batch_size * seq_len, top_k]\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with profiling information\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Count expert usage\n",
        "        expert_counts = torch.zeros(self.n_experts, device=selected_experts.device)\n",
        "        for expert_id in range(self.n_experts):\n",
        "            expert_counts[expert_id] = (selected_experts == expert_id).sum().float()\n",
        "\n",
        "        # Update global statistics\n",
        "        self.expert_usage_counts += expert_counts\n",
        "        self.total_tokens_processed += selected_experts.numel()\n",
        "\n",
        "        # Compute usage statistics\n",
        "        total_assignments = expert_counts.sum()\n",
        "        expert_utilization = expert_counts / (total_assignments + 1e-8)\n",
        "\n",
        "        metrics.update({\n",
        "            'expert_usage_current': expert_counts.cpu().numpy(),\n",
        "            'expert_utilization_current': expert_utilization.cpu().numpy(),\n",
        "            'expert_usage_cumulative': self.expert_usage_counts.cpu().numpy(),\n",
        "            'total_assignments': total_assignments.item(),\n",
        "            'usage_variance': expert_utilization.var().item(),\n",
        "            'max_expert_usage': expert_utilization.max().item(),\n",
        "            'min_expert_usage': expert_utilization.min().item(),\n",
        "        })\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict:\n",
        "        \"\"\"\n",
        "        Forward pass of the router.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing routing decisions and metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Reshape for easier processing\n",
        "        x_flat = x.view(-1, d_model)  # [batch_size * seq_len, d_model]\n",
        "\n",
        "        # Timing for gate computation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            gate_start = torch.cuda.Event(enable_timing=True)\n",
        "            gate_end = torch.cuda.Event(enable_timing=True)\n",
        "            gate_start.record()\n",
        "\n",
        "        # Compute gate logits\n",
        "        gate_logits = self.gate(x_flat)  # [batch_size * seq_len, n_experts]\n",
        "\n",
        "        # Add noise for exploration\n",
        "        gate_logits = self.add_noise(gate_logits)\n",
        "\n",
        "        # Get top-k experts\n",
        "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            gate_end.record()\n",
        "            torch.cuda.synchronize()\n",
        "            gate_time = gate_start.elapsed_time(gate_end)\n",
        "        else:\n",
        "            gate_time = 0.0\n",
        "\n",
        "        # Compute routing probabilities (softmax over top-k)\n",
        "        top_k_probs = F.softmax(top_k_values, dim=-1)  # [batch_size * seq_len, top_k]\n",
        "\n",
        "        # Compute load balancing loss\n",
        "        balance_loss = self.compute_load_balancing_loss(gate_logits, top_k_indices)\n",
        "\n",
        "        # Profile expert usage\n",
        "        usage_metrics = self.profile_expert_timing(top_k_indices)\n",
        "\n",
        "        # Compute expert capacity\n",
        "        capacity = self.compute_capacity(batch_size, seq_len)\n",
        "\n",
        "        # Apply expert dropout during training\n",
        "        if self.training and self.expert_dropout > 0:\n",
        "            dropout_mask = torch.rand_like(top_k_probs) > self.expert_dropout\n",
        "            top_k_probs = top_k_probs * dropout_mask\n",
        "            # Renormalize\n",
        "            top_k_probs = top_k_probs / (top_k_probs.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        # Prepare output\n",
        "        output = {\n",
        "            'expert_indices': top_k_indices,  # [batch_size * seq_len, top_k]\n",
        "            'expert_weights': top_k_probs,    # [batch_size * seq_len, top_k]\n",
        "            'gate_logits': gate_logits,       # [batch_size * seq_len, n_experts]\n",
        "            'balance_loss': balance_loss * self.balance_loss_weight,\n",
        "            'capacity': capacity,\n",
        "            'metrics': {\n",
        "                'gate_computation_time_ms': gate_time,\n",
        "                'balance_loss_raw': balance_loss.item(),\n",
        "                'balance_loss_weighted': (balance_loss * self.balance_loss_weight).item(),\n",
        "                'capacity': capacity,\n",
        "                'avg_top_k_confidence': top_k_probs.mean().item(),\n",
        "                'gate_entropy': -F.softmax(gate_logits, dim=-1).mul(F.log_softmax(gate_logits, dim=-1)).sum(-1).mean().item(),\n",
        "                **usage_metrics,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_expert_stats(self) -> Dict:\n",
        "        \"\"\"Get comprehensive expert usage statistics.\"\"\"\n",
        "        if self.total_tokens_processed == 0:\n",
        "            return {'message': 'No tokens processed yet'}\n",
        "\n",
        "        cumulative_usage = self.expert_usage_counts / self.total_tokens_processed\n",
        "\n",
        "        return {\n",
        "            'total_tokens_processed': self.total_tokens_processed.item(),\n",
        "            'expert_usage_distribution': cumulative_usage.cpu().numpy(),\n",
        "            'usage_std': cumulative_usage.std().item(),\n",
        "            'usage_coefficient_of_variation': cumulative_usage.std().item() / (cumulative_usage.mean().item() + 1e-8),\n",
        "            'most_used_expert': cumulative_usage.argmax().item(),\n",
        "            'least_used_expert': cumulative_usage.argmin().item(),\n",
        "            'perfect_balance_target': 1.0 / self.n_experts,\n",
        "        }\n",
        "\n",
        "    def reset_stats(self):\n",
        "        \"\"\"Reset expert usage statistics.\"\"\"\n",
        "        self.expert_usage_counts.zero_()\n",
        "        self.total_tokens_processed.zero_()\n",
        "\n",
        "\n",
        "# Auxiliary router for comparison/experimentation\n",
        "class SwitchRouter(TopKRouter):\n",
        "    \"\"\"\n",
        "    Switch Transformer style router (top-1 with capacity dropping).\n",
        "    Inherits from TopKRouter but modifies behavior for top-1 routing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, n_experts: int, **kwargs):\n",
        "        # Force top_k = 1 for Switch routing\n",
        "        super().__init__(d_model, n_experts, top_k=1, **kwargs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict:\n",
        "        \"\"\"Switch-style routing with capacity dropping.\"\"\"\n",
        "        # Get base routing decisions\n",
        "        output = super().forward(x)\n",
        "\n",
        "        # For Switch, we need to implement capacity dropping\n",
        "        batch_size, seq_len = x.shape[:2]\n",
        "        capacity = output['capacity']\n",
        "        expert_indices = output['expert_indices'].squeeze(-1)  # [batch_size * seq_len]\n",
        "\n",
        "        # Count tokens per expert\n",
        "        expert_counts = torch.zeros(self.n_experts, device=x.device)\n",
        "        for expert_id in range(self.n_experts):\n",
        "            expert_counts[expert_id] = (expert_indices == expert_id).sum()\n",
        "\n",
        "        # Create capacity mask\n",
        "        capacity_mask = torch.ones_like(expert_indices, dtype=torch.bool)\n",
        "\n",
        "        for expert_id in range(self.n_experts):\n",
        "            expert_tokens = (expert_indices == expert_id).nonzero(as_tuple=True)[0]\n",
        "            if len(expert_tokens) > capacity:\n",
        "                # Randomly drop tokens exceeding capacity\n",
        "                dropped_indices = expert_tokens[capacity:]\n",
        "                capacity_mask[dropped_indices] = False\n",
        "\n",
        "        # Update metrics\n",
        "        tokens_dropped = (~capacity_mask).sum().item()\n",
        "        output['metrics']['tokens_dropped'] = tokens_dropped\n",
        "        output['metrics']['drop_rate'] = tokens_dropped / expert_indices.numel()\n",
        "        output['capacity_mask'] = capacity_mask\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the router\n",
        "    d_model = 256\n",
        "    n_experts = 8\n",
        "    top_k = 2\n",
        "    batch_size = 4\n",
        "    seq_len = 32\n",
        "\n",
        "    router = TopKRouter(\n",
        "        d_model=d_model,\n",
        "        n_experts=n_experts,\n",
        "        top_k=top_k,\n",
        "        capacity_factor=1.5,\n",
        "        balance_loss_weight=0.01,\n",
        "    )\n",
        "\n",
        "    # Test input\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = router(x)\n",
        "\n",
        "    print(\"Router output keys:\", output.keys())\n",
        "    print(f\"Expert indices shape: {output['expert_indices'].shape}\")\n",
        "    print(f\"Expert weights shape: {output['expert_weights'].shape}\")\n",
        "    print(f\"Balance loss: {output['balance_loss'].item():.6f}\")\n",
        "    print(f\"Expert capacity: {output['capacity']}\")\n",
        "\n",
        "    print(\"\\nMetrics:\")\n",
        "    for key, value in output['metrics'].items():\n",
        "        if isinstance(value, (list, np.ndarray)):\n",
        "            print(f\"  {key}: {np.array(value)}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"\\nExpert statistics:\")\n",
        "    stats = router.get_expert_stats()\n",
        "    for key, value in stats.items():\n",
        "        if isinstance(value, (list, np.ndarray)):\n",
        "            print(f\"  {key}: {np.array(value)}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Test Switch router\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing Switch Router\")\n",
        "\n",
        "    switch_router = SwitchRouter(d_model=d_model, n_experts=n_experts)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        switch_output = switch_router(x)\n",
        "\n",
        "    print(f\"Switch expert indices shape: {switch_output['expert_indices'].shape}\")\n",
        "    print(f\"Tokens dropped: {switch_output['metrics']['tokens_dropped']}\")\n",
        "    print(f\"Drop rate: {switch_output['metrics']['drop_rate']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gvrgY9W5vXT",
        "outputId": "177bde5b-aec5-4435-a6eb-a8c88d52ebb8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Router output keys: dict_keys(['expert_indices', 'expert_weights', 'gate_logits', 'balance_loss', 'capacity', 'metrics'])\n",
            "Expert indices shape: torch.Size([128, 2])\n",
            "Expert weights shape: torch.Size([128, 2])\n",
            "Balance loss: 0.010015\n",
            "Expert capacity: 48\n",
            "\n",
            "Metrics:\n",
            "  gate_computation_time_ms: 0.3563520014286041\n",
            "  balance_loss_raw: 1.0015389919281006\n",
            "  balance_loss_weighted: 0.010015389882028103\n",
            "  capacity: 48\n",
            "  avg_top_k_confidence: 0.5\n",
            "  gate_entropy: 2.0398693084716797\n",
            "  expert_usage_current: [27. 30. 32. 44. 31. 35. 30. 27.]\n",
            "  expert_utilization_current: [0.10546875 0.1171875  0.125      0.171875   0.12109375 0.13671875\n",
            " 0.1171875  0.10546875]\n",
            "  expert_usage_cumulative: [27. 30. 32. 44. 31. 35. 30. 27.]\n",
            "  total_assignments: 256.0\n",
            "  usage_variance: 0.00046212333836592734\n",
            "  max_expert_usage: 0.171875\n",
            "  min_expert_usage: 0.10546875\n",
            "\n",
            "Expert statistics:\n",
            "  total_tokens_processed: 256.0\n",
            "  expert_usage_distribution: [0.10546875 0.1171875  0.125      0.171875   0.12109375 0.13671875\n",
            " 0.1171875  0.10546875]\n",
            "  usage_std: 0.021497054025530815\n",
            "  usage_coefficient_of_variation: 0.17197641844613304\n",
            "  most_used_expert: 3\n",
            "  least_used_expert: 0\n",
            "  perfect_balance_target: 0.125\n",
            "\n",
            "==================================================\n",
            "Testing Switch Router\n",
            "Switch expert indices shape: torch.Size([128, 1])\n",
            "Tokens dropped: 2\n",
            "Drop rate: 0.016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import numpy as np\n",
        "# from fairscale.nn.moe import MOELayer # Removing fairscale dependency\n",
        "\n",
        "class SimpleMoELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified, non-distributed MoE layer for profiling.\n",
        "    Handles top-k routing, expert dispatch, and output combining.\n",
        "    \"\"\"\n",
        "    def __init__(self, gate: nn.Module, experts: nn.ModuleList, top_k: int = 2, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.gate = gate # The gating network (e.g., a Linear layer)\n",
        "        self.experts = experts # A ModuleList of expert networks\n",
        "        self.n_experts = len(experts)\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor # Keep for potential future use/comparison, not strictly used in this simple version\n",
        "\n",
        "        if top_k > self.n_experts:\n",
        "            raise ValueError(f\"top_k ({top_k}) cannot be greater than n_experts ({self.n_experts})\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass of the simple MoE layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size * seq_len, d_model] (assuming flattened input)\n",
        "\n",
        "        Returns:\n",
        "            output: Output tensor [batch_size * seq_len, d_model]\n",
        "            aux_loss: Load balancing loss scalar\n",
        "            metrics: Dictionary containing routing metrics\n",
        "        \"\"\"\n",
        "        # Input shape is expected to be [num_tokens, d_model]\n",
        "        num_tokens, d_model = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Compute gate logits\n",
        "        gate_logits = self.gate(x)  # [num_tokens, n_experts]\n",
        "\n",
        "        # Get top-k experts and their probabilities\n",
        "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1) # [num_tokens, top_k]\n",
        "        top_k_probs = F.softmax(top_k_values, dim=-1) # [num_tokens, top_k]\n",
        "\n",
        "        # Compute load balancing loss (similar to the router's logic)\n",
        "        gate_probs_all = F.softmax(gate_logits, dim=-1) # [num_tokens, n_experts]\n",
        "\n",
        "        # Fraction of tokens assigned to each expert (based on top-1 for simplicity in loss)\n",
        "        # For top-k, a more accurate loss might be needed, but this is a common approximation\n",
        "        top1_indices = top_k_indices[:, 0] # [num_tokens]\n",
        "        expert_mask_top1 = F.one_hot(top1_indices, num_classes=self.n_experts).float() # [num_tokens, n_experts]\n",
        "        tokens_per_expert = expert_mask_top1.sum(dim=0) # [n_experts]\n",
        "        total_tokens_for_loss = num_tokens # Use total tokens for normalization\n",
        "\n",
        "        # Normalize by total tokens\n",
        "        fraction_per_expert = tokens_per_expert / (total_tokens_for_loss + 1e-8) # Add epsilon for stability\n",
        "\n",
        "        # Average gate probability for each expert (average over all tokens)\n",
        "        avg_gate_prob = gate_probs_all.mean(dim=0) # [n_experts]\n",
        "\n",
        "        # Load balancing loss\n",
        "        # This formulation encourages both fraction_per_expert and avg_gate_prob to be uniform\n",
        "        aux_loss = (fraction_per_expert * avg_gate_prob).sum() * self.n_experts\n",
        "\n",
        "\n",
        "        # Dispatch tokens to experts\n",
        "        # Create sparse dispatch tensor: [num_tokens, top_k, n_experts]\n",
        "        # entry is 1 if token i is routed to expert j with weight k\n",
        "        dispatch_mask = F.one_hot(top_k_indices, num_classes=self.n_experts).float()\n",
        "\n",
        "        # Combine weights with dispatch mask: [num_tokens, top_k, n_experts]\n",
        "        weights = top_k_probs.unsqueeze(-1) * dispatch_mask # [num_tokens, top_k, 1] * [num_tokens, top_k, n_experts]\n",
        "\n",
        "        # Reshape weights for scattering: [num_tokens, n_experts, top_k]\n",
        "        weights = weights.permute(0, 2, 1)\n",
        "\n",
        "        # Create expert inputs by repeating tokens for each selected expert\n",
        "        # Input x is [num_tokens, d_model]. We need to send x to each expert it's routed to.\n",
        "        # A simpler approach for a non-distributed setting is to loop through experts.\n",
        "\n",
        "        output = torch.zeros_like(x) # [num_tokens, d_model]\n",
        "        metrics = {}\n",
        "\n",
        "        # Collect metrics for profiling (can be expanded)\n",
        "        expert_usage_counts = torch.zeros(self.n_experts, device=device)\n",
        "\n",
        "        # Loop through each expert\n",
        "        for expert_id in range(self.n_experts):\n",
        "            # Find tokens routed to this expert (at any of the top-k positions)\n",
        "            expert_tokens_mask = (top_k_indices == expert_id).any(dim=-1) # [num_tokens]\n",
        "            expert_token_indices = torch.where(expert_tokens_mask)[0] # Indices of tokens for this expert\n",
        "\n",
        "            if expert_token_indices.numel() > 0:\n",
        "                # Get inputs for this expert\n",
        "                expert_input = x[expert_token_indices] # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Get weights for this expert's tokens\n",
        "                # Find where in top_k_indices this expert appears for these tokens\n",
        "                # We need the weight corresponding to this expert for each token it was routed to\n",
        "                # This is a bit tricky with arbitrary top-k positions.\n",
        "                # A common simplification in non-distributed is to just use the weights.\n",
        "                # We'll use the weights corresponding to where the expert_id appears in top_k_indices\n",
        "                # For simplicity, let's just use the sum of weights for this expert for each token.\n",
        "                # More correctly, we should apply the weight *before* summing outputs.\n",
        "\n",
        "                # Simplified approach: Get the weights for the tokens routed to this expert\n",
        "                # We need the weights corresponding to expert_id for each of expert_token_indices\n",
        "                # This requires looking up the weight in top_k_probs based on where expert_id is in top_k_indices\n",
        "                expert_weights_for_tokens = torch.zeros(expert_token_indices.numel(), device=device)\n",
        "                for i, token_idx in enumerate(expert_token_indices):\n",
        "                    # Find where expert_id is in top_k_indices for this token\n",
        "                    pos_in_topk = torch.where(top_k_indices[token_idx] == expert_id)[0]\n",
        "                    if pos_in_topk.numel() > 0:\n",
        "                         # Sum weights if expert appears multiple times (shouldn't happen with unique top-k)\n",
        "                         expert_weights_for_tokens[i] = top_k_probs[token_idx, pos_in_topk].sum()\n",
        "\n",
        "                # Run expert forward pass\n",
        "                expert_output = self.experts[expert_id](expert_input) # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Weight expert output by the routing probability and scatter back\n",
        "                # The output needs to be weighted by the probability the token was sent to this expert.\n",
        "                # If top_k > 1, a token's output is a weighted sum of multiple experts.\n",
        "                # We need to add the weighted output of this expert for its tokens back to the main output tensor.\n",
        "\n",
        "                # Weighted expert output: [num_expert_tokens, d_model]\n",
        "                weighted_expert_output = expert_output * expert_weights_for_tokens.unsqueeze(-1)\n",
        "\n",
        "                # Scatter weighted output back to the original token positions\n",
        "                output[expert_token_indices] += weighted_expert_output\n",
        "\n",
        "                # Update usage counts\n",
        "                expert_usage_counts[expert_id] = expert_token_indices.numel()\n",
        "\n",
        "\n",
        "        # Collect simple routing metrics\n",
        "        metrics['expert_usage_current'] = expert_usage_counts.cpu().numpy()\n",
        "        metrics['total_assignments'] = expert_usage_counts.sum().item() # Total tokens dispatched\n",
        "\n",
        "        # In a non-distributed setting, load balancing loss is calculated based on global token distribution\n",
        "        # We already computed aux_loss above based on the full batch.\n",
        "\n",
        "\n",
        "        return output, aux_loss, metrics\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, n_experts: int = 8,\n",
        "                 top_k: int = 2, dropout: float = 0.1, use_moe: bool = True, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.use_moe = use_moe\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        if use_moe:\n",
        "            # Gating network for the MoE layer\n",
        "            gate_layer = nn.Linear(d_model, n_experts, bias=False)\n",
        "             # Initialize gate weights\n",
        "            nn.init.normal_(gate_layer.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "            # Create a ModuleList with n_experts distinct instances of the expert_module\n",
        "            experts_list = nn.ModuleList([\n",
        "                nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(dropout),\n",
        "                                   nn.Linear(d_ff, d_model)) for _ in range(n_experts)\n",
        "            ])\n",
        "\n",
        "            self.moe_layer = SimpleMoELayer(\n",
        "                gate=gate_layer,\n",
        "                experts=experts_list,\n",
        "                top_k=top_k,\n",
        "                capacity_factor=capacity_factor # Not used in SimpleMoELayer but kept for compatibility\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                              nn.ReLU(),\n",
        "                                              nn.Dropout(dropout),\n",
        "                                              nn.Linear(d_ff, d_model))\n",
        "\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.expert_timings = {} # Keep for potential future profiling\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass with optional profiling.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "            mask: Attention mask\n",
        "            profile: Whether to collect timing information\n",
        "\n",
        "        Returns:\n",
        "            output: Transformed tensor\n",
        "            metrics: Dictionary containing routing metrics and timings\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Self-attention\n",
        "        residual = x\n",
        "        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(residual + self.dropout(attn_out))\n",
        "\n",
        "        # MoE or FFN\n",
        "        residual = x\n",
        "\n",
        "        if self.use_moe:\n",
        "            # Reshape input for MoE layer: [batch_size * seq_len, d_model]\n",
        "            batch_size, seq_len, d_model = x.shape\n",
        "            x_flat = x.view(-1, d_model)\n",
        "\n",
        "            # Profile MoE forward pass\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "\n",
        "            # MoE forward pass using SimpleMoELayer\n",
        "            moe_out_flat, aux_loss, moe_metrics = self.moe_layer(x_flat)\n",
        "\n",
        "            if profile and torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                total_time = start_event.elapsed_time(end_event)\n",
        "                metrics['moe_forward_time_ms'] = total_time\n",
        "                metrics.update(moe_metrics) # Add simple moe metrics\n",
        "\n",
        "            # Reshape output back to [batch_size, seq_len, d_model]\n",
        "            moe_out = moe_out_flat.view(batch_size, seq_len, d_model)\n",
        "\n",
        "            x = residual + self.dropout(moe_out)\n",
        "            metrics['aux_loss'] = aux_loss\n",
        "\n",
        "        else:\n",
        "            # Standard FFN\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "                ffn_out = self.feed_forward(x)\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                metrics['ffn_time_ms'] = start_event.elapsed_time(end_event)\n",
        "            else:\n",
        "                ffn_out = self.feed_forward(x)\n",
        "\n",
        "            x = residual + self.dropout(ffn_out)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x, metrics\n",
        "\n",
        "\n",
        "class MoETransformer(nn.Module):\n",
        "    \"\"\"Simple MoE Transformer model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 6,\n",
        "        d_ff: int = 2048,\n",
        "        n_experts: int = 8,\n",
        "        top_k: int = 2,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        use_moe_layers: Optional[list] = None,  # Which layers use MoE\n",
        "        capacity_factor: float = 1.25,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Determine which layers use MoE\n",
        "        if use_moe_layers is None:\n",
        "            # By default, use MoE in every other layer starting from layer 1\n",
        "            use_moe_layers = [i % 2 == 1 for i in range(n_layers)]\n",
        "        elif len(use_moe_layers) != n_layers:\n",
        "             raise ValueError(f\"Length of use_moe_layers ({len(use_moe_layers)}) must match n_layers ({n_layers})\")\n",
        "\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                d_ff=d_ff,\n",
        "                n_experts=n_experts,\n",
        "                top_k=top_k,\n",
        "                dropout=dropout,\n",
        "                use_moe=use_moe_layers[i],\n",
        "                capacity_factor=capacity_factor,\n",
        "            )\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token indices [batch_size, seq_len]\n",
        "            attention_mask: Attention mask [batch_size, seq_len]\n",
        "            profile: Whether to collect profiling information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing logits, aux_loss, and optional metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(0, seq_len, device=device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "\n",
        "        # Attention mask for causal modeling\n",
        "        if attention_mask is None:\n",
        "            # Create causal mask\n",
        "            causal_mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
        "            ).bool()\n",
        "        else:\n",
        "            causal_mask = attention_mask\n",
        "\n",
        "        total_aux_loss = 0.0\n",
        "        all_metrics = {} if profile else None\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Pass profiler instance to the layer if needed, or handle profiling inside layer\n",
        "            x, layer_metrics = layer(x, mask=causal_mask, profile=profile)\n",
        "\n",
        "            # Accumulate auxiliary loss from MoE layers\n",
        "            if 'aux_loss' in layer_metrics:\n",
        "                total_aux_loss += layer_metrics['aux_loss']\n",
        "\n",
        "            # Collect metrics\n",
        "            if profile:\n",
        "                for key, value in layer_metrics.items():\n",
        "                    if key != 'aux_loss':\n",
        "                        # Append metrics to lists if they exist, otherwise create\n",
        "                        metric_key = f'layer_{i}__{key}'\n",
        "                        if metric_key in all_metrics:\n",
        "                            if isinstance(all_metrics[metric_key], list):\n",
        "                                all_metrics[metric_key].append(value)\n",
        "                            else: # Convert to list if first time appending\n",
        "                                all_metrics[metric_key] = [all_metrics[metric_key], value]\n",
        "                        else:\n",
        "                            all_metrics[metric_key] = value\n",
        "\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        output = {\n",
        "            'logits': logits,\n",
        "            'aux_loss': total_aux_loss,\n",
        "        }\n",
        "\n",
        "        if profile:\n",
        "            output['metrics'] = all_metrics\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model\n",
        "    model = MoETransformer(\n",
        "        vocab_size=1000,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        n_layers=4,\n",
        "        d_ff=1024,\n",
        "        n_experts=4,\n",
        "        top_k=2,\n",
        "        use_moe_layers=[False, True, False, True],  # MoE in layers 1 and 3\n",
        "    )\n",
        "\n",
        "    # Test input\n",
        "    batch_size, seq_len = 2, 32\n",
        "    input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
        "\n",
        "    # Forward pass with profiling\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, profile=True)\n",
        "\n",
        "    print(f\"Output logits shape: {output['logits'].shape}\")\n",
        "    print(f\"Auxiliary loss: {output['aux_loss'].item():.6f}\")\n",
        "\n",
        "    if 'metrics' in output:\n",
        "        print(\"\\nProfiling metrics:\")\n",
        "        for key, value in output['metrics'].items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"  {key}: {value.shape}\")\n",
        "            elif isinstance(value, list):\n",
        "                 print(f\"  {key}: {value}\") # Print list content\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFxw54dM6hoX",
        "outputId": "adaaeb7a-6d5c-4f82-bb92-66ae75a42073"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits shape: torch.Size([2, 32, 1000])\n",
            "Auxiliary loss: 2.016642\n",
            "\n",
            "Profiling metrics:\n",
            "  layer_0__ffn_time_ms: 1.5723199844360352\n",
            "  layer_1__moe_forward_time_ms: 13.217599868774414\n",
            "  layer_1__expert_usage_current: [37. 35. 32. 24.]\n",
            "  layer_1__total_assignments: 128.0\n",
            "  layer_2__ffn_time_ms: 1.5655360221862793\n",
            "  layer_3__moe_forward_time_ms: 8.697471618652344\n",
            "  layer_3__expert_usage_current: [39. 29. 28. 32.]\n",
            "  layer_3__total_assignments: 128.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Dict, Tuple, Optional, List, Any\n",
        "import time\n",
        "import logging\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pynvml\n",
        "import numpy as np\n",
        "from fairscale.nn.moe import MOELayer\n",
        "\n",
        "class WikiText2Dataset(Dataset):\n",
        "    # just a simulation for now, skeleton code\n",
        "    def __init__(self, vocab_size: int = 1000, seq_len: int = 512, num_samples: int = 1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = num_samples\n",
        "        logging.info(f\"Initialized WikiText2Dataset with {num_samples} samples, seq_len={seq_len}, vocab_size={vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Simulate text data: random token IDs\n",
        "        input_ids = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
        "        # For language modeling, the target is usually the next token\n",
        "        labels = torch.cat([input_ids[1:], torch.tensor([0])]) # Simple shift\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "\n",
        "def evaluate_model(\n",
        "    model: MoETransformer,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    profiler: GPUProfiler,\n",
        "    log_interval: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs an evaluation loop for the MoE model, logging inference time\n",
        "    and GPU metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The MoETransformer model.\n",
        "        dataloader: DataLoader for the evaluation dataset.\n",
        "        device: Device to run evaluation on (e.g., 'cuda' or 'cpu').\n",
        "        profiler: GPUProfiler instance for logging metrics.\n",
        "        log_interval: How often to log progress and metrics.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing average perplexity, total inference time,\n",
        "        average power draw, and aggregated MoE metrics.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    inference_times_ms = []\n",
        "    power_draws_watts = []\n",
        "    temperatures_c = []\n",
        "    gpu_utilizations_percent = []\n",
        "\n",
        "    # Aggregated MoE metrics across all layers and batches\n",
        "    aggregated_moe_metrics: Dict[str, List[float]] = {}\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    logging.info(f\"Starting evaluation on device: {device}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Measure inference time for the batch\n",
        "            if torch.cuda.is_available():\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "                start_event.record()\n",
        "\n",
        "            # Forward pass with profiling enabled for MoE metrics\n",
        "            model_output = model(input_ids, profile=True) # Enable profiling in model for detailed metrics\n",
        "            logits = model_output['logits']\n",
        "            aux_loss = model_output.get('aux_loss', torch.tensor(0.0)).item()\n",
        "            metrics = model_output.get('metrics', {})\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "                batch_inference_time_ms = start_event.elapsed_time(end_event)\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "            else:\n",
        "                # Approximate time for CPU\n",
        "                batch_inference_time_ms = (time.time() - start_time) * 1000 # Rough estimate\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "\n",
        "\n",
        "            # Calculate loss (for perplexity)\n",
        "            # Reshape logits and labels for CrossEntropyLoss\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            labels_flat = labels.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=0) # Assuming 0 is padding/ignore\n",
        "\n",
        "            total_loss += loss.item() * labels.numel() # Accumulate loss weighted by number of elements\n",
        "            total_tokens += labels.numel()\n",
        "            total_batches += 1\n",
        "\n",
        "            # Log GPU metrics\n",
        "            gpu_metrics = profiler.get_current_metrics()\n",
        "            if gpu_metrics:\n",
        "                power_draws_watts.append(gpu_metrics.power_draw)\n",
        "                temperatures_c.append(gpu_metrics.temperature)\n",
        "                gpu_utilizations_percent.append(gpu_metrics.gpu_utilization)\n",
        "\n",
        "            # Aggregate MoE specific metrics\n",
        "            if metrics:\n",
        "                for key, value in metrics.items():\n",
        "                    if isinstance(value, np.ndarray):\n",
        "                        # Convert arrays to lists for consistent aggregation\n",
        "                        value = value.tolist()\n",
        "                    if isinstance(value, (int, float)):\n",
        "                        aggregated_moe_metrics.setdefault(key, []).append(value)\n",
        "                    elif isinstance(value, list):\n",
        "                        # Handle array-like metrics (e.g., expert_usage)\n",
        "                        # We'll need to sum/average these appropriately later\n",
        "                        # For now, just store them as lists of lists/arrays\n",
        "                        aggregated_moe_metrics.setdefault(key, []).append(value)\n",
        "\n",
        "\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                avg_batch_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "                current_perplexity = math.exp(avg_batch_loss) if avg_batch_loss < 100 else float('inf') # Avoid overflow\n",
        "\n",
        "                log_msg = (\n",
        "                    f\"Batch {batch_idx+1}/{len(dataloader)} | \"\n",
        "                    f\"Loss: {avg_batch_loss:.4f} | \"\n",
        "                    f\"Perplexity: {current_perplexity:.2f} | \"\n",
        "                    f\"Batch Time: {batch_inference_time_ms:.2f} ms\"\n",
        "                )\n",
        "                if gpu_metrics:\n",
        "                    log_msg += (\n",
        "                        f\" | Power: {gpu_metrics.power_draw:.1f}W | \"\n",
        "                        f\"Temp: {gpu_metrics.temperature:.1f}°C | \"\n",
        "                        f\"GPU Util: {gpu_metrics.gpu_utilization:.1f}%\"\n",
        "                    )\n",
        "                logging.info(log_msg)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_inference_duration_sec = end_time - start_time\n",
        "\n",
        "    # Calculate overall averages\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "    final_perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
        "\n",
        "    avg_inference_time_ms = np.mean(inference_times_ms) if inference_times_ms else 0\n",
        "    avg_power_draw_watts = np.mean(power_draws_watts) if power_draws_watts else 0\n",
        "    avg_temperature_c = np.mean(temperatures_c) if temperatures_c else 0\n",
        "    avg_gpu_utilization_percent = np.mean(gpu_utilizations_percent) if gpu_utilizations_percent else 0\n",
        "\n",
        "    # Aggregate MoE metrics (e.g., average expert usage across batches)\n",
        "    final_moe_metrics: Dict[str, Any] = {}\n",
        "    for key, values_list in aggregated_moe_metrics.items():\n",
        "        if isinstance(values_list[0], (int, float)):\n",
        "            final_moe_metrics[f'avg_{key}'] = np.mean(values_list)\n",
        "        elif isinstance(values_list[0], list) or isinstance(values_list[0], np.ndarray):\n",
        "            # For expert usage arrays, sum them up and then normalize/average if needed\n",
        "            # For now, just average the arrays\n",
        "            final_moe_metrics[f'avg_{key}'] = np.mean(values_list, axis=0).tolist()\n",
        "\n",
        "\n",
        "    results = {\n",
        "        \"final_perplexity\": final_perplexity,\n",
        "        \"total_inference_duration_sec\": total_inference_duration_sec,\n",
        "        \"avg_inference_time_per_batch_ms\": avg_inference_time_ms,\n",
        "        \"avg_power_draw_watts\": avg_power_draw_watts,\n",
        "        \"avg_temperature_c\": avg_temperature_c,\n",
        "        \"avg_gpu_utilization_percent\": avg_gpu_utilization_percent,\n",
        "        \"aggregated_moe_metrics\": final_moe_metrics\n",
        "    }\n",
        "\n",
        "    logging.info(\"\\n--- Evaluation Summary ---\")\n",
        "    logging.info(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
        "    logging.info(f\"Total Inference Duration: {total_inference_duration_sec:.2f} seconds\")\n",
        "    logging.info(f\"Average Batch Inference Time: {avg_inference_time_ms:.2f} ms\")\n",
        "    if avg_power_draw_watts > 0:\n",
        "        logging.info(f\"Average Power Draw: {avg_power_draw_watts:.1f} W\")\n",
        "        logging.info(f\"Average Temperature: {avg_temperature_c:.1f} °C\")\n",
        "        logging.info(f\"Average GPU Utilization: {avg_gpu_utilization_percent:.1f} %\")\n",
        "    logging.info(\"Aggregated MoE Metrics:\")\n",
        "    for k, v in final_moe_metrics.items():\n",
        "        logging.info(f\"  {k}: {v}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Main execution block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # 1. Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 2. Initialize GPUProfiler\n",
        "    profiler = GPUProfiler()\n",
        "\n",
        "    # 3. Model Parameters (Adjust as needed for your specific MoE setup)\n",
        "    VOCAB_SIZE = 10000 # Example vocab size\n",
        "    D_MODEL = 512\n",
        "    N_HEADS = 8\n",
        "    N_LAYERS = 6\n",
        "    D_FF = 2048\n",
        "    N_EXPERTS = 8\n",
        "    TOP_K = 2\n",
        "    MAX_SEQ_LEN = 512\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    # Set which layers use MoE (e.g., every other layer)\n",
        "    USE_MOE_LAYERS = [i % 2 == 1 for i in range(N_LAYERS)] # [False, True, False, True, False, True]\n",
        "\n",
        "    # 4. Instantiate MoE Model\n",
        "    logging.info(\"Initializing MoETransformer model...\")\n",
        "    model = MoETransformer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        n_experts=N_EXPERTS,\n",
        "        top_k=TOP_K,\n",
        "        max_seq_len=MAX_SEQ_LEN,\n",
        "        use_moe_layers=USE_MOE_LAYERS\n",
        "    ).to(device)\n",
        "    logging.info(f\"Model instantiated with {sum(USE_MOE_LAYERS)} MoE layers.\")\n",
        "\n",
        "    # Optional: Load a pre-trained checkpoint if you have one\n",
        "    # checkpoint_path = \"path/to/your/checkpoint.pth\"\n",
        "    # if Path(checkpoint_path).exists():\n",
        "    #     logging.info(f\"Loading model checkpoint from {checkpoint_path}...\")\n",
        "    #     model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    #     logging.info(\"Model checkpoint loaded.\")\n",
        "    # else:\n",
        "    #     logging.warning(\"No model checkpoint found. Using randomly initialized weights.\")\n",
        "\n",
        "\n",
        "    # 5. Prepare Dataset and DataLoader (using simulated WikiText-2 for baseline)\n",
        "    # For actual WikiText-2, you'd use torchtext or similar to load and preprocess.\n",
        "    # Example: from torchtext.datasets import WikiText2\n",
        "    # For now, we use our dummy dataset.\n",
        "    logging.info(\"Preparing dataset...\")\n",
        "    eval_dataset = WikiText2Dataset(vocab_size=VOCAB_SIZE, seq_len=MAX_SEQ_LEN, num_samples=100) # Use a small number of samples for baseline\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    logging.info(f\"Evaluation DataLoader ready with {len(eval_dataloader)} batches.\")\n",
        "\n",
        "    # 6. Run Evaluation\n",
        "    logging.info(\"\\n--- Starting Baseline Inference Evaluation ---\")\n",
        "    baseline_results = evaluate_model(\n",
        "        model=model,\n",
        "        dataloader=eval_dataloader,\n",
        "        device=device,\n",
        "        profiler=profiler,\n",
        "        log_interval=10\n",
        "    )\n",
        "\n",
        "    # 7. Final Sanity Checks and Cleanup\n",
        "    logging.info(\"\\n--- Sanity Checks ---\")\n",
        "    if baseline_results['final_perplexity'] < float('inf'):\n",
        "        logging.info(f\"Perplexity sanity check: {baseline_results['final_perplexity']:.2f} (lower is better, typically starts high for untrained models)\")\n",
        "    else:\n",
        "        logging.warning(\"Perplexity is infinite. This might indicate issues like very high loss or training with random weights.\")\n",
        "\n",
        "    logging.info(f\"Average Power Draw: {baseline_results['avg_power_draw_watts']:.2f} W\")\n",
        "    logging.info(f\"Average Inference Time per Batch: {baseline_results['avg_inference_time_per_batch_ms']:.2f} ms\")\n",
        "\n",
        "    # Access detailed MoE metrics\n",
        "    if 'aggregated_moe_metrics' in baseline_results:\n",
        "        logging.info(\"\\nDetailed MoE Metrics (Averaged):\")\n",
        "        for key, value in baseline_results['aggregated_moe_metrics'].items():\n",
        "            logging.info(f\"  {key}: {value}\")\n",
        "\n",
        "    profiler.shutdown()\n",
        "    logging.info(\"Evaluation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "enJ7KJ286pQj",
        "outputId": "1bec8f35-18a4-41b8-82e3-e66970b3f392"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GPUProfiler' object has no attribute 'shutdown'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-804915877>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  {key}: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPUProfiler' object has no attribute 'shutdown'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d0c412d",
        "outputId": "6fb7ed37-ff51-49c5-937d-ea89f31c15d0"
      },
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "\n",
        "# Set environment variables for distributed training\n",
        "# In a real distributed setup, these would be set differently on each node/process\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29500' # Use a free port\n",
        "os.environ['RANK'] = '0' # Rank of the current process\n",
        "os.environ['WORLD_SIZE'] = '1' # Total number of processes\n",
        "\n",
        "# Initialize the distributed environment\n",
        "if not dist.is_initialized():\n",
        "    dist.init_process_group(\"nccl\" if torch.cuda.is_available() else \"gloo\")\n",
        "    print(f\"Distributed environment initialized with backend: {dist.get_backend()}\")\n",
        "    print(f\"Rank {dist.get_rank()} out of {dist.get_world_size()} processes\")\n",
        "else:\n",
        "    print(\"Distributed environment already initialized.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distributed environment initialized with backend: nccl\n",
            "Rank 0 out of 1 processes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf69bb6e"
      },
      "source": [
        "# Task\n",
        "Integrate GPU profiling and thermal signal generation into the provided model evaluation code. Modify the `SimpleMoELayer` and `TransformerBlock` to collect detailed expert usage metrics. Update the evaluation loop to use `GPUProfiler` and `ThermalSignalGenerator`, collect metrics, and demonstrate basic analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b782521"
      },
      "source": [
        "## Refine `simplemoelayer` for detailed profiling\n",
        "\n",
        "### Subtask:\n",
        "Modify the `SimpleMoELayer` class to record the execution time and token counts for each expert it dispatches to.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a2c41fd"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `SimpleMoELayer` class to include expert timing and token count collection within its forward pass, storing this information in the returned metrics dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232fff34",
        "outputId": "b1995cc3-6a4a-4560-9a5f-54cc0f120bb3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class SimpleMoELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified, non-distributed MoE layer for profiling.\n",
        "    Handles top-k routing, expert dispatch, and output combining.\n",
        "    \"\"\"\n",
        "    def __init__(self, gate: nn.Module, experts: nn.ModuleList, top_k: int = 2, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.gate = gate # The gating network (e.g., a Linear layer)\n",
        "        self.experts = experts # A ModuleList of expert networks\n",
        "        self.n_experts = len(experts)\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor # Keep for potential future use/comparison, not strictly used in this simple version\n",
        "\n",
        "        if top_k > self.n_experts:\n",
        "            raise ValueError(f\"top_k ({top_k}) cannot be greater than n_experts ({self.n_experts})\")\n",
        "\n",
        "        # Dictionary to store accumulated expert timings\n",
        "        self.expert_timings: Dict[int, float] = {}\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass of the simple MoE layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size * seq_len, d_model] (assuming flattened input)\n",
        "\n",
        "        Returns:\n",
        "            output: Output tensor [batch_size * seq_len, d_model]\n",
        "            aux_loss: Load balancing loss scalar\n",
        "            metrics: Dictionary containing routing metrics\n",
        "        \"\"\"\n",
        "        # Input shape is expected to be [num_tokens, d_model]\n",
        "        num_tokens, d_model = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Compute gate logits\n",
        "        gate_logits = self.gate(x)  # [num_tokens, n_experts]\n",
        "\n",
        "        # Get top-k experts and their probabilities\n",
        "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1) # [num_tokens, top_k]\n",
        "        top_k_probs = F.softmax(top_k_values, dim=-1) # [num_tokens, top_k]\n",
        "\n",
        "        # Compute load balancing loss (similar to the router's logic)\n",
        "        gate_probs_all = F.softmax(gate_logits, dim=-1) # [num_tokens, n_experts]\n",
        "\n",
        "        # Fraction of tokens assigned to each expert (based on top-1 for simplicity in loss)\n",
        "        # For top-k, a more accurate loss might be needed, but this is a common approximation\n",
        "        top1_indices = top_k_indices[:, 0] # [num_tokens]\n",
        "        expert_mask_top1 = F.one_hot(top1_indices, num_classes=self.n_experts).float() # [num_tokens, n_experts]\n",
        "        tokens_per_expert_for_loss = expert_mask_top1.sum(dim=0) # [n_experts]\n",
        "        total_tokens_for_loss = num_tokens # Use total tokens for normalization\n",
        "\n",
        "        # Normalize by total tokens\n",
        "        fraction_per_expert = tokens_per_expert_for_loss / (total_tokens_for_loss + 1e-8) # Add epsilon for stability\n",
        "\n",
        "        # Average gate probability for each expert (average over all tokens)\n",
        "        avg_gate_prob = gate_probs_all.mean(dim=0) # [n_experts]\n",
        "\n",
        "        # Load balancing loss\n",
        "        # This formulation encourages both fraction_per_expert and avg_gate_prob to be uniform\n",
        "        aux_loss = (fraction_per_expert * avg_gate_prob).sum() * self.n_experts\n",
        "\n",
        "\n",
        "        # Dispatch tokens to experts\n",
        "        output = torch.zeros_like(x) # [num_tokens, d_model]\n",
        "        metrics: Dict[str, Any] = {} # Use Dict[str, Any] for flexibility\n",
        "\n",
        "        # Collect metrics for profiling\n",
        "        expert_usage_counts = torch.zeros(self.n_experts, device=device)\n",
        "        expert_batch_timings: Dict[int, float] = {} # Timings for this batch\n",
        "\n",
        "        # Loop through each expert\n",
        "        for expert_id in range(self.n_experts):\n",
        "            # Find tokens routed to this expert (at any of the top-k positions)\n",
        "            expert_tokens_mask = (top_k_indices == expert_id).any(dim=-1) # [num_tokens]\n",
        "            expert_token_indices = torch.where(expert_tokens_mask)[0] # Indices of tokens for this expert\n",
        "\n",
        "            if expert_token_indices.numel() > 0:\n",
        "                # Get inputs for this expert\n",
        "                expert_input = x[expert_token_indices] # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Get weights for this expert's tokens\n",
        "                expert_weights_for_tokens = torch.zeros(expert_token_indices.numel(), device=device)\n",
        "                for i, token_idx in enumerate(expert_token_indices):\n",
        "                    pos_in_topk = torch.where(top_k_indices[token_idx] == expert_id)[0]\n",
        "                    if pos_in_topk.numel() > 0:\n",
        "                         expert_weights_for_tokens[i] = top_k_probs[token_idx, pos_in_topk].sum()\n",
        "\n",
        "\n",
        "                # Record start time for expert computation\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.synchronize()\n",
        "                    start_event = torch.cuda.Event(enable_timing=True)\n",
        "                    end_event = torch.cuda.Event(enable_timing=True)\n",
        "                    start_event.record()\n",
        "                else:\n",
        "                    start_time = time.time()\n",
        "\n",
        "\n",
        "                # Run expert forward pass\n",
        "                expert_output = self.experts[expert_id](expert_input) # [num_expert_tokens, d_model]\n",
        "\n",
        "                # Record end time and calculate duration\n",
        "                if torch.cuda.is_available():\n",
        "                    end_event.record()\n",
        "                    torch.cuda.synchronize()\n",
        "                    duration_ms = start_event.elapsed_time(end_event)\n",
        "                else:\n",
        "                    duration_ms = (time.time() - start_time) * 1000.0 # Convert to ms\n",
        "\n",
        "                expert_batch_timings[expert_id] = duration_ms\n",
        "                self.expert_timings[expert_id] = self.expert_timings.get(expert_id, 0.0) + duration_ms # Accumulate timing\n",
        "\n",
        "\n",
        "                # Weight expert output by the routing probability and scatter back\n",
        "                weighted_expert_output = expert_output * expert_weights_for_tokens.unsqueeze(-1)\n",
        "\n",
        "                # Scatter weighted output back to the original token positions\n",
        "                output[expert_token_indices] += weighted_expert_output\n",
        "\n",
        "                # Update usage counts\n",
        "                expert_usage_counts[expert_id] = expert_token_indices.numel()\n",
        "\n",
        "\n",
        "        # Add metrics to the dictionary\n",
        "        metrics['expert_usage_current'] = expert_usage_counts.cpu().numpy()\n",
        "        metrics['total_assignments'] = expert_usage_counts.sum().item() # Total tokens dispatched\n",
        "        metrics['expert_batch_timings_ms'] = expert_batch_timings\n",
        "        metrics['expert_cumulative_timings_ms'] = self.expert_timings\n",
        "\n",
        "\n",
        "        return output, aux_loss, metrics\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, n_experts: int = 8,\n",
        "                 top_k: int = 2, dropout: float = 0.1, use_moe: bool = True, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.use_moe = use_moe\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        if use_moe:\n",
        "            # Gating network for the MoE layer\n",
        "            gate_layer = nn.Linear(d_model, n_experts, bias=False)\n",
        "             # Initialize gate weights\n",
        "            nn.init.normal_(gate_layer.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "            # Create a ModuleList with n_experts distinct instances of the expert_module\n",
        "            experts_list = nn.ModuleList([\n",
        "                nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(dropout),\n",
        "                                   nn.Linear(d_ff, d_model)) for _ in range(n_experts)\n",
        "            ])\n",
        "\n",
        "            self.moe_layer = SimpleMoELayer(\n",
        "                gate=gate_layer,\n",
        "                experts=experts_list,\n",
        "                top_k=top_k,\n",
        "                capacity_factor=capacity_factor # Not used in SimpleMoELayer but kept for compatibility\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),\n",
        "                                              nn.ReLU(),\n",
        "                                              nn.Dropout(dropout),\n",
        "                                              nn.Linear(d_ff, d_model))\n",
        "\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.expert_timings = {} # Keep for potential future profiling\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Tuple[torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Forward pass with optional profiling.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "            mask: Attention mask\n",
        "            profile: Whether to collect timing information\n",
        "\n",
        "        Returns:\n",
        "            output: Transformed tensor\n",
        "            metrics: Dictionary containing routing metrics and timings\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Self-attention\n",
        "        residual = x\n",
        "        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(residual + self.dropout(attn_out))\n",
        "\n",
        "        # MoE or FFN\n",
        "        residual = x\n",
        "\n",
        "        if self.use_moe:\n",
        "            # Reshape input for MoE layer: [batch_size * seq_len, d_model]\n",
        "            batch_size, seq_len, d_model = x.shape\n",
        "            x_flat = x.view(-1, d_model)\n",
        "\n",
        "            # Profile MoE forward pass\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "\n",
        "            # MoE forward pass using SimpleMoELayer\n",
        "            moe_out_flat, aux_loss, moe_metrics = self.moe_layer(x_flat)\n",
        "\n",
        "            if profile and torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                total_time = start_event.elapsed_time(end_event)\n",
        "                metrics['moe_forward_time_ms'] = total_time\n",
        "                metrics.update(moe_metrics) # Add simple moe metrics\n",
        "\n",
        "            # Reshape output back to [batch_size, seq_len, d_model]\n",
        "            moe_out = moe_out_flat.view(batch_size, seq_len, d_model)\n",
        "\n",
        "            x = residual + self.dropout(moe_out)\n",
        "            metrics['aux_loss'] = aux_loss\n",
        "\n",
        "        else:\n",
        "            # Standard FFN\n",
        "            if profile and torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                start_event.record()\n",
        "                ffn_out = self.feed_forward(x)\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                metrics['ffn_time_ms'] = start_event.elapsed_time(end_event)\n",
        "            else:\n",
        "                ffn_out = self.feed_forward(x)\n",
        "\n",
        "            x = residual + self.dropout(ffn_out)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x, metrics\n",
        "\n",
        "\n",
        "class MoETransformer(nn.Module):\n",
        "    \"\"\"Simple MoE Transformer model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 6,\n",
        "        d_ff: int = 2048,\n",
        "        n_experts: int = 8,\n",
        "        top_k: int = 2,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        use_moe_layers: Optional[list] = None,  # Which layers use MoE\n",
        "        capacity_factor: float = 1.25,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Determine which layers use MoE\n",
        "        if use_moe_layers is None:\n",
        "            # By default, use MoE in every other layer starting from layer 1\n",
        "            use_moe_layers = [i % 2 == 1 for i in range(n_layers)]\n",
        "        elif len(use_moe_layers) != n_layers:\n",
        "             raise ValueError(f\"Length of use_moe_layers ({len(use_moe_layers)}) must match n_layers ({n_layers})\")\n",
        "\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                d_ff=d_ff,\n",
        "                n_experts=n_experts,\n",
        "                top_k=top_k,\n",
        "                dropout=dropout,\n",
        "                use_moe=use_moe_layers[i],\n",
        "                capacity_factor=capacity_factor,\n",
        "            )\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        profile: bool = False\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token indices [batch_size, seq_len]\n",
        "            attention_mask: Attention mask [batch_size, seq_len]\n",
        "            profile: Whether to collect profiling information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing logits, aux_loss, and optional metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        device = input_ids.device\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(0, seq_len, device=device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "\n",
        "        # Attention mask for causal modeling\n",
        "        if attention_mask is None:\n",
        "            # Create causal mask\n",
        "            causal_mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
        "            ).bool()\n",
        "        else:\n",
        "            causal_mask = attention_mask\n",
        "\n",
        "        total_aux_loss = 0.0\n",
        "        all_metrics = {} if profile else None\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Pass profiler instance to the layer if needed, or handle profiling inside layer\n",
        "            x, layer_metrics = layer(x, mask=causal_mask, profile=profile)\n",
        "\n",
        "            # Accumulate auxiliary loss from MoE layers\n",
        "            if 'aux_loss' in layer_metrics:\n",
        "                total_aux_loss += layer_metrics['aux_loss']\n",
        "\n",
        "            # Collect metrics\n",
        "            if profile:\n",
        "                for key, value in layer_metrics.items():\n",
        "                    if key != 'aux_loss':\n",
        "                        # Append metrics to lists if they exist, otherwise create\n",
        "                        metric_key = f'layer_{i}__{key}'\n",
        "                        if metric_key in all_metrics:\n",
        "                            if isinstance(all_metrics[metric_key], list) or isinstance(all_metrics[metric_key], dict):\n",
        "                                all_metrics[metric_key].append(value)\n",
        "                            else: # Convert to list if first time appending\n",
        "                                all_metrics[metric_key] = [all_metrics[metric_key], value]\n",
        "                        else:\n",
        "                            all_metrics[metric_key] = value\n",
        "\n",
        "\n",
        "        # Final layer norm and projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        output = {\n",
        "            'logits': logits,\n",
        "            'aux_loss': total_aux_loss,\n",
        "        }\n",
        "\n",
        "        if profile:\n",
        "            output['metrics'] = all_metrics\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage and testing (optional, moved from main execution block)\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model\n",
        "    model = MoETransformer(\n",
        "        vocab_size=1000,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        n_layers=4,\n",
        "        d_ff=1024,\n",
        "        n_experts=4,\n",
        "        top_k=2,\n",
        "        use_moe_layers=[False, True, False, True],  # MoE in layers 1 and 3\n",
        "    )\n",
        "\n",
        "    # Test input\n",
        "    batch_size, seq_len = 2, 32\n",
        "    input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
        "\n",
        "    # Forward pass with profiling\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, profile=True)\n",
        "\n",
        "    print(f\"Output logits shape: {output['logits'].shape}\")\n",
        "    print(f\"Auxiliary loss: {output['aux_loss'].item():.6f}\")\n",
        "\n",
        "    if 'metrics' in output:\n",
        "        print(\"\\nProfiling metrics:\")\n",
        "        for key, value in output['metrics'].items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"  {key}: {value.shape}\")\n",
        "            elif isinstance(value, list):\n",
        "                 print(f\"  {key}: {value}\") # Print list content\n",
        "            elif isinstance(value, dict):\n",
        "                 print(f\"  {key}: {value}\") # Print dict content\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits shape: torch.Size([2, 32, 1000])\n",
            "Auxiliary loss: 2.019300\n",
            "\n",
            "Profiling metrics:\n",
            "  layer_0__ffn_time_ms: 1.6239680051803589\n",
            "  layer_1__moe_forward_time_ms: 10.083104133605957\n",
            "  layer_1__expert_usage_current: [29. 35. 27. 37.]\n",
            "  layer_1__total_assignments: 128.0\n",
            "  layer_1__expert_batch_timings_ms: {0: 1.0079679489135742, 1: 1.061568021774292, 2: 0.8278719782829285, 3: 1.039072036743164}\n",
            "  layer_1__expert_cumulative_timings_ms: {0: 1.0079679489135742, 1: 1.061568021774292, 2: 0.8278719782829285, 3: 1.039072036743164}\n",
            "  layer_2__ffn_time_ms: 1.9371520280838013\n",
            "  layer_3__moe_forward_time_ms: 10.05891227722168\n",
            "  layer_3__expert_usage_current: [25. 42. 26. 35.]\n",
            "  layer_3__total_assignments: 128.0\n",
            "  layer_3__expert_batch_timings_ms: {0: 0.8411840200424194, 1: 1.0929280519485474, 2: 0.9604799747467041, 3: 0.9831680059432983}\n",
            "  layer_3__expert_cumulative_timings_ms: {0: 0.8411840200424194, 1: 1.0929280519485474, 2: 0.9604799747467041, 3: 0.9831680059432983}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02d4f90"
      },
      "source": [
        "**Reasoning**:\n",
        "The `SimpleMoELayer` class has been successfully modified to include expert timing and token counts in the metrics. The next step is to update the evaluation loop to utilize the `GPUProfiler` and `ThermalSignalGenerator` and collect the detailed metrics from the model's forward pass. This involves instantiating the `ThermalSignalGenerator` and modifying the `evaluate_model` function to get thermal signals and aggregate the detailed MoE metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b1d45a",
        "outputId": "800bad83-e059-44a6-8a0a-7aabc8632c8f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Dict, Tuple, Optional, List, Any\n",
        "import time\n",
        "import logging\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pynvml\n",
        "import numpy as np\n",
        "# from fairscale.nn.moe import MOELayer # Removing fairscale dependency\n",
        "\n",
        "# Assuming the GPUMetrics, ExpertProfile, GPUProfiler,\n",
        "# ThermalState, PowerMode, ThermalBudget, ThermalSignal,\n",
        "# and ThermalSignalGenerator classes are defined in previous cells\n",
        "# and available in the environment.\n",
        "\n",
        "class WikiText2Dataset(Dataset):\n",
        "    # just a simulation for now, skeleton code\n",
        "    def __init__(self, vocab_size: int = 1000, seq_len: int = 512, num_samples: int = 1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = num_samples\n",
        "        logging.info(f\"Initialized WikiText2Dataset with {num_samples} samples, seq_len={seq_len}, vocab_size={vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Simulate text data: random token IDs\n",
        "        input_ids = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
        "        # For language modeling, the target is usually the next token\n",
        "        labels = torch.cat([input_ids[1:], torch.tensor([0])]) # Simple shift\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "\n",
        "def evaluate_model(\n",
        "    model: MoETransformer,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    profiler: GPUProfiler,\n",
        "    thermal_signal_generator: ThermalSignalGenerator, # Add thermal signal generator\n",
        "    log_interval: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs an evaluation loop for the MoE model, logging inference time\n",
        "    and GPU metrics, and collecting thermal signals.\n",
        "\n",
        "    Args:\n",
        "        model: The MoETransformer model.\n",
        "        dataloader: DataLoader for the evaluation dataset.\n",
        "        device: Device to run evaluation on (e.g., 'cuda' or 'cpu').\n",
        "        profiler: GPUProfiler instance for logging metrics.\n",
        "        thermal_signal_generator: ThermalSignalGenerator instance.\n",
        "        log_interval: How often to log progress and metrics.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing average perplexity, total inference time,\n",
        "        average power draw, and aggregated MoE metrics.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    inference_times_ms = []\n",
        "    power_draws_watts = []\n",
        "    temperatures_c = []\n",
        "    gpu_utilizations_percent = []\n",
        "\n",
        "    # Aggregated MoE metrics across all layers and batches\n",
        "    aggregated_moe_metrics: Dict[str, List[Any]] = {} # Use Any to handle dicts/lists\n",
        "\n",
        "    thermal_signals: List[ThermalSignal] = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    logging.info(f\"Starting evaluation on device: {device}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get thermal signal before computation\n",
        "            thermal_signal = thermal_signal_generator.get_thermal_signal()\n",
        "            if thermal_signal:\n",
        "                thermal_signals.append(thermal_signal)\n",
        "                # Log thermal state and recommendations\n",
        "                logging.info(\n",
        "                    f\"Batch {batch_idx+1} Thermal Signal: State={thermal_signal.thermal_state.value}, \"\n",
        "                    f\"PowerMode={thermal_signal.power_mode.value}, Temp={thermal_signal.temperature:.1f}°C, \"\n",
        "                    f\"Power={thermal_signal.power_draw:.1f}W\"\n",
        "                )\n",
        "                # Note: In a real system, you would use these signals to adapt model behavior\n",
        "                # (e.g., select different experts, apply throttle factors). For this\n",
        "                # baseline evaluation, we just log them.\n",
        "\n",
        "\n",
        "            # Measure inference time for the batch\n",
        "            if torch.cuda.is_available():\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "                start_event.record()\n",
        "\n",
        "            # Forward pass with profiling enabled for MoE metrics\n",
        "            model_output = model(input_ids, profile=True) # Enable profiling in model for detailed metrics\n",
        "            logits = model_output['logits']\n",
        "            aux_loss = model_output.get('aux_loss', torch.tensor(0.0)).item()\n",
        "            metrics = model_output.get('metrics', {})\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "                batch_inference_time_ms = start_event.elapsed_time(end_event)\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "            else:\n",
        "                # Approximate time for CPU\n",
        "                batch_inference_time_ms = (time.time() - start_time) * 1000 # Rough estimate\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "\n",
        "\n",
        "            # Calculate loss (for perplexity)\n",
        "            # Reshape logits and labels for CrossEntropyLoss\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            labels_flat = labels.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=0) # Assuming 0 is padding/ignore\n",
        "\n",
        "            total_loss += loss.item() * labels.numel() # Accumulate loss weighted by number of elements\n",
        "            total_tokens += labels.numel()\n",
        "            total_batches += 1\n",
        "\n",
        "            # Log GPU metrics (most recent from profiler)\n",
        "            gpu_metrics = profiler.get_current_metrics()\n",
        "            if gpu_metrics:\n",
        "                power_draws_watts.append(gpu_metrics.power_draw)\n",
        "                temperatures_c.append(gpu_metrics.temperature)\n",
        "                gpu_utilizations_percent.append(gpu_metrics.gpu_utilization)\n",
        "\n",
        "            # Aggregate MoE specific metrics\n",
        "            if metrics:\n",
        "                for key, value in metrics.items():\n",
        "                    if isinstance(value, np.ndarray):\n",
        "                        # Convert arrays to lists for consistent aggregation\n",
        "                        value = value.tolist()\n",
        "\n",
        "                    if key not in aggregated_moe_metrics:\n",
        "                         aggregated_moe_metrics[key] = []\n",
        "\n",
        "                    if isinstance(value, (int, float, list)):\n",
        "                         aggregated_moe_metrics[key].append(value)\n",
        "                    elif isinstance(value, dict):\n",
        "                         # For dictionaries (like expert timings), aggregate per key\n",
        "                         for sub_key, sub_value in value.items():\n",
        "                             agg_key = f'{key}__{sub_key}' # e.g., 'expert_batch_timings_ms__0'\n",
        "                             if agg_key not in aggregated_moe_metrics:\n",
        "                                 aggregated_moe_metrics[agg_key] = []\n",
        "                             aggregated_moe_metrics[agg_key].append(sub_value)\n",
        "                    else:\n",
        "                         logging.warning(f\"Skipping aggregation for metric {key} with unsupported type {type(value)}\")\n",
        "\n",
        "\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                avg_batch_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "                current_perplexity = math.exp(avg_batch_loss) if avg_batch_loss < 100 else float('inf') # Avoid overflow\n",
        "\n",
        "                log_msg = (\n",
        "                    f\"Batch {batch_idx+1}/{len(dataloader)} | \"\n",
        "                    f\"Loss: {avg_batch_loss:.4f} | \"\n",
        "                    f\"Perplexity: {current_perplexity:.2f} | \"\n",
        "                    f\"Batch Time: {batch_inference_time_ms:.2f} ms\"\n",
        "                )\n",
        "                if gpu_metrics:\n",
        "                    log_msg += (\n",
        "                        f\" | Power: {gpu_metrics.power_draw:.1f}W | \"\n",
        "                        f\"Temp: {gpu_metrics.temperature:.1f}°C | \"\n",
        "                        f\"GPU Util: {gpu_metrics.gpu_utilization:.1f}%\"\n",
        "                    )\n",
        "                logging.info(log_msg)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_inference_duration_sec = end_time - start_time\n",
        "\n",
        "    # Calculate overall averages\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "    final_perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
        "\n",
        "    avg_inference_time_ms = np.mean(inference_times_ms) if inference_times_ms else 0\n",
        "    avg_power_draw_watts = np.mean(power_draws_watts) if power_draws_watts else 0\n",
        "    avg_temperature_c = np.mean(temperatures_c) if temperatures_c else 0\n",
        "    avg_gpu_utilization_percent = np.mean(gpu_utilizations_percent) if gpu_utilizations_percent else 0\n",
        "\n",
        "    # Aggregate MoE metrics (e.g., average expert usage across batches)\n",
        "    final_moe_metrics: Dict[str, Any] = {}\n",
        "    for key, values_list in aggregated_moe_metrics.items():\n",
        "        if not values_list:\n",
        "            continue\n",
        "\n",
        "        if isinstance(values_list[0], (int, float)):\n",
        "            final_moe_metrics[f'avg_{key}'] = np.mean(values_list)\n",
        "        elif isinstance(values_list[0], list) or isinstance(values_list[0], np.ndarray):\n",
        "             # For list/array metrics (like expert_usage), average the lists/arrays\n",
        "             # Ensure all lists/arrays have the same shape before averaging\n",
        "             try:\n",
        "                 final_moe_metrics[f'avg_{key}'] = np.mean([np.array(v) for v in values_list], axis=0).tolist()\n",
        "             except Exception as e:\n",
        "                 logging.warning(f\"Could not average list/array metric {key}: {e}\")\n",
        "                 final_moe_metrics[f'raw_{key}'] = values_list # Store raw list if averaging fails\n",
        "        elif isinstance(values_list[0], dict):\n",
        "             # This case should ideally be handled by the sub_key aggregation above,\n",
        "             # but as a fallback, log a warning.\n",
        "             logging.warning(f\"Metric {key} contains dictionaries, averaging not supported directly.\")\n",
        "             final_moe_metrics[f'raw_{key}'] = values_list # Store raw list of dicts\n",
        "\n",
        "    results = {\n",
        "        \"final_perplexity\": final_perplexity,\n",
        "        \"total_inference_duration_sec\": total_inference_duration_sec,\n",
        "        \"avg_inference_time_per_batch_ms\": avg_inference_time_ms,\n",
        "        \"avg_power_draw_watts\": avg_power_draw_watts,\n",
        "        \"avg_temperature_c\": avg_temperature_c,\n",
        "        \"avg_gpu_utilization_percent\": avg_gpu_utilization_percent,\n",
        "        \"aggregated_moe_metrics\": final_moe_metrics,\n",
        "        \"thermal_signals\": thermal_signals # Include collected thermal signals\n",
        "    }\n",
        "\n",
        "    logging.info(\"\\n--- Evaluation Summary ---\")\n",
        "    logging.info(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
        "    logging.info(f\"Total Inference Duration: {total_inference_duration_sec:.2f} seconds\")\n",
        "    logging.info(f\"Average Batch Inference Time: {avg_inference_time_ms:.2f} ms\")\n",
        "    if avg_power_draw_watts > 0:\n",
        "        logging.info(f\"Average Power Draw: {avg_power_draw_watts:.1f} W\")\n",
        "        logging.info(f\"Average Temperature: {avg_temperature_c:.1f} °C\")\n",
        "        logging.info(f\"Average GPU Utilization: {avg_gpu_utilization_percent:.1f} %\")\n",
        "    logging.info(\"Aggregated MoE Metrics:\")\n",
        "    for k, v in final_moe_metrics.items():\n",
        "        # Format array output nicely\n",
        "        if isinstance(v, list) and all(isinstance(i, (int, float)) for i in v):\n",
        "             logging.info(f\"  {k}: {np.array(v)}\")\n",
        "        else:\n",
        "             logging.info(f\"  {k}: {v}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Main execution block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # 1. Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 2. Initialize GPUProfiler\n",
        "    profiler = GPUProfiler()\n",
        "    profiler.start_profiling() # Start the profiling thread\n",
        "\n",
        "    # 3. Initialize ThermalSignalGenerator\n",
        "    # Assuming a default cost table exists or is handled by the class\n",
        "    thermal_signal_generator = ThermalSignalGenerator(profiler=profiler)\n",
        "    logging.info(\"ThermalSignalGenerator initialized.\")\n",
        "\n",
        "\n",
        "    # 4. Model Parameters (Adjust as needed for your specific MoE setup)\n",
        "    VOCAB_SIZE = 10000 # Example vocab size\n",
        "    D_MODEL = 512\n",
        "    N_HEADS = 8\n",
        "    N_LAYERS = 6\n",
        "    D_FF = 2048\n",
        "    N_EXPERTS = 8\n",
        "    TOP_K = 2\n",
        "    MAX_SEQ_LEN = 512\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    # Set which layers use MoE (e.g., every other layer)\n",
        "    USE_MOE_LAYERS = [i % 2 == 1 for i in range(N_LAYERS)] # [False, True, False, True, False, True]\n",
        "\n",
        "    # 5. Instantiate MoE Model\n",
        "    logging.info(\"Initializing MoETransformer model...\")\n",
        "    model = MoETransformer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        n_experts=N_EXPERTS,\n",
        "        top_k=TOP_K,\n",
        "        max_seq_len=MAX_SEQ_LEN,\n",
        "        use_moe_layers=USE_MOE_LAYERS\n",
        "    ).to(device)\n",
        "    logging.info(f\"Model instantiated with {sum(USE_MOE_LAYERS)} MoE layers.\")\n",
        "\n",
        "    # Optional: Load a pre-trained checkpoint if you have one\n",
        "    # checkpoint_path = \"path/to/your/checkpoint.pth\"\n",
        "    # if Path(checkpoint_path).exists():\n",
        "    #     logging.info(f\"Loading model checkpoint from {checkpoint_path}...\")\n",
        "    #     model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    #     logging.info(\"Model checkpoint loaded.\")\n",
        "    # else:\n",
        "    #     logging.warning(\"No model checkpoint found. Using randomly initialized weights.\")\n",
        "\n",
        "\n",
        "    # 6. Prepare Dataset and DataLoader (using simulated WikiText-2 for baseline)\n",
        "    # For actual WikiText-2, you'd use torchtext or similar to load and preprocess.\n",
        "    # Example: from torchtext.datasets import WikiText2\n",
        "    # For now, we use our dummy dataset.\n",
        "    logging.info(\"Preparing dataset...\")\n",
        "    eval_dataset = WikiText2Dataset(vocab_size=VOCAB_SIZE, seq_len=MAX_SEQ_LEN, num_samples=100) # Use a small number of samples for baseline\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    logging.info(f\"Evaluation DataLoader ready with {len(eval_dataloader)} batches.\")\n",
        "\n",
        "    # 7. Run Evaluation\n",
        "    logging.info(\"\\n--- Starting Baseline Inference Evaluation ---\")\n",
        "    baseline_results = evaluate_model(\n",
        "        model=model,\n",
        "        dataloader=eval_dataloader,\n",
        "        device=device,\n",
        "        profiler=profiler,\n",
        "        thermal_signal_generator=thermal_signal_generator, # Pass thermal signal generator\n",
        "        log_interval=10\n",
        "    )\n",
        "\n",
        "    # 8. Final Sanity Checks and Cleanup\n",
        "    logging.info(\"\\n--- Sanity Checks ---\")\n",
        "    if baseline_results['final_perplexity'] < float('inf'):\n",
        "        logging.info(f\"Perplexity sanity check: {baseline_results['final_perplexity']:.2f} (lower is better, typically starts high for untrained models)\")\n",
        "    else:\n",
        "        logging.warning(\"Perplexity is infinite. This might indicate issues like very high loss or training with random weights.\")\n",
        "\n",
        "    logging.info(f\"Average Power Draw: {baseline_results['avg_power_draw_watts']:.2f} W\")\n",
        "    logging.info(f\"Average Inference Time per Batch: {baseline_results['avg_inference_time_per_batch_ms']:.2f} ms\")\n",
        "\n",
        "    # Access detailed MoE metrics\n",
        "    if 'aggregated_moe_metrics' in baseline_results:\n",
        "        logging.info(\"\\nDetailed MoE Metrics (Averaged):\")\n",
        "        for key, value in baseline_results['aggregated_moe_metrics'].items():\n",
        "            logging.info(f\"  {key}: {value}\")\n",
        "\n",
        "    logging.info(\"\\nCollected Thermal Signals:\")\n",
        "    for i, signal in enumerate(baseline_results['thermal_signals']):\n",
        "         logging.info(f\"  Signal {i+1}: Temp={signal.temperature:.1f}°C, Power={signal.power_draw:.1f}W, State={signal.thermal_state.value}\")\n",
        "\n",
        "\n",
        "    profiler.stop_profiling() # Stop the profiling thread\n",
        "    logging.info(\"Evaluation complete.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Failed to load cost table from energy/cost_table.json: [Errno 2] No such file or directory: 'energy/cost_table.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71f5b19"
      },
      "source": [
        "**Reasoning**:\n",
        "The ThermalSignalGenerator failed to load the cost table because the file does not exist. Create a dummy `energy/cost_table.json` file with default values to allow the `ThermalSignalGenerator` to initialize successfully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uC9lNvs-tOK",
        "outputId": "6b32c519-2f31-4afb-c0c2-0d2577b2b17e"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create a dummy cost table directory and file\n",
        "cost_table_dir = Path(\"energy\")\n",
        "cost_table_dir.mkdir(exist_ok=True)\n",
        "cost_table_path = cost_table_dir / \"cost_table.json\"\n",
        "\n",
        "dummy_cost_table = {\n",
        "    \"thermal_parameters\": {\n",
        "        \"base_temperature\": 45.0,\n",
        "        \"warm_temperature_threshold\": 65.0,\n",
        "        \"hot_temperature_threshold\": 83.0,\n",
        "        \"critical_temperature\": 87.0,\n",
        "        \"thermal_time_constant\": 15.0,\n",
        "        \"emergency_cooldown_duration\": 30.0\n",
        "    },\n",
        "    \"energy_budgets\": {\n",
        "        \"low_power\": {\"max_power_watts\": 200, \"max_temperature\": 75.0, \"max_energy_per_token_mj\": 5.0},\n",
        "        \"balanced\": {\"max_power_watts\": 350, \"max_temperature\": 80.0, \"max_energy_per_token_mj\": 3.0},\n",
        "        \"performance\": {\"max_power_watts\": 450, \"max_temperature\": 85.0, \"max_energy_per_token_mj\": 1.5},\n",
        "        \"emergency\": {\"max_power_watts\": 100, \"max_temperature\": 70.0, \"max_energy_per_token_mj\": 10.0} # Added emergency budget\n",
        "    },\n",
        "    \"expert_profiles\": {\n",
        "        \"expert_A\": {\"average_power_watts\": 50, \"energy_per_token_mj\": 2.0, \"thermal_impact\": 0.1},\n",
        "        \"expert_B\": {\"average_power_watts\": 70, \"energy_per_token_mj\": 2.5, \"thermal_impact\": 0.15},\n",
        "        \"expert_C\": {\"average_power_watts\": 30, \"energy_per_token_mj\": 1.0, \"thermal_impact\": 0.05},\n",
        "        \"expert_D\": {\"average_power_watts\": 60, \"energy_per_token_mj\": 2.2, \"thermal_impact\": 0.12},\n",
        "        \"expert_E\": {\"average_power_watts\": 40, \"energy_per_token_mj\": 1.8, \"thermal_impact\": 0.08},\n",
        "        \"expert_F\": {\"average_power_watts\": 80, \"energy_per_token_mj\": 3.0, \"thermal_impact\": 0.2},\n",
        "        \"expert_G\": {\"average_power_watts\": 55, \"energy_per_token_mj\": 2.1, \"thermal_impact\": 0.11},\n",
        "        \"expert_H\": {\"average_power_watts\": 75, \"energy_per_token_mj\": 2.8, \"thermal_impact\": 0.18}\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(cost_table_path, 'w') as f:\n",
        "    json.dump(dummy_cost_table, f, indent=2)\n",
        "\n",
        "print(f\"Created dummy cost table at {cost_table_path}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dummy cost table at energy/cost_table.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "192c7ae4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dummy cost table exists, the `ThermalSignalGenerator` should be able to initialize correctly. The next step is to rerun the evaluation loop code to integrate the GPU profiling and thermal signal generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "e257cf15",
        "outputId": "381a40b1-63b1-4a5a-984c-67bd5ba78a93"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Dict, Tuple, Optional, List, Any\n",
        "import time\n",
        "import logging\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pynvml\n",
        "import numpy as np\n",
        "# from fairscale.nn.moe import MOELayer # Removing fairscale dependency\n",
        "\n",
        "# Assuming the GPUMetrics, ExpertProfile, GPUProfiler,\n",
        "# ThermalState, PowerMode, ThermalBudget, ThermalSignal,\n",
        "# and ThermalSignalGenerator classes are defined in previous cells\n",
        "# and available in the environment.\n",
        "\n",
        "class WikiText2Dataset(Dataset):\n",
        "    # just a simulation for now, skeleton code\n",
        "    def __init__(self, vocab_size: int = 1000, seq_len: int = 512, num_samples: int = 1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = num_samples\n",
        "        logging.info(f\"Initialized WikiText2Dataset with {num_samples} samples, seq_len={seq_len}, vocab_size={vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Simulate text data: random token IDs\n",
        "        input_ids = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
        "        # For language modeling, the target is usually the next token\n",
        "        labels = torch.cat([input_ids[1:], torch.tensor([0])]) # Simple shift\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "\n",
        "def evaluate_model(\n",
        "    model: MoETransformer,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    profiler: GPUProfiler,\n",
        "    thermal_signal_generator: ThermalSignalGenerator, # Add thermal signal generator\n",
        "    log_interval: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs an evaluation loop for the MoE model, logging inference time\n",
        "    and GPU metrics, and collecting thermal signals.\n",
        "\n",
        "    Args:\n",
        "        model: The MoETransformer model.\n",
        "        dataloader: DataLoader for the evaluation dataset.\n",
        "        device: Device to run evaluation on (e.g., 'cuda' or 'cpu').\n",
        "        profiler: GPUProfiler instance for logging metrics.\n",
        "        thermal_signal_generator: ThermalSignalGenerator instance.\n",
        "        log_interval: How often to log progress and metrics.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing average perplexity, total inference time,\n",
        "        average power draw, and aggregated MoE metrics.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    inference_times_ms = []\n",
        "    power_draws_watts = []\n",
        "    temperatures_c = []\n",
        "    gpu_utilizations_percent = []\n",
        "\n",
        "    # Aggregated MoE metrics across all layers and batches\n",
        "    aggregated_moe_metrics: Dict[str, List[Any]] = {} # Use Any to handle dicts/lists\n",
        "\n",
        "    thermal_signals: List[ThermalSignal] = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    logging.info(f\"Starting evaluation on device: {device}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get thermal signal before computation\n",
        "            thermal_signal = thermal_signal_generator.get_thermal_signal()\n",
        "            if thermal_signal:\n",
        "                thermal_signals.append(thermal_signal)\n",
        "                # Log thermal state and recommendations\n",
        "                logging.info(\n",
        "                    f\"Batch {batch_idx+1} Thermal Signal: State={thermal_signal.thermal_state.value}, \"\n",
        "                    f\"PowerMode={thermal_signal.power_mode.value}, Temp={thermal_signal.temperature:.1f}°C, \"\n",
        "                    f\"Power={thermal_signal.power_draw:.1f}W\"\n",
        "                )\n",
        "                # Note: In a real system, you would use these signals to adapt model behavior\n",
        "                # (e.g., select different experts, apply throttle factors). For this\n",
        "                # baseline evaluation, we just log them.\n",
        "\n",
        "\n",
        "            # Measure inference time for the batch\n",
        "            if torch.cuda.is_available():\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "                start_event.record()\n",
        "\n",
        "            # Forward pass with profiling enabled for MoE metrics\n",
        "            model_output = model(input_ids, profile=True) # Enable profiling in model for detailed metrics\n",
        "            logits = model_output['logits']\n",
        "            aux_loss = model_output.get('aux_loss', torch.tensor(0.0)).item()\n",
        "            metrics = model_output.get('metrics', {})\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "                batch_inference_time_ms = start_event.elapsed_time(end_event)\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "            else:\n",
        "                # Approximate time for CPU\n",
        "                batch_inference_time_ms = (time.time() - start_time) * 1000 # Rough estimate\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "\n",
        "\n",
        "            # Calculate loss (for perplexity)\n",
        "            # Reshape logits and labels for CrossEntropyLoss\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            labels_flat = labels.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=0) # Assuming 0 is padding/ignore\n",
        "\n",
        "            total_loss += loss.item() * labels.numel() # Accumulate loss weighted by number of elements\n",
        "            total_tokens += labels.numel()\n",
        "            total_batches += 1\n",
        "\n",
        "            # Log GPU metrics (most recent from profiler)\n",
        "            gpu_metrics = profiler.get_current_metrics()\n",
        "            if gpu_metrics:\n",
        "                power_draws_watts.append(gpu_metrics.power_draw)\n",
        "                temperatures_c.append(gpu_metrics.temperature)\n",
        "                gpu_utilizations_percent.append(gpu_metrics.gpu_utilization)\n",
        "\n",
        "            # Aggregate MoE specific metrics\n",
        "            if metrics:\n",
        "                for key, value in metrics.items():\n",
        "                    if isinstance(value, np.ndarray):\n",
        "                        # Convert arrays to lists for consistent aggregation\n",
        "                        value = value.tolist()\n",
        "\n",
        "                    if key not in aggregated_moe_metrics:\n",
        "                         aggregated_moe_metrics[key] = []\n",
        "\n",
        "                    if isinstance(value, (int, float, list)):\n",
        "                         aggregated_moe_metrics[key].append(value)\n",
        "                    elif isinstance(value, dict):\n",
        "                         # For dictionaries (like expert timings), aggregate per key\n",
        "                         for sub_key, sub_value in value.items():\n",
        "                             agg_key = f'{key}__{sub_key}' # e.g., 'expert_batch_timings_ms__0'\n",
        "                             if agg_key not in aggregated_moe_metrics:\n",
        "                                 aggregated_moe_metrics[agg_key] = []\n",
        "                             aggregated_moe_metrics[agg_key].append(sub_value)\n",
        "                    else:\n",
        "                         logging.warning(f\"Skipping aggregation for metric {key} with unsupported type {type(value)}\")\n",
        "\n",
        "\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                avg_batch_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "                current_perplexity = math.exp(avg_batch_loss) if avg_batch_loss < 100 else float('inf') # Avoid overflow\n",
        "\n",
        "                log_msg = (\n",
        "                    f\"Batch {batch_idx+1}/{len(dataloader)} | \"\n",
        "                    f\"Loss: {avg_batch_loss:.4f} | \"\n",
        "                    f\"Perplexity: {current_perplexity:.2f} | \"\n",
        "                    f\"Batch Time: {batch_inference_time_ms:.2f} ms\"\n",
        "                )\n",
        "                if gpu_metrics:\n",
        "                    log_msg += (\n",
        "                        f\" | Power: {gpu_metrics.power_draw:.1f}W | \"\n",
        "                        f\"Temp: {gpu_metrics.temperature:.1f}°C | \"\n",
        "                        f\"GPU Util: {gpu_metrics.gpu_utilization:.1f}%\"\n",
        "                    )\n",
        "                logging.info(log_msg)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_inference_duration_sec = end_time - start_time\n",
        "\n",
        "    # Calculate overall averages\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "    final_perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
        "\n",
        "    avg_inference_time_ms = np.mean(inference_times_ms) if inference_times_ms else 0\n",
        "    avg_power_draw_watts = np.mean(power_draws_watts) if power_draws_watts else 0\n",
        "    avg_temperature_c = np.mean(temperatures_c) if temperatures_c else 0\n",
        "    avg_gpu_utilization_percent = np.mean(gpu_utilizations_percent) if gpu_utilizations_percent else 0\n",
        "\n",
        "    # Aggregate MoE metrics (e.g., average expert usage across batches)\n",
        "    final_moe_metrics: Dict[str, Any] = {}\n",
        "    for key, values_list in aggregated_moe_metrics.items():\n",
        "        if not values_list:\n",
        "            continue\n",
        "\n",
        "        if isinstance(values_list[0], (int, float)):\n",
        "            final_moe_metrics[f'avg_{key}'] = np.mean(values_list)\n",
        "        elif isinstance(values_list[0], list) or isinstance(values_list[0], np.ndarray):\n",
        "             # For list/array metrics (like expert_usage), average the lists/arrays\n",
        "             # Ensure all lists/arrays have the same shape before averaging\n",
        "             try:\n",
        "                 final_moe_metrics[f'avg_{key}'] = np.mean([np.array(v) for v in values_list], axis=0).tolist()\n",
        "             except Exception as e:\n",
        "                 logging.warning(f\"Could not average list/array metric {key}: {e}\")\n",
        "                 final_moe_metrics[f'raw_{key}'] = values_list # Store raw list if averaging fails\n",
        "        elif isinstance(values_list[0], dict):\n",
        "             # This case should ideally be handled by the sub_key aggregation above,\n",
        "             # but as a fallback, log a warning.\n",
        "             logging.warning(f\"Metric {key} contains dictionaries, averaging not supported directly.\")\n",
        "             final_moe_metrics[f'raw_{key}'] = values_list # Store raw list of dicts\n",
        "\n",
        "    results = {\n",
        "        \"final_perplexity\": final_perplexity,\n",
        "        \"total_inference_duration_sec\": total_inference_duration_sec,\n",
        "        \"avg_inference_time_per_batch_ms\": avg_inference_time_ms,\n",
        "        \"avg_power_draw_watts\": avg_power_draw_watts,\n",
        "        \"avg_temperature_c\": avg_temperature_c,\n",
        "        \"avg_gpu_utilization_percent\": avg_gpu_utilization_percent,\n",
        "        \"aggregated_moe_metrics\": final_moe_metrics,\n",
        "        \"thermal_signals\": thermal_signals # Include collected thermal signals\n",
        "    }\n",
        "\n",
        "    logging.info(\"\\n--- Evaluation Summary ---\")\n",
        "    logging.info(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
        "    logging.info(f\"Total Inference Duration: {total_inference_duration_sec:.2f} seconds\")\n",
        "    logging.info(f\"Average Batch Inference Time: {avg_inference_time_per_batch_ms:.2f} ms\")\n",
        "    if avg_power_draw_watts > 0:\n",
        "        logging.info(f\"Average Power Draw: {avg_power_draw_watts:.1f} W\")\n",
        "        logging.info(f\"Average Temperature: {avg_temperature_c:.1f} °C\")\n",
        "        logging.info(f\"Average GPU Utilization: {avg_gpu_utilization_percent:.1f} %\")\n",
        "    logging.info(\"Aggregated MoE Metrics:\")\n",
        "    for k, v in final_moe_metrics.items():\n",
        "        # Format array output nicely\n",
        "        if isinstance(v, list) and all(isinstance(i, (int, float)) for i in v):\n",
        "             logging.info(f\"  {k}: {np.array(v)}\")\n",
        "        else:\n",
        "             logging.info(f\"  {k}: {v}\")\n",
        "\n",
        "    logging.info(\"\\nCollected Thermal Signals:\")\n",
        "    for i, signal in enumerate(results['thermal_signals']):\n",
        "         logging.info(f\"  Signal {i+1}: Temp={signal.temperature:.1f}°C, Power={signal.power_draw:.1f}W, State={signal.thermal_state.value}\")\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Main execution block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # 1. Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 2. Initialize GPUProfiler\n",
        "    profiler = GPUProfiler()\n",
        "    profiler.start_profiling() # Start the profiling thread\n",
        "\n",
        "    # 3. Initialize ThermalSignalGenerator\n",
        "    # Assuming a default cost table exists or is handled by the class\n",
        "    thermal_signal_generator = ThermalSignalGenerator(profiler=profiler)\n",
        "    logging.info(\"ThermalSignalGenerator initialized.\")\n",
        "\n",
        "\n",
        "    # 4. Model Parameters (Adjust as needed for your specific MoE setup)\n",
        "    VOCAB_SIZE = 10000 # Example vocab size\n",
        "    D_MODEL = 512\n",
        "    N_HEADS = 8\n",
        "    N_LAYERS = 6\n",
        "    D_FF = 2048\n",
        "    N_EXPERTS = 8\n",
        "    TOP_K = 2\n",
        "    MAX_SEQ_LEN = 512\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    # Set which layers use MoE (e.g., every other layer)\n",
        "    USE_MOE_LAYERS = [i % 2 == 1 for i in range(N_LAYERS)] # [False, True, False, True, False, True]\n",
        "\n",
        "    # 5. Instantiate MoE Model\n",
        "    logging.info(\"Initializing MoETransformer model...\")\n",
        "    model = MoETransformer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        n_experts=N_EXPERTS,\n",
        "        top_k=TOP_K,\n",
        "        max_seq_len=MAX_SEQ_LEN,\n",
        "        use_moe_layers=USE_MOE_LAYERS\n",
        "    ).to(device)\n",
        "    logging.info(f\"Model instantiated with {sum(USE_MOE_LAYERS)} MoE layers.\")\n",
        "\n",
        "    # Optional: Load a pre-trained checkpoint if you have one\n",
        "    # checkpoint_path = \"path/to/your/checkpoint.pth\"\n",
        "    # if Path(checkpoint_path).exists():\n",
        "    #     logging.info(f\"Loading model checkpoint from {checkpoint_path}...\")\n",
        "    #     model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    #     logging.info(\"Model checkpoint loaded.\")\n",
        "    # else:\n",
        "    #     logging.warning(\"No model checkpoint found. Using randomly initialized weights.\")\n",
        "\n",
        "\n",
        "    # 6. Prepare Dataset and DataLoader (using simulated WikiText-2 for baseline)\n",
        "    # For actual WikiText-2, you'd use torchtext or similar to load and preprocess.\n",
        "    # Example: from torchtext.datasets import WikiText2\n",
        "    # For now, we use our dummy dataset.\n",
        "    logging.info(\"Preparing dataset...\")\n",
        "    eval_dataset = WikiText2Dataset(vocab_size=VOCAB_SIZE, seq_len=MAX_SEQ_LEN, num_samples=100) # Use a small number of samples for baseline\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    logging.info(f\"Evaluation DataLoader ready with {len(eval_dataloader)} batches.\")\n",
        "\n",
        "    # 7. Run Evaluation\n",
        "    logging.info(\"\\n--- Starting Baseline Inference Evaluation ---\")\n",
        "    baseline_results = evaluate_model(\n",
        "        model=model,\n",
        "        dataloader=eval_dataloader,\n",
        "        device=device,\n",
        "        profiler=profiler,\n",
        "        thermal_signal_generator=thermal_signal_generator, # Pass thermal signal generator\n",
        "        log_interval=10\n",
        "    )\n",
        "\n",
        "    # 8. Final Sanity Checks and Cleanup\n",
        "    logging.info(\"\\n--- Sanity Checks ---\")\n",
        "    if baseline_results['final_perplexity'] < float('inf'):\n",
        "        logging.info(f\"Perplexity sanity check: {baseline_results['final_perplexity']:.2f} (lower is better, typically starts high for untrained models)\")\n",
        "    else:\n",
        "        logging.warning(\"Perplexity is infinite. This might indicate issues like very high loss or training with random weights.\")\n",
        "\n",
        "    logging.info(f\"Average Power Draw: {baseline_results['avg_power_draw_watts']:.2f} W\")\n",
        "    logging.info(f\"Average Inference Time per Batch: {baseline_results['avg_inference_time_per_batch_ms']:.2f} ms\")\n",
        "\n",
        "    # Access detailed MoE metrics\n",
        "    if 'aggregated_moe_metrics' in baseline_results:\n",
        "        logging.info(\"\\nDetailed MoE Metrics (Averaged):\")\n",
        "        for key, value in baseline_results['aggregated_moe_metrics'].items():\n",
        "            logging.info(f\"  {key}: {value}\")\n",
        "\n",
        "    logging.info(\"\\nCollected Thermal Signals:\")\n",
        "    for i, signal in enumerate(results['thermal_signals']):\n",
        "         logging.info(f\"  Signal {i+1}: Temp={signal.temperature:.1f}°C, Power={signal.power_draw:.1f}W, State={signal.thermal_state.value}\")\n",
        "\n",
        "\n",
        "    profiler.stop_profiling() # Stop the profiling thread\n",
        "    logging.info(\"Evaluation complete.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'avg_inference_time_per_batch_ms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-652785987>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# 7. Run Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Starting Baseline Inference Evaluation ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     baseline_results = evaluate_model(\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-652785987>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, device, profiler, thermal_signal_generator, log_interval)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final Perplexity: {final_perplexity:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total Inference Duration: {total_inference_duration_sec:.2f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Batch Inference Time: {avg_inference_time_per_batch_ms:.2f} ms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mavg_power_draw_watts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Power Draw: {avg_power_draw_watts:.1f} W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'avg_inference_time_per_batch_ms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415e0de5"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a `NameError` because `avg_inference_time_per_batch_ms` was used in the logging message outside the scope where it was defined within the `evaluate_model` function. This variable is part of the `results` dictionary returned by the function. Access the variable from the `baseline_results` dictionary instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Mnc9Gr_F4I"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Dict, Tuple, Optional, List, Any\n",
        "import time\n",
        "import logging\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pynvml\n",
        "import numpy as np\n",
        "# from fairscale.nn.moe import MOELayer # Removing fairscale dependency\n",
        "\n",
        "# Assuming the GPUMetrics, ExpertProfile, GPUProfiler,\n",
        "# ThermalState, PowerMode, ThermalBudget, ThermalSignal,\n",
        "# and ThermalSignalGenerator classes are defined in previous cells\n",
        "# and available in the environment.\n",
        "\n",
        "class WikiText2Dataset(Dataset):\n",
        "    # just a simulation for now, skeleton code\n",
        "    def __init__(self, vocab_size: int = 1000, seq_len: int = 512, num_samples: int = 1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = num_samples\n",
        "        logging.info(f\"Initialized WikiText2Dataset with {num_samples} samples, seq_len={seq_len}, vocab_size={vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Simulate text data: random token IDs\n",
        "        input_ids = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
        "        # For language modeling, the target is usually the next token\n",
        "        labels = torch.cat([input_ids[1:], torch.tensor([0])]) # Simple shift\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "\n",
        "def evaluate_model(\n",
        "    model: MoETransformer,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    profiler: GPUProfiler,\n",
        "    thermal_signal_generator: ThermalSignalGenerator, # Add thermal signal generator\n",
        "    log_interval: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs an evaluation loop for the MoE model, logging inference time\n",
        "    and GPU metrics, and collecting thermal signals.\n",
        "\n",
        "    Args:\n",
        "        model: The MoETransformer model.\n",
        "        dataloader: DataLoader for the evaluation dataset.\n",
        "        device: Device to run evaluation on (e.g., 'cuda' or 'cpu').\n",
        "        profiler: GPUProfiler instance for logging metrics.\n",
        "        thermal_signal_generator: ThermalSignalGenerator instance.\n",
        "        log_interval: How often to log progress and metrics.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing average perplexity, total inference time,\n",
        "        average power draw, and aggregated MoE metrics.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    inference_times_ms = []\n",
        "    power_draws_watts = []\n",
        "    temperatures_c = []\n",
        "    gpu_utilizations_percent = []\n",
        "\n",
        "    # Aggregated MoE metrics across all layers and batches\n",
        "    aggregated_moe_metrics: Dict[str, List[Any]] = {} # Use Any to handle dicts/lists\n",
        "\n",
        "    thermal_signals: List[ThermalSignal] = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    logging.info(f\"Starting evaluation on device: {device}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get thermal signal before computation\n",
        "            thermal_signal = thermal_signal_generator.get_thermal_signal()\n",
        "            if thermal_signal:\n",
        "                thermal_signals.append(thermal_signal)\n",
        "                # Log thermal state and recommendations\n",
        "                logging.info(\n",
        "                    f\"Batch {batch_idx+1} Thermal Signal: State={thermal_signal.thermal_state.value}, \"\n",
        "                    f\"PowerMode={thermal_signal.power_mode.value}, Temp={thermal_signal.temperature:.1f}°C, \"\n",
        "                    f\"Power={thermal_signal.power_draw:.1f}W\"\n",
        "                )\n",
        "                # Note: In a real system, you would use these signals to adapt model behavior\n",
        "                # (e.g., select different experts, apply throttle factors). For this\n",
        "                # baseline evaluation, we just log them.\n",
        "\n",
        "\n",
        "            # Measure inference time for the batch\n",
        "            if torch.cuda.is_available():\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "                start_event.record()\n",
        "\n",
        "            # Forward pass with profiling enabled for MoE metrics\n",
        "            model_output = model(input_ids, profile=True) # Enable profiling in model for detailed metrics\n",
        "            logits = model_output['logits']\n",
        "            aux_loss = model_output.get('aux_loss', torch.tensor(0.0)).item()\n",
        "            metrics = model_output.get('metrics', {})\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "                batch_inference_time_ms = start_event.elapsed_time(end_event)\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "            else:\n",
        "                # Approximate time for CPU\n",
        "                batch_inference_time_ms = (time.time() - start_time) * 1000 # Rough estimate\n",
        "                inference_times_ms.append(batch_inference_time_ms)\n",
        "\n",
        "\n",
        "            # Calculate loss (for perplexity)\n",
        "            # Reshape logits and labels for CrossEntropyLoss\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            labels_flat = labels.view(-1)\n",
        "            loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=0) # Assuming 0 is padding/ignore\n",
        "\n",
        "            total_loss += loss.item() * labels.numel() # Accumulate loss weighted by number of elements\n",
        "            total_tokens += labels.numel()\n",
        "            total_batches += 1\n",
        "\n",
        "            # Log GPU metrics (most recent from profiler)\n",
        "            gpu_metrics = profiler.get_current_metrics()\n",
        "            if gpu_metrics:\n",
        "                power_draws_watts.append(gpu_metrics.power_draw)\n",
        "                temperatures_c.append(gpu_metrics.temperature)\n",
        "                gpu_utilizations_percent.append(gpu_metrics.gpu_utilization)\n",
        "\n",
        "            # Aggregate MoE specific metrics\n",
        "            if metrics:\n",
        "                for key, value in metrics.items():\n",
        "                    if isinstance(value, np.ndarray):\n",
        "                        # Convert arrays to lists for consistent aggregation\n",
        "                        value = value.tolist()\n",
        "\n",
        "                    if key not in aggregated_moe_metrics:\n",
        "                         aggregated_moe_metrics[key] = []\n",
        "\n",
        "                    if isinstance(value, (int, float, list)):\n",
        "                         aggregated_moe_metrics[key].append(value)\n",
        "                    elif isinstance(value, dict):\n",
        "                         # For dictionaries (like expert timings), aggregate per key\n",
        "                         for sub_key, sub_value in value.items():\n",
        "                             agg_key = f'{key}__{sub_key}' # e.g., 'expert_batch_timings_ms__0'\n",
        "                             if agg_key not in aggregated_moe_metrics:\n",
        "                                 aggregated_moe_metrics[agg_key] = []\n",
        "                             aggregated_moe_metrics[agg_key].append(sub_value)\n",
        "                    else:\n",
        "                         logging.warning(f\"Skipping aggregation for metric {key} with unsupported type {type(value)}\")\n",
        "\n",
        "\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                avg_batch_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "                current_perplexity = math.exp(avg_batch_loss) if avg_batch_loss < 100 else float('inf') # Avoid overflow\n",
        "\n",
        "                log_msg = (\n",
        "                    f\"Batch {batch_idx+1}/{len(dataloader)} | \"\n",
        "                    f\"Loss: {avg_batch_loss:.4f} | \"\n",
        "                    f\"Perplexity: {current_perplexity:.2f} | \"\n",
        "                    f\"Batch Time: {batch_inference_time_ms:.2f} ms\"\n",
        "                )\n",
        "                if gpu_metrics:\n",
        "                    log_msg += (\n",
        "                        f\" | Power: {gpu_metrics.power_draw:.1f}W | \"\n",
        "                        f\"Temp: {gpu_metrics.temperature:.1f}°C | \"\n",
        "                        f\"GPU Util: {gpu_metrics.gpu_utilization:.1f}%\"\n",
        "                    )\n",
        "                logging.info(log_msg)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_inference_duration_sec = end_time - start_time\n",
        "\n",
        "    # Calculate overall averages\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "    final_perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n",
        "\n",
        "    avg_inference_time_ms = np.mean(inference_times_ms) if inference_times_ms else 0\n",
        "    avg_power_draw_watts = np.mean(power_draws_watts) if power_draws_watts else 0\n",
        "    avg_temperature_c = np.mean(temperatures_c) if temperatures_c else 0\n",
        "    avg_gpu_utilization_percent = np.mean(gpu_utilizations_percent) if gpu_utilizations_percent else 0\n",
        "\n",
        "    # Aggregate MoE metrics (e.g., average expert usage across batches)\n",
        "    final_moe_metrics: Dict[str, Any] = {}\n",
        "    for key, values_list in aggregated_moe_metrics.items():\n",
        "        if not values_list:\n",
        "            continue\n",
        "\n",
        "        if isinstance(values_list[0], (int, float)):\n",
        "            final_moe_metrics[f'avg_{key}'] = np.mean(values_list)\n",
        "        elif isinstance(values_list[0], list) or isinstance(values_list[0], np.ndarray):\n",
        "             # For list/array metrics (like expert_usage), average the lists/arrays\n",
        "             # Ensure all lists/arrays have the same shape before averaging\n",
        "             try:\n",
        "                 final_moe_metrics[f'avg_{key}'] = np.mean([np.array(v) for v in values_list], axis=0).tolist()\n",
        "             except Exception as e:\n",
        "                 logging.warning(f\"Could not average list/array metric {key}: {e}\")\n",
        "                 final_moe_metrics[f'raw_{key}'] = values_list # Store raw list if averaging fails\n",
        "        elif isinstance(values_list[0], dict):\n",
        "             # This case should ideally be handled by the sub_key aggregation above,\n",
        "             # but as a fallback, log a warning.\n",
        "             logging.warning(f\"Metric {key} contains dictionaries, averaging not supported directly.\")\n",
        "             final_moe_metrics[f'raw_{key}'] = values_list # Store raw list of dicts\n",
        "\n",
        "    results = {\n",
        "        \"final_perplexity\": final_perplexity,\n",
        "        \"total_inference_duration_sec\": total_inference_duration_sec,\n",
        "        \"avg_inference_time_per_batch_ms\": avg_inference_time_ms,\n",
        "        \"avg_power_draw_watts\": avg_power_draw_watts,\n",
        "        \"avg_temperature_c\": avg_temperature_c,\n",
        "        \"avg_gpu_utilization_percent\": avg_gpu_utilization_percent,\n",
        "        \"aggregated_moe_metrics\": final_moe_metrics,\n",
        "        \"thermal_signals\": thermal_signals # Include collected thermal signals\n",
        "    }\n",
        "\n",
        "    logging.info(\"\\n--- Evaluation Summary ---\")\n",
        "    logging.info(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
        "    logging.info(f\"Total Inference Duration: {total_inference_duration_sec:.2f} seconds\")\n",
        "    # Corrected access to avg_inference_time_per_batch_ms\n",
        "    logging.info(f\"Average Batch Inference Time: {results['avg_inference_time_per_batch_ms']:.2f} ms\")\n",
        "    if results['avg_power_draw_watts'] > 0:\n",
        "        logging.info(f\"Average Power Draw: {results['avg_power_draw_watts']:.1f} W\")\n",
        "        logging.info(f\"Average Temperature: {results['avg_temperature_c']:.1f} °C\")\n",
        "        logging.info(f\"Average GPU Utilization: {results['avg_gpu_utilization_percent']:.1f} %\")\n",
        "    logging.info(\"Aggregated MoE Metrics:\")\n",
        "    for k, v in final_moe_metrics.items():\n",
        "        # Format array output nicely\n",
        "        if isinstance(v, list) and all(isinstance(i, (int, float)) for i in v):\n",
        "             logging.info(f\"  {k}: {np.array(v)}\")\n",
        "        else:\n",
        "             logging.info(f\"  {k}: {v}\")\n",
        "\n",
        "    logging.info(\"\\nCollected Thermal Signals:\")\n",
        "    for i, signal in enumerate(results['thermal_signals']):\n",
        "         logging.info(f\"  Signal {i+1}: Temp={signal.temperature:.1f}°C, Power={signal.power_draw:.1f}W, State={signal.thermal_state.value}\")\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Main execution block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # 1. Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    # 2. Initialize GPUProfiler\n",
        "    profiler = GPUProfiler()\n",
        "    profiler.start_profiling() # Start the profiling thread\n",
        "\n",
        "    # 3. Initialize ThermalSignalGenerator\n",
        "    # Assuming a default cost table exists or is handled by the class\n",
        "    thermal_signal_generator = ThermalSignalGenerator(profiler=profiler)\n",
        "    logging.info(\"ThermalSignalGenerator initialized.\")\n",
        "\n",
        "\n",
        "    # 4. Model Parameters (Adjust as needed for your specific MoE setup)\n",
        "    VOCAB_SIZE = 10000 # Example vocab size\n",
        "    D_MODEL = 512\n",
        "    N_HEADS = 8\n",
        "    N_LAYERS = 6\n",
        "    D_FF = 2048\n",
        "    N_EXPERTS = 8\n",
        "    TOP_K = 2\n",
        "    MAX_SEQ_LEN = 512\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    # Set which layers use MoE (e.g., every other layer)\n",
        "    USE_MOE_LAYERS = [i % 2 == 1 for i in range(N_LAYERS)] # [False, True, False, True, False, True]\n",
        "\n",
        "    # 5. Instantiate MoE Model\n",
        "    logging.info(\"Initializing MoETransformer model...\")\n",
        "    model = MoETransformer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        n_experts=N_EXPERTS,\n",
        "        top_k=TOP_K,\n",
        "        max_seq_len=MAX_SEQ_LEN,\n",
        "        use_moe_layers=USE_MOE_LAYERS\n",
        "    ).to(device)\n",
        "    logging.info(f\"Model instantiated with {sum(USE_MOE_LAYERS)} MoE layers.\")\n",
        "\n",
        "    # Optional: Load a pre-trained checkpoint if you have one\n",
        "    # checkpoint_path = \"path/to/your/checkpoint.pth\"\n",
        "    # if Path(checkpoint_path).exists():\n",
        "    #     logging.info(f\"Loading model checkpoint from {checkpoint_path}...\")\n",
        "    #     model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    #     logging.info(\"Model checkpoint loaded.\")\n",
        "    # else:\n",
        "    #     logging.warning(\"No model checkpoint found. Using randomly initialized weights.\")\n",
        "\n",
        "\n",
        "    # 6. Prepare Dataset and DataLoader (using simulated WikiText-2 for baseline)\n",
        "    # For actual WikiText-2, you'd use torchtext or similar to load and preprocess.\n",
        "    # Example: from torchtext.datasets import WikiText2\n",
        "    # For now, we use our dummy dataset.\n",
        "    logging.info(\"Preparing dataset...\")\n",
        "    eval_dataset = WikiText2Dataset(vocab_size=VOCAB_SIZE, seq_len=MAX_SEQ_LEN, num_samples=100) # Use a small number of samples for baseline\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    logging.info(f\"Evaluation DataLoader ready with {len(eval_dataloader)} batches.\")\n",
        "\n",
        "    # 7. Run Evaluation\n",
        "    logging.info(\"\\n--- Starting Baseline Inference Evaluation ---\")\n",
        "    baseline_results = evaluate_model(\n",
        "        model=model,\n",
        "        dataloader=eval_dataloader,\n",
        "        device=device,\n",
        "        profiler=profiler,\n",
        "        thermal_signal_generator=thermal_signal_generator, # Pass thermal signal generator\n",
        "        log_interval=10\n",
        "    )\n",
        "\n",
        "    # 8. Final Sanity Checks and Cleanup\n",
        "    logging.info(\"\\n--- Sanity Checks ---\")\n",
        "    if baseline_results['final_perplexity'] < float('inf'):\n",
        "        logging.info(f\"Perplexity sanity check: {baseline_results['final_perplexity']:.2f} (lower is better, typically starts high for untrained models)\")\n",
        "    else:\n",
        "        logging.warning(\"Perplexity is infinite. This might indicate issues like very high loss or training with random weights.\")\n",
        "\n",
        "    logging.info(f\"Average Power Draw: {baseline_results['avg_power_draw_watts']:.2f} W\")\n",
        "    logging.info(f\"Average Inference Time per Batch: {baseline_results['avg_inference_time_per_batch_ms']:.2f} ms\")\n",
        "\n",
        "    # Access detailed MoE metrics\n",
        "    if 'aggregated_moe_metrics' in baseline_results:\n",
        "        logging.info(\"\\nDetailed MoE Metrics (Averaged):\")\n",
        "        for key, value in baseline_results['aggregated_moe_metrics'].items():\n",
        "            logging.info(f\"  {key}: {value}\")\n",
        "\n",
        "    logging.info(\"\\nCollected Thermal Signals:\")\n",
        "    for i, signal in enumerate(baseline_results['thermal_signals']):\n",
        "         logging.info(f\"  Signal {i+1}: Temp={signal.temperature:.1f}°C, Power={signal.power_draw:.1f}W, State={signal.thermal_state.value}\")\n",
        "\n",
        "\n",
        "    profiler.stop_profiling() # Stop the profiling thread\n",
        "    logging.info(\"Evaluation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}