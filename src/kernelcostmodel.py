# src/kernel_cost_model.py
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple

class KernelCostModel:
    """
    Manages and provides lookup for profiled kernel costs.
    It supports loading from a JSON file and interpolating costs for non-profiled batch sizes.
    """
    def __init__(self, data_path: str = None):
        self.data = pd.DataFrame()
        self.interpolation_cache: Dict[Tuple[str, int], Dict[str, float]] = {}

        if data_path:
            try:
                # Assuming data_path points to a JSON file generated by parse_profiler_output.py
                self.data = pd.read_json(data_path)
                # Ensure correct indexing for lookup
                self.data.set_index(["op_type", "batch_size"], inplace=True)
                print(f"KernelCostModel loaded from {data_path}. Entries: {len(self.data)}")
                # Convert index to a regular column for easier unique op_type access
                self.unique_op_types = self.data.index.get_level_values(0).unique().tolist()

            except FileNotFoundError:
                print(f"Warning: KernelCostModel data_path '{data_path}' not found. Initializing with dummy data.")
                self._initialize_dummy_data()
            except Exception as e:
                print(f"Error loading KernelCostModel from '{data_path}': {e}. Initializing with dummy data.")
                self._initialize_dummy_data()
        else:
            print("KernelCostModel initialized without data_path. Using dummy data.")
            self._initialize_dummy_data()

    def _initialize_dummy_data(self):
        """Initializes the model with synthetic/dummy data for testing when no real profiles exist."""
        dummy_ops = ["linear_fc1", "relu", "linear_fc2", "dequant_unpack_op"]
        dummy_batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
        all_dummy_costs = []

        for op in dummy_ops:
            for bs in dummy_batch_sizes:
                # Realistic-ish dummy scaling
                scale_factor = (bs / 32.0) ** 0.9 # Slightly non-linear
                energy_base = {'linear_fc1': 0.05, 'relu': 0.001, 'linear_fc2': 0.05, 'dequant_unpack_op': 0.02}.get(op, 0.01)
                latency_base = {'linear_fc1': 1.0, 'relu': 0.05, 'linear_fc2': 1.0, 'dequant_unpack_op': 0.1}.get(op, 0.1)
                temp_impact_base = {'linear_fc1': 0.01, 'relu': 0.0001, 'linear_fc2': 0.01, 'dequant_unpack_op': 0.005}.get(op, 0.001)

                all_dummy_costs.append({
                    "op_type": op,
                    "batch_size": bs,
                    "energy_joules": max(energy_base * scale_factor, 0.0001),
                    "latency_ms": max(latency_base * scale_factor, 0.001),
                    "temp_impact": max(temp_impact_base * scale_factor, 0.00001)
                })
        self.data = pd.DataFrame(all_dummy_costs)
        self.data.set_index(["op_type", "batch_size"], inplace=True)
        self.unique_op_types = dummy_ops
        print("KernelCostModel initialized with synthetic dummy data.")


    def get_cost(self, op_type: str, batch_size: int) -> Dict[str, float]:
        """
        Looks up the profiled cost for a given operation type and batch size.
        Performs linear interpolation if an exact match is not found.
        """
        cache_key = (op_type, batch_size)
        if cache_key in self.interpolation_cache:
            return self.interpolation_cache[cache_key]

        if op_type not in self.unique_op_types:
            print(f"Warning: Unknown op_type '{op_type}'. Returning default zero costs.")
            return {"energy_joules": 0.0, "latency_ms": 0.0, "temp_impact": 0.0}

        # Try exact match first
        try:
            result = self.data.loc[(op_type, batch_size)].to_dict()
            self.interpolation_cache[cache_key] = result
            return result
        except KeyError:
            pass # Proceed to interpolation


        # Interpolation logic
        available_batches = sorted(self.data.loc[op_type].index.get_level_values(0).unique().tolist())
        
        if not available_batches: # Should not happen if op_type is in unique_op_types
            print(f"Error: No batch sizes found for op_type '{op_type}'. Returning default zero costs.")
            return {"energy_joules": 0.0, "latency_ms": 0.0, "temp_impact": 0.0}

        # Find bounding sizes for interpolation
        lower = max([s for s in available_batches if s <= batch_size], default=available_batches[0])
        upper = min([s for s in available_batches if s >= batch_size], default=available_batches[-1])
        
        if lower == upper: # Fallback to closest if no range
            result = self.data.loc[(op_type, lower)].to_dict()
        else:
            # Linear interpolation
            alpha = (batch_size - lower) / (upper - lower)
            lower_costs = self.data.loc[(op_type, lower)].to_dict()
            upper_costs = self.data.loc[(op_type, upper)].to_dict()
            
            result = {}
            for key in lower_costs: # Assume all cost metrics are present in both bounds
                result[key] = lower_costs[key] * (1 - alpha) + upper_costs[key] * alpha
        
        self.interpolation_cache[cache_key] = result
        return result

    def get_all_op_types(self) -> List[str]:
        """Returns a list of all unique operation types known to the model."""
        return self.unique_op_types.copy()